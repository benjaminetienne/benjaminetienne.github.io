{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f526f0",
   "metadata": {},
   "source": [
    "# BERT Sentiment Classifier with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e162480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.etienne/Projects/CAR-DATA-TEAM/sentiment-analysis/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d87fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb8aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlit   : 1.33.0\n",
      "transformers: 4.40.1\n",
      "torch       : 2.2.2\n",
      "pandas      : 2.0.3\n",
      "lightning   : 2.2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p streamlit,transformers,torch,pandas,lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1012309",
   "metadata": {},
   "source": [
    "## Local training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84e0ee",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64d9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data can be found in the csb-sentiment-analysis bucket\n",
    "\n",
    "df=pd.concat([\n",
    "    pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),\n",
    "    pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a496fe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform                                           bodyText  sentiment  \\\n",
       "0  Twitter  @Kenichan I dived many times for the ball. Man...     0.4939   \n",
       "1  Twitter  @nationwideclass no, it's not behaving at all....    -0.4939   \n",
       "2  Twitter                                        Need a hug      0.4767   \n",
       "3  Twitter  @LOLTrish hey  long time no see! Yes.. Rains a...     0.6208   \n",
       "4  Twitter               @Tatiana_K nope they didn't have it      0.0000   \n",
       "\n",
       "         date  country  Target  \n",
       "0  2009-04-06      NaN       2  \n",
       "1  2009-04-06      NaN       0  \n",
       "2  2009-04-06      NaN       2  \n",
       "3  2009-04-06      NaN       2  \n",
       "4  2009-04-06      NaN       1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['sentiment'], axis=0)\n",
    "df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e075a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, _df = train_test_split(df, stratify=df['Target'], test_size=0.2)\n",
    "df_val, df_test = train_test_split(_df, stratify=_df['Target'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9fa1bd",
   "metadata": {},
   "source": [
    "### Load pretrained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0826b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be downloaded https://hf-mirror.com/google/bert_uncased_L-2_H-128_A-2/tree/main\n",
    "# or from the csb-sentiment-analysis bucket\n",
    "\n",
    "PRETRAINED_MODEL_DIR = '../models/bert_uncased_L-2_H-128_A-2'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "model = BertModel.from_pretrained(PRETRAINED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c125b",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb152fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=100):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.df=df\n",
    "        self.tokenizer=tokenizer\n",
    "        self.target=self.df['Target']\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = self.df['bodyText'].values[idx]\n",
    "        y = self.target.values[idx]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            X,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        x = {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
    "            }\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a45695",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds= BertDataset(df_train, tokenizer, max_length=100)\n",
    "train_loader=DataLoader(dataset=train_ds, batch_size=512)\n",
    "eval_ds= BertDataset(df_test, tokenizer, max_length=100)\n",
    "eval_loader=DataLoader(dataset=eval_ds, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3d5043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f669e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ids :\n",
      "---\n",
      "tensor([  101,  7842,  4246,  4948,  4542,  4122,  7084,  2000,  2272,  2461,\n",
      "         2085,  1060,  1012,  8299,  1024,  1013,  1013,  4714,  3126,  2140,\n",
      "         1012,  4012,  1013,  1051, 10354,  2615, 28311,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "> mask :\n",
      "----\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "> token_type_ids :\n",
      "--------------\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "> target\n",
      "------\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x, y = sample_batch\n",
    "for k, v in x.items():\n",
    "    print(\">\", k, \":\")\n",
    "    print(\"-\"*len(k))\n",
    "    print(v[0, :])\n",
    "print()\n",
    "print(\"> target\")\n",
    "print(\"-\"*6)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5118a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f928fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2293, 10733,   102,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"I love pizza\",\n",
    "                      max_length = 10,           # Pad & truncate all sentences.\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffe45d",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef9f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert_module = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n",
    "        \n",
    "        self.bert_module.requires_grad_(False)\n",
    "        for param in self.bert_module.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n",
    "        # print(ids.size(), mask.size(), token_type_ids.size())\n",
    "        x = self.bert_module(ids, mask, token_type_ids)\n",
    "        x = self.dropout(x['pooler_output'])\n",
    "        out = self.final(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4378590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentBERT(\n",
      "  (bert_module): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (final): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentimentBERT(model)\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "358bfc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params : 4386436 - Trainable : 397060 (9.05199574324121% of total)\n"
     ]
    }
   ],
   "source": [
    "total_parameters = sum([np.prod(p.size()) for p in bert_model.parameters()])\n",
    "model_parameters = filter(lambda p: p.requires_grad, bert_model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa78ebf",
   "metadata": {},
   "source": [
    "### Training and evaluation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c100d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (inputs, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(inputs)\n",
    "        \n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, label) in enumerate(dataloader):\n",
    "            predicted_label = model(inputs)\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc4d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE=512\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in bert_model.parameters() if p.requires_grad], LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_ds= BertDataset(df_train, tokenizer, max_length=100)\n",
    "train_loader=DataLoader(dataset=train_ds,batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "eval_ds= BertDataset(df_test, tokenizer, max_length=100)\n",
    "eval_loader=DataLoader(dataset=eval_ds,batch_size=BATCH_SIZE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f99862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(bert_model, train_loader, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    accu_val = evaluate(bert_model, valid_loader, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5ae19",
   "metadata": {},
   "source": [
    "## Train model on Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109be068",
   "metadata": {},
   "source": [
    "The first step is to build a Docker image from the Dockerfile\n",
    "\n",
    "`Dockerfile here`\n",
    "\n",
    "Then, we need to write a little `build.sh` script to build and push the image to Artefact Registry\n",
    "\n",
    "```bash\n",
    "# build.sh\n",
    "\n",
    "export PROJECT_ID=...\n",
    "export IMAGE_REPO_NAME=pt_bert_sentiment\n",
    "export IMAGE_TAG=dev\n",
    "export IMAGE_URI=eu.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "gcloud builds submit --tag $IMAGE_URI .\n",
    "```\n",
    "\n",
    "We will then launch a job, with 2 options:\n",
    "\n",
    "#### 1. Option 1: via gcloud : Create a little `job.sh` script as below \n",
    "\n",
    "```bash\n",
    "# job.sh\n",
    "\n",
    "export PROJECT_ID=...\n",
    "export BUCKET=\"csb-sentiment-analysis\"\n",
    "export REGION=\"europe-west4\"\n",
    "export SERVICE_ACCOUNT=...\n",
    "export JOB_NAME=\"pytorch_bert_training\"\n",
    "export MACHINE_TYPE=\"n1-standard-8\"  # We can specify GPUs here\n",
    "export ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "export IMAGE_URI=\"eu.gcr.io/$PROJECT_ID/pt_bert_sentiment:dev\"\n",
    "\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "--region=$REGION \\\n",
    "--display-name=$JOB_NAME \\\n",
    "--worker-pool-spec=machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=1,replica-count=1,container-image-uri=$IMAGE_URI \\\n",
    "--service-account=$SERVICE_ACCOUNT \\\n",
    "--args=\\\n",
    "--training-file=gs://$BUCKET/data/train.csv,\\\n",
    "--validation-file=gs://$BUCKET/data/eval.csv,\\\n",
    "--testing-file=gs://$BUCKET/data/test.csv,\\\n",
    "--job-dir=gs://$BUCKET/model/model.pt,\\\n",
    "--epochs=1,\\\n",
    "--batch-size=128,\\\n",
    "--learning-rate=0.0001\n",
    "```\n",
    "\n",
    "#### option 2: via the Python Client\n",
    "\n",
    "```python\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID=...\n",
    "BUCKET=\"csb-sentiment-analysis\"\n",
    "\n",
    "my_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name='pytorch_bert_training',\n",
    "    container_uri='eu.gcr.io/{PROJECT_ID}/pt_bert_sentiment:dev',\n",
    "    staging_bucket='gs://{BUCKET}')\n",
    "    \n",
    "my_job.run(replica_count=1,\n",
    "           machine_type='n1-standard-8',\n",
    "           accelerator_type='NVIDIA_TESLA_T4',\n",
    "           accelerator_count=1)\n",
    "           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cc19f",
   "metadata": {},
   "source": [
    "### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai custom-jobs stream-logs projects/1011434374459/locations/europe-west4/customJobs/8968484625693278208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953023f",
   "metadata": {},
   "source": [
    "## Inference with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4fc9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentBERT(\n",
       "  (bert_module): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(\"csb-sentiment-analysis\")\n",
    "blob = bucket.blob(\"model/model.pt\")\n",
    "loaded_model = SentimentBERT(model)\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    loaded_model.load_state_dict(torch.load(f, map_location=torch.device('cpu')))\n",
    "    \n",
    "loaded_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b01cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(comment):\n",
    "    mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    inputs = tokenizer(comment, return_tensors='pt')\n",
    "    ids = inputs[\"input_ids\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    x = {\n",
    "        'ids': ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids\n",
    "        }\n",
    "    result = loaded_model(x)\n",
    "    y = nn.Softmax()(result)\n",
    "    for n, x in enumerate(y[0]):\n",
    "        print(f\"{mapping[n]}: {100*x:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12cc52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 99.76%\n",
      "Neutral: 0.15%\n",
      "Positive: 0.09%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I hate watching this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf4cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.06%\n",
      "Neutral: 0.10%\n",
      "Positive: 99.84%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I really love this ring, it's so beautiful !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e17d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 97.73%\n",
      "Neutral: 0.49%\n",
      "Positive: 1.77%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"This place is a scam, i highly don't recommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c830c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.07%\n",
      "Neutral: 99.87%\n",
      "Positive: 0.06%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I don't know what to say\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6994cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.06%\n",
      "Neutral: 99.89%\n",
      "Positive: 0.06%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"The sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6041e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 96.94%\n",
      "Neutral: 2.93%\n",
      "Positive: 0.13%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"the cartier trinity is ugly bruh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bad6d40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 6.61%\n",
      "Neutral: 0.61%\n",
      "Positive: 92.78%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I have no idea what wedding band to get for this - any ideas? for now I have the cartier trinity ring underneath which works surprisingly well but will be changed to the wedding band, once we get married in 2025!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b63d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentenv",
   "language": "python",
   "name": "sentimentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
