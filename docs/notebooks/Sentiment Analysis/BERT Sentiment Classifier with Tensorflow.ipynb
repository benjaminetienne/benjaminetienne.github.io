{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f526f0",
   "metadata": {},
   "source": [
    "# BERT Sentiment Classifier with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e162480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d87fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2eb8aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow     : 2.13.1\n",
      "tensorflow_hub : 0.16.1\n",
      "tensorflow_text: 2.13.0\n",
      "sklearn        : 1.3.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p tensorflow,tensorflow_hub,tensorflow_text,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d64d9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data downloaded from https://www.kaggle.com/datasets/farisdurrani/sentimentsearch\n",
    "\n",
    "df=pd.concat([\n",
    "    pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),\n",
    "    pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64468a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821081"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba1f8d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform                                           bodyText  sentiment  \\\n",
       "0  Twitter  @Kenichan I dived many times for the ball. Man...     0.4939   \n",
       "1  Twitter  @nationwideclass no, it's not behaving at all....    -0.4939   \n",
       "2  Twitter                                        Need a hug      0.4767   \n",
       "3  Twitter  @LOLTrish hey  long time no see! Yes.. Rains a...     0.6208   \n",
       "4  Twitter               @Tatiana_K nope they didn't have it      0.0000   \n",
       "\n",
       "         date  country  Target  \n",
       "0  2009-04-06      NaN     1.0  \n",
       "1  2009-04-06      NaN    -1.0  \n",
       "2  2009-04-06      NaN     1.0  \n",
       "3  2009-04-06      NaN     1.0  \n",
       "4  2009-04-06      NaN     0.0  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "198bc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['sentiment'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a496fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/l97qqwps37qgq6txcj84v9b00000gn/T/ipykernel_10352/1955999727.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platform</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platform                                           bodyText  sentiment  \\\n",
       "0  Twitter  @Kenichan I dived many times for the ball. Man...     0.4939   \n",
       "1  Twitter  @nationwideclass no, it's not behaving at all....    -0.4939   \n",
       "2  Twitter                                        Need a hug      0.4767   \n",
       "3  Twitter  @LOLTrish hey  long time no see! Yes.. Rains a...     0.6208   \n",
       "4  Twitter               @Tatiana_K nope they didn't have it      0.0000   \n",
       "\n",
       "         date  country  Target  \n",
       "0  2009-04-06      NaN       2  \n",
       "1  2009-04-06      NaN       0  \n",
       "2  2009-04-06      NaN       2  \n",
       "3  2009-04-06      NaN       2  \n",
       "4  2009-04-06      NaN       1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3895661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "2    0.476433\n",
       "1    0.262097\n",
       "0    0.261471\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "517e85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "efd4b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "23ef87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (((328355,), (328355,))) samples\n",
      "Val : (((41044,), (41044,))) samples\n",
      "Test : (((41045,), (41045,))) samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train : ({X_train.shape, y_train.shape}) samples\")\n",
    "print(f\"Val : ({X_val.shape, y_val.shape}) samples\")\n",
    "print(f\"Test : ({X_test.shape, y_test.shape}) samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "eb152fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "48f4d7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'just back from a gig!, imadethis and Furlo rawked! met up with some old friends too &gt;.&lt; thanks for all the nice comments '\n",
      " b'@nerdist YAY FOR @COLINMELOY '\n",
      " b\"Wow am I tired..was up talking to my bffl Daisha til 12ish..she was talking some sense into me..Maybe I'll eat some cake to wake up \"\n",
      " b'has lost her voice, '], shape=(4,), dtype=string) tf.Tensor([2 2 2 0], shape=(4,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 16:46:28.695647: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.batch(4).take(1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "005fc268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "2    19561\n",
       "1    10786\n",
       "0    10698\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0826b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "TOKENIZER_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "BERT_MODEL_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "tokenizer = hub.KerasLayer(TOKENIZER_URL)\n",
    "bert_model = hub.KerasLayer(BERT_MODEL_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f9d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = ['sometimes i wish i was a panda']\n",
    "text_preprocessed = tokenizer(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f26a333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [  101  2823  1045  4299  1045  2001  1037 25462   102     0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :10]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :10]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "279b9d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': <tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       " array([[-0.9999753 ,  0.16285095, -0.9991155 ,  0.94176614, -0.99974805,\n",
       "          0.06312796, -0.998636  ,  0.4763149 ,  0.13516821, -0.02416325,\n",
       "         -0.6557846 , -0.04298883, -0.1316844 ,  1.        , -0.88311327,\n",
       "         -0.8912707 ,  0.90224254,  0.00613371, -0.8089084 ,  0.99620104,\n",
       "          0.9636094 ,  0.08133331,  0.99200433,  0.95425904, -0.999996  ,\n",
       "          0.05177542, -0.9996473 ,  0.9649326 ,  0.9903885 ,  0.07173721,\n",
       "          0.10204275,  0.09355631, -0.97235924, -0.15794337,  0.7986553 ,\n",
       "          0.9996173 , -0.6733732 , -0.155746  ,  0.8948879 , -0.99969214,\n",
       "          0.7239788 ,  0.9898049 , -0.9991481 ,  0.9894511 , -0.9999779 ,\n",
       "         -0.22439677, -0.99981123,  0.99636346,  0.9813393 ,  0.9832557 ,\n",
       "          0.9895193 , -0.43293884,  0.0314689 ,  0.9930125 ,  0.99891675,\n",
       "          0.999489  , -0.9907511 , -0.68675447,  0.5443378 , -0.77976435,\n",
       "         -0.05668417,  0.29758197, -0.84936285,  0.9744014 , -0.7764867 ,\n",
       "         -0.9999987 ,  0.7285832 ,  0.6136133 ,  0.8953313 ,  0.8456571 ,\n",
       "          0.9994263 , -0.03930187, -0.9997915 ,  0.10469574,  0.86636907,\n",
       "         -0.99544203, -0.26130468,  0.12262645, -0.91191435,  0.20496912,\n",
       "          0.6147858 , -0.22474639, -0.9962038 , -0.9999554 ,  0.99960154,\n",
       "         -0.8798603 ,  0.5636777 ,  0.557866  , -0.18760201,  0.6673624 ,\n",
       "         -0.9041123 ,  0.99915344, -0.94210184,  0.9994909 , -0.31144068,\n",
       "          0.40841162, -0.9936153 , -0.72045016, -0.9998872 , -0.9636848 ,\n",
       "         -0.9957678 ,  0.80706525, -0.9997106 , -0.9754736 , -0.99751425,\n",
       "          0.92712474, -0.9940275 , -0.9979612 , -0.46771565,  0.9972338 ,\n",
       "          0.9993334 ,  0.97802055, -0.7866818 ,  0.99930024, -0.9999989 ,\n",
       "          0.11323293,  0.95258844,  0.56515914,  0.21961828, -0.97894686,\n",
       "          0.26843572, -0.9999831 , -0.76925075,  0.90130097, -0.9996943 ,\n",
       "          0.93291706,  0.8831948 ,  0.99951047]], dtype=float32)>,\n",
       " 'encoder_outputs': [<tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n",
       "  array([[[-0.4104218 ,  0.7154593 , -6.4672914 , ..., -0.64346975,\n",
       "           -0.6155251 ,  0.51599675],\n",
       "          [-0.6587324 ,  1.3788172 ,  0.07930201, ..., -1.9233143 ,\n",
       "            0.69141245,  0.47687203],\n",
       "          [-2.4409764 ,  1.15798   ,  0.6488332 , ..., -4.0650206 ,\n",
       "           -0.51734275, -0.04621631],\n",
       "          ...,\n",
       "          [-1.394653  ,  0.4066436 , -0.25429088, ..., -1.3632857 ,\n",
       "            0.59795535,  1.0136952 ],\n",
       "          [-1.0741649 ,  0.6947189 , -0.23569326, ..., -1.1689848 ,\n",
       "            0.45016924,  0.924365  ],\n",
       "          [-0.7173106 ,  0.92680454, -0.17283653, ..., -0.9315816 ,\n",
       "            0.07558171,  0.79331887]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n",
       "  array([[[-1.1300321e+00, -9.9121027e-02, -3.1297979e+00, ...,\n",
       "            3.6399342e-02, -5.6742907e-01,  1.5096064e+00],\n",
       "          [-6.6179633e-01,  9.4794190e-01, -3.8296500e-01, ...,\n",
       "           -7.0248544e-04,  5.1630849e-01,  1.5306761e+00],\n",
       "          [-1.2262596e+00, -1.2711237e-01, -1.0036832e-01, ...,\n",
       "           -1.5480896e+00, -8.4113240e-01,  6.0122180e-01],\n",
       "          ...,\n",
       "          [-1.0829040e+00, -2.5349934e-02, -7.6939929e-01, ...,\n",
       "           -7.1985650e-01, -4.6738574e-01,  1.7646772e+00],\n",
       "          [-8.4669703e-01,  7.9183981e-02, -6.3098383e-01, ...,\n",
       "           -6.3131255e-01, -4.4220948e-01,  1.6185466e+00],\n",
       "          [-3.8328338e-01,  1.8087508e-01, -5.4923224e-01, ...,\n",
       "           -4.9273196e-01, -4.5769203e-01,  1.1787802e+00]]], dtype=float32)>],\n",
       " 'pooled_output': <tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       " array([[-0.9999753 ,  0.16285095, -0.9991155 ,  0.94176614, -0.99974805,\n",
       "          0.06312796, -0.998636  ,  0.4763149 ,  0.13516821, -0.02416325,\n",
       "         -0.6557846 , -0.04298883, -0.1316844 ,  1.        , -0.88311327,\n",
       "         -0.8912707 ,  0.90224254,  0.00613371, -0.8089084 ,  0.99620104,\n",
       "          0.9636094 ,  0.08133331,  0.99200433,  0.95425904, -0.999996  ,\n",
       "          0.05177542, -0.9996473 ,  0.9649326 ,  0.9903885 ,  0.07173721,\n",
       "          0.10204275,  0.09355631, -0.97235924, -0.15794337,  0.7986553 ,\n",
       "          0.9996173 , -0.6733732 , -0.155746  ,  0.8948879 , -0.99969214,\n",
       "          0.7239788 ,  0.9898049 , -0.9991481 ,  0.9894511 , -0.9999779 ,\n",
       "         -0.22439677, -0.99981123,  0.99636346,  0.9813393 ,  0.9832557 ,\n",
       "          0.9895193 , -0.43293884,  0.0314689 ,  0.9930125 ,  0.99891675,\n",
       "          0.999489  , -0.9907511 , -0.68675447,  0.5443378 , -0.77976435,\n",
       "         -0.05668417,  0.29758197, -0.84936285,  0.9744014 , -0.7764867 ,\n",
       "         -0.9999987 ,  0.7285832 ,  0.6136133 ,  0.8953313 ,  0.8456571 ,\n",
       "          0.9994263 , -0.03930187, -0.9997915 ,  0.10469574,  0.86636907,\n",
       "         -0.99544203, -0.26130468,  0.12262645, -0.91191435,  0.20496912,\n",
       "          0.6147858 , -0.22474639, -0.9962038 , -0.9999554 ,  0.99960154,\n",
       "         -0.8798603 ,  0.5636777 ,  0.557866  , -0.18760201,  0.6673624 ,\n",
       "         -0.9041123 ,  0.99915344, -0.94210184,  0.9994909 , -0.31144068,\n",
       "          0.40841162, -0.9936153 , -0.72045016, -0.9998872 , -0.9636848 ,\n",
       "         -0.9957678 ,  0.80706525, -0.9997106 , -0.9754736 , -0.99751425,\n",
       "          0.92712474, -0.9940275 , -0.9979612 , -0.46771565,  0.9972338 ,\n",
       "          0.9993334 ,  0.97802055, -0.7866818 ,  0.99930024, -0.9999989 ,\n",
       "          0.11323293,  0.95258844,  0.56515914,  0.21961828, -0.97894686,\n",
       "          0.26843572, -0.9999831 , -0.76925075,  0.90130097, -0.9996943 ,\n",
       "          0.93291706,  0.8831948 ,  0.99951047]], dtype=float32)>,\n",
       " 'sequence_output': <tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n",
       " array([[[-1.1300321e+00, -9.9121027e-02, -3.1297979e+00, ...,\n",
       "           3.6399342e-02, -5.6742907e-01,  1.5096064e+00],\n",
       "         [-6.6179633e-01,  9.4794190e-01, -3.8296500e-01, ...,\n",
       "          -7.0248544e-04,  5.1630849e-01,  1.5306761e+00],\n",
       "         [-1.2262596e+00, -1.2711237e-01, -1.0036832e-01, ...,\n",
       "          -1.5480896e+00, -8.4113240e-01,  6.0122180e-01],\n",
       "         ...,\n",
       "         [-1.0829040e+00, -2.5349934e-02, -7.6939929e-01, ...,\n",
       "          -7.1985650e-01, -4.6738574e-01,  1.7646772e+00],\n",
       "         [-8.4669703e-01,  7.9183981e-02, -6.3098383e-01, ...,\n",
       "          -6.3131255e-01, -4.4220948e-01,  1.6185466e+00],\n",
       "         [-3.8328338e-01,  1.8087508e-01, -5.4923224e-01, ...,\n",
       "          -4.9273196e-01, -4.5769203e-01,  1.1787802e+00]]], dtype=float32)>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "766434a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBERT(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = hub.KerasLayer(TOKENIZER_URL, name='tokenizer')\n",
    "        self.bert_model = hub.KerasLayer(BERT_MODEL_URL, trainable=True, name='bert_model')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.final = tf.keras.layers.Dense(3, activation=None)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.tokenizer(inputs)\n",
    "        x = self.bert_model(x)\n",
    "        x = self.dropout(x['pooled_output'])\n",
    "        out = self.final(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b4378590",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b134c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.14317669 0.43381873 0.42300454]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_raw_result = classifier(tf.constant(text_test))\n",
    "print(tf.keras.activations.softmax(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9ce2a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.2786571  0.41875377 0.30258903]\n",
      " [0.28298354 0.28624907 0.43076733]\n",
      " [0.11631086 0.41331226 0.47037688]\n",
      " [0.11895362 0.3087614  0.572285  ]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_raw_result = classifier(x)\n",
    "print(tf.keras.activations.softmax(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "358bfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "92f99862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1dc6f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params : 4386307\n",
      "Total params : 4386308.0\n",
      "% : 100.0000\n"
     ]
    }
   ],
   "source": [
    "trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.trainable_weights])\n",
    "non_trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.non_trainable_weights])\n",
    "total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "print(f\"Trainable params : {trainable_params}\")\n",
    "print(f\"Total params : {total_params}\")\n",
    "print(f\"% : {trainable_params/total_params*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7ea922d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with small_bert/bert_en_uncased_L-2_H-128_A-2\n",
      "Epoch 1/2\n",
      "642/642 [==============================] - 3013s 5s/step - loss: 0.3813 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.2611 - val_sparse_categorical_accuracy: 0.9037\n",
      "Epoch 2/2\n",
      "642/642 [==============================] - 2587s 4s/step - loss: 0.2471 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2170 - val_sparse_categorical_accuracy: 0.9204\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {BERT_MODEL_NAME}')\n",
    "history = classifier.fit(\n",
    "    x=X_train.values,\n",
    "    y=y_train.values,\n",
    "    validation_data=(X_val.values, y_val.values),\n",
    "    epochs=2,\n",
    "    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "84a67634",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"bert_classifier.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4dd8533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_labels = {v: k for k, v in sentiment_to_target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "01b01cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(comment):\n",
    "    inp = tf.data.Dataset.from_tensors(comment).batch(1)\n",
    "    for x in inp.take(1):\n",
    "        print(x)\n",
    "    bert_raw_result = classifier(x)\n",
    "    y = tf.keras.activations.softmax(bert_raw_result)\n",
    "    print(y)\n",
    "    for n, x in enumerate(y[0]):\n",
    "        print(f\"{reverse_labels[n]}: {100*x:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "12cc52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7.5321941e-04 1.0718697e-03 9.9817491e-01]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_raw_result = classifier(tf.constant(text_test))\n",
    "print(tf.keras.activations.softmax(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4cf4cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'I hate watching this'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[0.997603   0.00139782 0.00099914]], shape=(1, 3), dtype=float32)\n",
      "Negative: 99.76%\n",
      "Neutral: 0.14%\n",
      "Positive: 0.10%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I hate watching this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9e17d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"I really love this ring, it's so beautiful !\"], shape=(1,), dtype=string)\n",
      "tf.Tensor([[4.4591504e-04 1.4268260e-03 9.9812728e-01]], shape=(1, 3), dtype=float32)\n",
      "Negative: 0.04%\n",
      "Neutral: 0.14%\n",
      "Positive: 99.81%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I really love this ring, it's so beautiful !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a6994cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'This place is a scam, i highly disrecommend'], shape=(1,), dtype=string)\n",
      "tf.Tensor([[0.68084687 0.03579305 0.28336   ]], shape=(1, 3), dtype=float32)\n",
      "Negative: 68.08%\n",
      "Neutral: 3.58%\n",
      "Positive: 28.34%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"This place is a scam, i highly disrecommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "93593f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"I don't know what to say\"], shape=(1,), dtype=string)\n",
      "tf.Tensor([[7.085762e-04 9.987644e-01 5.271097e-04]], shape=(1, 3), dtype=float32)\n",
      "Negative: 0.07%\n",
      "Neutral: 99.88%\n",
      "Positive: 0.05%\n"
     ]
    }
   ],
   "source": [
    "sentiment_score(\"I don't know what to say\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10231efe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```py\n",
    "\n",
    "# %%writefile ../app/   msrc/models/tf_bert.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "BERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "TOKENIZER_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "BERT_MODEL_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "\n",
    "\n",
    "class SentimentBERT(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = hub.KerasLayer(TOKENIZER_URL, name='tokenizer')\n",
    "        self.bert_model = hub.KerasLayer(BERT_MODEL_URL, trainable=True, name='bert_model')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.final = tf.keras.layers.Dense(3, activation=None)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.tokenizer(inputs)\n",
    "        x = self.bert_model(x)\n",
    "        x = self.dropout(x['pooled_output'])\n",
    "        out = self.final(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def train_and_evaluate(**params):\n",
    "    \n",
    "    epochs = int(params.get('epochs'))\n",
    "    batch_size = int(params.get('batch_size'))\n",
    "    learning_rate = float(params.get('learning_rate'))\n",
    "    \n",
    "    df=pd.concat([\n",
    "        pd.read_csv(\"data/farisdurrani/twitter_filtered.csv\"),\n",
    "        pd.read_csv(\"data/farisdurrani/facebook_filtered.csv\")\n",
    "    ])\n",
    "    df = df.dropna(subset=['sentiment'], axis=0)\n",
    "    df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n",
    "\n",
    "    X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5)\n",
    "    \n",
    "    logging.info(f\"Train : ({X_train.shape, y_train.shape}) samples\")\n",
    "    logging.info(f\"Val : ({X_val.shape, y_val.shape}) samples\")\n",
    "    logging.info(f\"Test : ({X_test.shape, y_test.shape}) samples\")\n",
    "    \n",
    "    classifier = SentimentBERT()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.metrics.SparseCategoricalAccuracy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=metric)\n",
    "    \n",
    "    stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0, \n",
    "        patience=10, \n",
    "        verbose=2, \n",
    "        mode='min',\n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    logging.info(f'Training model with {BERT_MODEL_NAME}')\n",
    "    if args.dry_run:\n",
    "        logging.info(\"Dry run mode\")\n",
    "        epochs = 1\n",
    "        steps_per_epoch = 1\n",
    "    else:\n",
    "        steps_per_epoch = None\n",
    "        \n",
    "    history = classifier.fit(\n",
    "        x=X_train.values,\n",
    "        y=y_train.values,\n",
    "        validation_data=(X_val.values, y_val.values),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[stopping])\n",
    "    \n",
    "    if args.dry_run:\n",
    "        # If dry run, we do not run the evaluation\n",
    "        return None\n",
    "    \n",
    "    res = classifier.evaluate(\n",
    "        x=X_test.values, \n",
    "        y=y_test.values,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'train_acc': history.history['accuracy'],\n",
    "        'val_acc': history.history['val_accuracy'],\n",
    "        'test_acc': res[-1],\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "    \n",
    "    # save model and architecture to single file\n",
    "    if params.get('job_dir') is None:\n",
    "        logging.warning(\"No job dir provided, model will not be saved\")\n",
    "    else:\n",
    "        logging.info(\"Saving model to {} \".format(params.get('job_dir')))\n",
    "        classifier.save(params.get('job_dir'))\n",
    "    logging.info(\"Bye bye\")\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Create arguments here\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--job-dir', required=True)\n",
    "    parser.add_argument('--epochs', type=float, default=2)\n",
    "    parser.add_argument('--batch-size', type=float, default=1024)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.01)\n",
    "    parser.add_argument('--dry-run', action=\"store_true\")\n",
    "\n",
    "    # Parse them\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Execute training\n",
    "    train_and_evaluate(\n",
    "        job_dir=args.job_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        epochs=args.epochs\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3a32e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../app/src/models/tensorflow_bert/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../app/src/models/tensorflow_bert/requirements.txt\n",
    "\n",
    "tensorflow==2.13.1\n",
    "tensorflow_hub==0.16.1\n",
    "tensorflow_text==2.13.0\n",
    "scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "42259900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'oauth2.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Reauthentication required.\n",
      "/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'oauth2.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'reauth.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) There was a problem refreshing your current auth tokens: Reauthentication failed. Please run `gcloud auth login` to complete reauthentication with SAML.\n",
      "Please run:\n",
      "\n",
      "  $ gcloud auth login\n",
      "\n",
      "to obtain new credentials.\n",
      "\n",
      "If you have already logged in with a different account, run:\n",
      "\n",
      "  $ gcloud config set account ACCOUNT\n",
      "\n",
      "to select an already authenticated account to use.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = ...\n",
    "IMAGE_NAME=f'bert_tf_sentiment'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI ../app/src/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89915451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentenv",
   "language": "python",
   "name": "sentimentenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
