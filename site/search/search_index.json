{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"2019/08/21/Bayesian%20Basketball%20%3A%20were%20the%20Toronto%20Raptors%20really%20the%20best%20team%20during%20NBA%202019%20season%20%3F/","title":"Bayesian basketball","text":"<p>Work in progress</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/","title":"From notebooks to pipelines","text":"<p>(Article on Medium can be found here)</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#advocating-for-ai","title":"Advocating for AI","text":"<p>There is a misunderstanding (not to say fantasy) which keeps coming back in companies whenever it comes to AI and Machine Learning. People often misjudge the complexity and the skills needed to bring Machine Learning projects to production, either because they do not understand the job, or (even worse) because they think they understand it, whereas they don\u2019t.</p> <p>Their first reaction when discovering AI might be something like \u201cAI is actually pretty simple, I just need a Jupyter Notebook, copy paste code from here and there \u2014 or ask Copilot \u2014 and boom. No need to hire Data Scientists after all\u2026\u201d And the story always end badly, with bitterness, disappointment and a feeling that AI is a scam: difficulty to move to production, data drift, bugs, unwanted behavior.</p> <p>So let\u2019s write it down once and for all: AI/Machine Learning/any data-related job, is a real job, not a hobby. It requires skills, craftsmanship, and tools. If you think you can do ML in production with notebooks, you are wrong.</p> <p>This article aims at showing, with a simple example, all the effort, skills and tools, it takes to move from a notebook to a real pipeline in production. Because ML in production is, mostly, about being able to automate the run of your code on a regular basis, with automation and monitoring.</p> <p>And for those who are looking for an end-to-end \u201cnotebook to vertex pipelines\u201d tutorial, you might find this helpful.</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#a-simple-use-case","title":"A simple use case","text":"<p>Let\u2019s imagine you are a Data Scientist working at an e-commerce company. Your company is selling clothes online, and the marketing team asks for your help: they are preparing a special offer for specific products, and they would like to efficiently target customers by tailoring email content that will be pushed to them to maximize conversion. Your job is therefore simple: each customer should be assigned a score which represents the probability he/she purchases a product from the special offer.</p> <p>The special offer will specifically target those brands, meaning that the marketing team wants to know which customers will buy their next product from the below brands:</p> <p>Allegra K, Calvin Klein, Carhartt, Hanes, Volcom, Nautica, Quiksilver, Diesel, Dockers, Hurley</p> <p>We will, for this article, use a publicly available dataset from Google, the <code>thelook_ecommerce</code> dataset. It contains fake data with transactions, customer data, product data, everything we would have at our disposal when working at an online fashion retailer.</p> <p>To follow this notebook, you will need access to Google Cloud Platform, but the logic can be replicated to other Cloud providers or third-parties like Neptune, MLFlow, etc.</p> <p>As a respectable Data Scientist, you start by creating a notebook which will help us in exploring the data.</p> <p>We first import libraries which we will use during this article:</p> <p><pre><code>import catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport datetime as dt\n\nfrom dataclasses import dataclass\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\n%load_ext watermark\n%watermark --packages catboost,pandas,sklearn,numpy,google.cloud.bigquery\n</code></pre> catboost             : 1.0.4 pandas               : 1.4.2 numpy                : 1.22.4 google.cloud.bigquery: 3.2.0</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#before-production","title":"Before Production","text":""},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#getting-and-preparing-the-data","title":"Getting and preparing the data","text":"<p>We will then load the data from BigQuery using the Python Client. Be sure to use your own project id:</p> <p>{% highlight python %} query = \"\"\"     SELECT        transactions.user_id,       products.brand,       products.category,       products.department,       products.retail_price,       users.gender,       users.age,       users.created_at,       users.country,       users.city,       transactions.created_at     FROM <code>bigquery-public-data.thelook_ecommerce.order_items</code> as transactions     LEFT JOIN <code>bigquery-public-data.thelook_ecommerce.users</code> as users       ON transactions.user_id = users.id     LEFT JOIN <code>bigquery-public-data.thelook_ecommerce.products</code> as products       ON transactions.product_id = products.id     WHERE status &lt;&gt; 'Cancelled' \"\"\"</p> <p>client = bigquery.Client() df = client.query(query).to_dataframe() {% endhighlight %} You should see something like that when looking at the dataframe:</p> <p> </p> <p>These represent the transactions / purchases made by the customers, enriched with customer and product information.</p> <p>Given our objective is to predict which brand customers will buy in their next purchase, we will proceed as follows:</p> <ol> <li>Group purchases chronologically for each customer</li> <li>If a customer has N purchases, we consider the Nth purchase as the target, and the N-1 as our features.</li> <li>exclude customers with only 1 purchase</li> </ol> <p>Let\u2019s put that into code:</p> <pre><code># Compute recurrent customers\nrecurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n# Merge with dataset and filter those with more than 1 purchase\ndf = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\ndf = df.query('n_purchases &gt; 1')\n\n# Fill missing values\ndf.fillna('NA', inplace=True)\n\ntarget_brands = [\n    'Allegra K', \n    'Calvin Klein', \n    'Carhartt', \n    'Hanes', \n    'Volcom', \n    'Nautica', \n    'Quiksilver', \n    'Diesel',\n    'Dockers', \n    'Hurley'\n]\n\naggregation_columns = ['brand', 'department', 'category']\n\n# Group purchases by user chronologically\ndf_agg = (df.sort_values('created_at')\n          .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n          .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n         )\n\n# Create the target\ndf_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\ndf_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\ndf_agg['age'] = df_agg['age'].astype(float)\n\n# Remove last item of sequence features to avoid target leakage :\nfor col in aggregation_columns:\n    df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n</code></pre> <p>Notice how we removed the last item in the sequence features: this is very important as otherwise we get what we call a \u201cdata leakeage\u201d: the target is part of the features, the model is given the answer when learning.</p> <p>We now get this new df_agg dataframe:</p> <p> </p> <p>Comparing with the original dataframe, we see that user_id 2 has indeed purchased IZOD, Parke &amp; Ronen, and finally Orvis which is not in the target brands.</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#splitting-into-train-validation-and-test","title":"Splitting into train, validation and test","text":"<p>As a seasoned Data Scientist, you will now split your data into different sets, as you obviously know that all three are required to perform some rigorous Machine Learning. (Cross-validation is out of the scope for today folks, let\u2019s keep it simple.)</p> <p>One key thing when splitting the data is to use the not-so-well-known <code>stratify</code> parameter from the scikit-learn <code>train_test_split()</code> method. The reason for that is because of class-imbalance: if the target distribution (% of 0 and 1 in our case) differs between training and testing, we might get frustrated with poor results when deploying the model. </p> <p>ML 101 kids: keep you data distributions as similar as possible between training data and test data.</p> <pre><code># Remove unecessary features\n\ndf_agg.drop('last_purchase_category', axis=1, inplace=True)\ndf_agg.drop('last_purchase_brand', axis=1, inplace=True)\ndf_agg.drop('user_id', axis=1, inplace=True)\n\n# Split the data into train and eval\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\")\n\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\") \n# 30950 samples in train\n\ndf_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\nprint(f\"{len(df_val)} samples in val\")\nprint(f\"{len(df_test)} samples in test\")\n# 3869 samples in train\n# 3869 samples in test\nNow this is done, we will gracefully split our dataset between features and targets:\n\nX_train, y_train = df_train.iloc[:, :-1], df_train['target']\nX_val, y_val = df_val.iloc[:, :-1], df_val['target']\nX_test, y_test = df_test.iloc[:, :-1], df_test['target']\n</code></pre> <p>Among the feature are different types. We usually separate those between:</p> <ul> <li>numerical features: they are continuous, and reflect a measurable, or ordered, quantity.</li> <li>categorical features: they are usually discrete, and are often represented as strings (ex: a country, a color, etc\u2026)</li> <li>text features: they are usually sequences of words. Of course there can be more like image, video, audio, etc.</li> </ul>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#the-model-introducing-catboost","title":"The model: introducing CatBoost","text":"<p>For our classification problem (you already knew we were in a classification framework, didn\u2019t you?), we will use a simple yet very powerful library: CatBoost. It is built and maintained by Yandex, and provides a high-level API to easily play with boosted trees. It is close to XGBoost, though it does not work exactly the same under the hood.</p> <p>CatBoost offers a nice wrapper to deal with features from different kinds. In our case, some features can be considered as \u201ctext\u201d as they are the concatenation of words, such as \u201cCalvin Klein;BCBGeneration;Hanes\u201d. Dealing with this type of features can sometimes be painful as you need to handle them with text splitters, tokenizers, lemmatizers, etc. Hopefully, CatBoost can manage everything for us!</p> <pre><code># Define features\nfeatures = {\n    'numerical': ['retail_price', 'age'],\n    'static': ['gender', 'country', 'city'],\n    'dynamic': ['brand', 'department', 'category']\n}\n\n# Build CatBoost \"pools\", which are datasets\ntrain_pool = cb.Pool(\n    X_train,\n    y_train,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\nvalidation_pool = cb.Pool(\n    X_val,\n    y_val,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\n# Specify text processing options to handle our text features\ntext_processing_options = {\n    \"tokenizers\": [\n        {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n    ],\n    \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n    \"feature_processing\": {\n        \"default\": [\n            {\n                \"dictionaries_names\": [\"Word\"],\n                \"feature_calcers\": [\"BoW\"],\n                \"tokenizers_names\": [\"SemiColon\"],\n            }\n        ],\n    },\n}\n</code></pre> <p>We are now ready to define and train our model. Going through each and every parameter is out of today\u2019s scope as the number of parameters is quite impressive, but feel free to check the API yourself.</p> <p>And for brevity, we will not perform hyperparameter tuning today, but this is obviously a large part of the Data Scientist\u2019s job!</p> <pre><code># Train the model\nmodel = cb.CatBoostClassifier(\n    iterations=200,\n    loss_function=\"Logloss\",\n    random_state=42,\n    verbose=1,\n    auto_class_weights=\"SqrtBalanced\",\n    use_best_model=True,\n    text_processing=text_processing_options,\n    eval_metric='AUC'\n)\n\nmodel.fit(\n    train_pool, \n    eval_set=validation_pool, \n    verbose=10\n)\n</code></pre> <p>And voila, our model is trained. Are we done?</p> <p>No. We need to check that our model\u2019s performance between training and testing is consistent. A huge gap between training and testing means our model is overfitting (i.e. \u201clearning the training data by heart and not good at predicting unseen data\u201d).</p> <p>For our model evaluation, we will use the ROC-AUC score. Not deep-diving on this one either, but from my own experience this is a generally quite robust metric and way better than accuracy.</p> <p>A quick side note on accuracy: I usually do not recommend using this as your evaluation metric. Think of an imbalanced dataset where you have 1% of positives and 99% of negatives. What would be the accuracy of a very dumb model predicting 0 all the time? 99%. So accuracy not helpful here.</p> <p><pre><code>from sklearn.metrics import roc_auc_score\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=model.predict(X_train)):.2f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=model.predict(X_val)):.2f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=model.predict(X_test)):.2f}\")\n</code></pre> ROC-AUC for train set      : 0.612 ROC-AUC for validation set : 0.586 ROC-AUC for test set       : 0.622</p> <p>To be honest, 0.62 AUC is not great at all and a little bit disappointing for the expert Data Scientist you are. Our model definitely needs a little bit of parameter tuning here, and maybe we should also perform feature engineering more seriously.</p> <p>But it is already better than random predictions (phew):</p> <p><pre><code># random predictions\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=np.random.rand(len(y_train))):.3f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=np.random.rand(len(y_val))):.3f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=np.random.rand(len(y_test))):.3f}\")\n</code></pre> ROC-AUC for train set      : 0.501 ROC-AUC for validation set : 0.499 ROC-AUC for test set       : 0.501</p> <p>Let\u2019s assume we are satisfied for now with our model and our notebook. This is where amateur Data Scientists would stop. So how do we make the next step and become production ready?</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#moving-to-production","title":"Moving to Production","text":""},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#meet-docker","title":"Meet Docker","text":"<p>Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. This being said, think of Docker as code which can run everywhere, and allowing you to avoid the \u201cworks on your machine but not on mine\u201d situation.</p> <p>Why use Docker? Because among cool things such as being able to share your code, keep versions of it and ensure its easy deployment everywhere, it can also be used to build pipelines. Bear with me and you will understand as we go.</p> <p>The first step to building a containerized application is to refactor and clean up our messy notebook. We are going to define 2 files, <code>preprocess.py</code> and <code>train.py</code> for our very simple example, and put them in a src directory. We will also include our <code>requirements.txt</code> file with everything in it.</p> <p>(Blog post to come on poetry since requirements is so 2020)</p> <pre><code># src/preprocess.py\n\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\ndef create_dataset_from_bq():\n    query = \"\"\"\n        SELECT \n          transactions.user_id,\n          products.brand,\n          products.category,\n          products.department,\n          products.retail_price,\n          users.gender,\n          users.age,\n          users.created_at,\n          users.country,\n          users.city,\n          transactions.created_at\n        FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users\n          ON transactions.user_id = users.id\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products\n          ON transactions.product_id = products.id\n        WHERE status &lt;&gt; 'Cancelled'\n    \"\"\"\n    client = bigquery.Client(project='&lt;replace_with_your_project_id&gt;')\n    df = client.query(query).to_dataframe()\n    print(f\"{len(df)} rows loaded.\")\n\n    # Compute recurrent customers\n    recurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n    # Merge with dataset and filter those with more than 1 purchase\n    df = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\n    df = df.query('n_purchases &gt; 1')\n\n    # Fill missing value\n    df.fillna('NA', inplace=True)\n\n    target_brands = [\n        'Allegra K', \n        'Calvin Klein', \n        'Carhartt', \n        'Hanes', \n        'Volcom', \n        'Nautica', \n        'Quiksilver', \n        'Diesel',\n        'Dockers', \n        'Hurley'\n    ]\n\n    aggregation_columns = ['brand', 'department', 'category']\n\n    # Group purchases by user chronologically\n    df_agg = (df.sort_values('created_at')\n              .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n              .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n             )\n\n    # Create the target\n    df_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\n    df_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\n    df_agg['age'] = df_agg['age'].astype(float)\n\n    # Remove last item of sequence features to avoid target leakage :\n    for col in aggregation_columns:\n        df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n\n    df_agg.drop('last_purchase_category', axis=1, inplace=True)\n    df_agg.drop('last_purchase_brand', axis=1, inplace=True)\n    df_agg.drop('user_id', axis=1, inplace=True)\n    return df_agg\n\n\ndef make_data_splits(df_agg):\n\n    df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\n    print(f\"{len(df_train)} samples in train\")\n\n    df_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\n    print(f\"{len(df_val)} samples in val\")\n    print(f\"{len(df_test)} samples in test\")\n\n    return df_train, df_val, df_test\n</code></pre> <pre><code># src/train.py\n\nimport catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport argparse\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef train_and_evaluate(\n        train_path: str,\n        validation_path: str,\n        test_path: str\n    ):\n    df_train = pd.read_csv(train_path)\n    df_val = pd.read_csv(validation_path)\n    df_test = pd.read_csv(test_path)\n\n    df_train.fillna('NA', inplace=True)\n    df_val.fillna('NA', inplace=True)\n    df_test.fillna('NA', inplace=True)\n\n    X_train, y_train = df_train.iloc[:, :-1], df_train['target']\n    X_val, y_val = df_val.iloc[:, :-1], df_val['target']\n    X_test, y_test = df_test.iloc[:, :-1], df_test['target']\n\n    features = {\n        'numerical': ['retail_price', 'age'],\n        'static': ['gender', 'country', 'city'],\n        'dynamic': ['brand', 'department', 'category']\n    }\n\n    train_pool = cb.Pool(\n        X_train,\n        y_train,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    validation_pool = cb.Pool(\n        X_val,\n        y_val,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    test_pool = cb.Pool(\n        X_test,\n        y_test,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n\n    params = CatBoostParams()\n\n    text_processing_options = {\n        \"tokenizers\": [\n            {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n        ],\n        \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n        \"feature_processing\": {\n            \"default\": [\n                {\n                    \"dictionaries_names\": [\"Word\"],\n                    \"feature_calcers\": [\"BoW\"],\n                    \"tokenizers_names\": [\"SemiColon\"],\n                }\n            ],\n        },\n    }\n\n    # Train the model\n    model = cb.CatBoostClassifier(\n        iterations=200,\n        loss_function=\"Logloss\",\n        random_state=42,\n        verbose=1,\n        auto_class_weights=\"SqrtBalanced\",\n        use_best_model=True,\n        text_processing=text_processing_options,\n        eval_metric='AUC'\n    )\n\n\n    model.fit(\n        train_pool, \n        eval_set=validation_pool, \n        verbose=10\n    )\n\n    roc_train = roc_auc_score(y_true=y_train, y_score=model.predict(X_train))\n    roc_eval  = roc_auc_score(y_true=y_val, y_score=model.predict(X_val))\n    roc_test  = roc_auc_score(y_true=y_test, y_score=model.predict(X_test))\n    print(f\"ROC-AUC for train set      : {roc_train:.2f}\")\n    print(f\"ROC-AUC for validation set : {roc_eval:.2f}\")\n    print(f\"ROC-AUC for test.      set : {roc_test:.2f}\")\n\n    return {\"model\": model, \"scores\": {\"train\": roc_train, \"eval\": roc_eval, \"test\": roc_test}}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--train-path\", type=str)\n    parser.add_argument(\"--validation-path\", type=str)\n    parser.add_argument(\"--test-path\", type=str)\n    parser.add_argument(\"--output-dir\", type=str)\n    args, _ = parser.parse_known_args()\n    _ = train_and_evaluate(\n        args.train_path,\n        args.validation_path,\n        args.test_path)\n</code></pre> <p>Much cleaner now. You can actually launch your script from the command line now!</p> <p><code>$ python train.py --train-path xxx --validation-path yyy</code> etc. We are now ready to build our Docker image. For that we need to write a Dockerfile at the root of the project:</p> <p><pre><code># Dockerfile\n\nFROM python:3.8-slim\nWORKDIR /\nCOPY requirements.txt /requirements.txt\nCOPY src /src\nRUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\nENTRYPOINT [ \"bash\" ]\n</code></pre> This will take our requirements, copy the src folder and its contents, and install the requirements with pip when the image will build.</p> <p>To build and deploy this image to a container registry, we can use the Google Cloud SDK and the gcloud commands:</p> <p><pre><code>PROJECT_ID = ...\nIMAGE_NAME=f'thelook_training_demo'\nIMAGE_TAG='latest'\nIMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n\n!gcloud builds submit --tag $IMAGE_URI .\n</code></pre> If everything goes well, you should see something like that:</p> <p> </p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#vertex-pipelines-the-move-to-production","title":"Vertex Pipelines, the move to production","text":"<p>Docker images are the first step to doing some serious Machine Learning in production. The next step is building what we call \u201cpipelines\u201d. Pipelines are a series of operations orchestrated by a framework called Kubeflow. Kubeflow can run on Vertex AI on Google Cloud.</p> <p>The reasons for preferring pipelines over notebooks in production can be debatable, but I will give you three based on my experience:</p> <ul> <li>Monitoring and reproducibility: each pipeline is stored with its artefacts (datasets, models, metrics), meaning you can compare runs, re-run them, and audit them. Each time you re-run a notebook, you lose the history (or you have to manage artefacts yourself as weel as the logs. Good luck.)</li> <li>Costs: Running a notebook implies having a machine on which it runs. \u2014 This machine has a cost, and for large models or huge datasets you will need virtual machines with heavy specs.<ul> <li>You have to remember to switch it off when you don\u2019t use it.</li> <li>Or you may simply crash your local machine if you choose not to use a virtual machine and have other applications running.</li> <li>Vertex AI pipelines is a serverless service, meaning you do not have to manage the underlying infrastructure, and only pay for what you use, meaning the execution time.</li> </ul> </li> <li>Scalability: Good luck when running dozens of experiments on your local laptop simultaneously. You will roll back to using a VM, and scale that VM, and re-read the bullet point above. The last reason to prefer pipelines over notebooks is subjective and highly debatable as well, but in my opinion notebooks are simply not designed for running workloads on a schedule. They are great though for exploration.</li> </ul> <p>Use a cron job with a Docker image at least, or pipelines if you want to do things the right way, but never, ever, run a notebook in production.</p> <p>Without further ado, let\u2019s write the components of our pipeline:</p> <p><pre><code># IMPORT REQUIRED LIBRARIES\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (Artifact,\n                        Dataset,\n                        Input,\n                        Model,\n                        Output,\n                        Metrics,\n                        Markdown,\n                        HTML,\n                        component, \n                        OutputPath, \n                        InputPath)\nfrom kfp.v2 import compiler\nfrom google.cloud.aiplatform import pipeline_jobs\n\n%watermark --packages kfp,google.cloud.aiplatform\n</code></pre> kfp                    : 2.7.0 google.cloud.aiplatform: 1.50.0</p> <p>The first component will download the data from Bigquery and store it as a CSV file.</p> <p>The BASE_IMAGE we use is the image we build previously! We can use it to import modules and functions we defined in our Docker image src folder:</p> <p><pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"get_data.yaml\"\n)\ndef create_dataset_from_bq(\n    output_dir: Output[Dataset],\n):\n\n    from src.preprocess import create_dataset_from_bq\n\n    df = create_dataset_from_bq()\n\n    df.to_csv(output_dir.path, index=False)\n</code></pre> Next step: split data</p> <pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_test_split.yaml\",\n)\ndef make_data_splits(\n    dataset_full: Input[Dataset],\n    dataset_train: Output[Dataset],\n    dataset_val: Output[Dataset],\n    dataset_test: Output[Dataset]):\n\n    import pandas as pd\n    from src.preprocess import make_data_splits\n\n    df_agg = pd.read_csv(dataset_full.path)\n\n    df_agg.fillna('NA', inplace=True)\n\n    df_train, df_val, df_test = make_data_splits(df_agg)\n    print(f\"{len(df_train)} samples in train\")\n    print(f\"{len(df_val)} samples in train\")\n    print(f\"{len(df_test)} samples in test\")\n\n    df_train.to_csv(dataset_train.path, index=False)\n    df_val.to_csv(dataset_val.path, index=False)\n    df_test.to_csv(dataset_test.path, index=False)\n</code></pre> <p>Next step: model training. We will save the model scores to display them in the next step:</p> <pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_model.yaml\",\n)\ndef train_model(\n    dataset_train: Input[Dataset],\n    dataset_val: Input[Dataset],\n    dataset_test: Input[Dataset],\n    model: Output[Model]\n):\n\n    import json\n    from src.train import train_and_evaluate\n\n    outputs = train_and_evaluate(\n        dataset_train.path,\n        dataset_val.path,\n        dataset_test.path\n    )\n    cb_model = outputs['model']\n    scores = outputs['scores']\n\n\n    model.metadata[\"framework\"] = \"catboost\" \n    # Save the model as an artifact\n    with open(model.path, 'w') as f: \n        json.dump(scores, f)\n</code></pre> <p>The last step is computing the metrics (which are actually computed in the training of the model). It is merely necessary but is nice to show you how easy it is to build lightweight components. Notice how in this case we don\u2019t build the component from the BASE_IMAGE (which can be quite large sometimes), but only build a lightweight image with necessary components:</p> <p><pre><code>@component(\n    base_image=\"python:3.9\",\n    output_component_file=\"compute_metrics.yaml\",\n)\ndef compute_metrics(\n    model: Input[Model],\n    train_metric: Output[Metrics],\n    val_metric: Output[Metrics],\n    test_metric: Output[Metrics]\n):\n\n    import json\n\n    file_name = model.path\n    with open(file_name, 'r') as file:  \n        model_metrics = json.load(file)\n\n    train_metric.log_metric('train_auc', model_metrics['train'])\n    val_metric.log_metric('val_auc', model_metrics['eval'])\n    test_metric.log_metric('test_auc', model_metrics['test'])\n</code></pre> There are usually other steps which we can include, like if we want to deploy our model as an API endpoint, but this is more advanced-level and requires crafting another Docker image for the serving of the model. To be covered next time.</p> <p>Let\u2019s now glue the components together:</p> <pre><code># USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\nTIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\nDISPLAY_NAME = 'pipeline-thelook-demo-{}'.format(TIMESTAMP)\nPIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n\n# Define the pipeline. Notice how steps reuse outputs from previous steps\n@dsl.pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=\"pipeline-demo\"   \n)\n\ndef pipeline(\n    project: str = PROJECT_ID,\n    region: str = REGION, \n    display_name: str = DISPLAY_NAME\n):\n\n    load_data_op = create_dataset_from_bq()\n    train_test_split_op = make_data_splits(\n        dataset_full=load_data_op.outputs[\"output_dir\"]\n    )\n    train_model_op = train_model(\n        dataset_train=train_test_split_op.outputs[\"dataset_train\"], \n        dataset_val=train_test_split_op.outputs[\"dataset_val\"],\n        dataset_test=train_test_split_op.outputs[\"dataset_test\"],\n        )\n    model_evaluation_op = compute_metrics(\n        model=train_model_op.outputs[\"model\"]\n    )\n\n# Compile the pipeline as JSON\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path='thelook_pipeline.json'\n)\n\n# Start the pipeline\nstart_pipeline = pipeline_jobs.PipelineJob(\n    display_name=\"thelook-demo-pipeline\",\n    template_path=\"thelook_pipeline.json\",\n    enable_caching=False,\n    location=REGION,\n    project=PROJECT_ID\n)\n\n# Run the pipeline\nstart_pipeline.run(service_account=&lt;your_service_account_here&gt;)\n</code></pre> <p>If everything works well, you will now see your pipeline in the Vertex UI:</p> <p> </p> <p>You can click on it and see the different steps:</p> <p> </p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#conclusion","title":"Conclusion","text":"<p>Data Science, despite all the no-code/low-code enthusiasts telling you you don\u2019t need to be a developer to do Machine Learning, is a real job. Like every job, it requires skills, concepts and tools which go beyond notebooks.</p> <p>And for those who aspire to become Data Scientists, here is the reality of the job.</p> <p>Happy coding.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/","title":"Gpu on vertex","text":"<p>Jun 3, 2024</p> <p>(Article on Medium can be found here)</p> <p>In this article, we will take a concrete use case where we will fine-tune a BERT model on social media comments to perform sentiment analysis. As we will see, training this kind of model on a CPU is very cumbersome and not optimal. We will therefore see how we can leverage Google Cloud Platform to speed up the process by using a GPU for only 60 cents.</p> <p></p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#summary","title":"Summary","text":"<ul> <li>What is BERT</li> <li>What is Sentiment Analysis</li> <li>Get and prepare the data</li> <li>Use a small BERT pretrained model</li> <li>Create the dataloaders</li> <li>Write the main script to train the model</li> <li>Dockerize the script</li> <li>Build and push an image to Google Cloud</li> <li>Create a job on Vertex AI</li> </ul>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#what-is-bert","title":"What is BERT ?","text":"<p>BERT stands for Bidirectional Encoder Representations from Transformers and was open-sourced by Google in 2018. It is mainly used for NLP tasks as it was trained to capture semantics in sentences and provide rich word embeddings (representations). The difference with other models such as Word2Vec and Glove lies in the fact that it uses Transformers to process text. Transformers (refer to my previous article if you want to know more) are a family of neural networks which, a little bit like RNNs, have the ability to process sequences in both directions, therefore able to capture context around a word for example.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#what-is-sentiment-analysis","title":"What is Sentiment Analysis ?","text":"<p>Sentiment Analysis is a specific task within the NLP domain which objective is to classify text into categories related to the tonality of it. Tonality is often expressed as positive, negative, or neutral. It is very commonly used to analyze verbatims, posts on social media, product reviews, etc.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#fine-tuning-a-bert-model-on-social-media-data","title":"Fine-tuning a BERT model on social media data","text":""},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#getting-and-preparing-the-data","title":"Getting and preparing the data","text":"<p>The dataset we will use comes from Kaggle, you can download it here : https://www.kaggle.com/datasets/farisdurrani/sentimentsearch (CC BY 4.0 License). In my experiments, I only chose the datasets from Facebook and Twitter.</p> <p>The following snippet will take the csv files and save 3 splits (training, validation, and test) to where you want. I recommend saving them in Google Cloud Storage.</p> <p>You can run the script with:</p> <pre><code>python make_splits --output-dir gs://your-bucket/\n</code></pre> <p>And here is the script in itself:</p> <pre><code>import pandas as pd\nimport argparse\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n\ndef make_splits(output_dir):\n    df=pd.concat([        \n        pd.read_csv(\"data/farisdurrani/twitter_filtered.csv\"),\n        pd.read_csv(\"data/farisdurrani/facebook_filtered.csv\")\n    ])\n    df = df.dropna(subset=['sentiment'], axis=0)\n    df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n    df_train, df_ = train_test_split(df, stratify=df['Target'], test_size=0.2)\n    df_eval, df_test = train_test_split(df_, stratify=df_['Target'], test_size=0.5)\n\n    print(f\"Files will be saved in {output_dir}\")\n\n    df_train.to_csv(output_dir + \"/train.csv\", index=False)\n    df_eval.to_csv(output_dir + \"/eval.csv\", index=False)\n    df_test.to_csv(output_dir + \"/test.csv\", index=False)\n\n    print(f\"Train : ({df_train.shape}) samples\")\n    print(f\"Val : ({df_eval.shape}) samples\")\n    print(f\"Test : ({df_test.shape}) samples\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-dir')\n    args, _ = parser.parse_known_args()\n    make_splits(args.output_dir)\n</code></pre> <p>The data should look roughly like this:</p> <p></p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#using-a-small-bert-pretrained-model","title":"Using a small BERT pretrained model","text":"<p>For our model, we will use a lightweight BERT model, BERT-Tiny. This model has already been pretrained on vasts amount of data, but not necessarily with social media data and not necessarily with the objective of doing Sentiment Analysis. This is why we will fine-tune it.</p> <p>It contains only 2 layers with a 128-units dimension, the full list of models can be seen here if you want to take a larger one.</p> <p>Let\u2019s first create a <code>main.py</code> file, with all necessary modules:</p> <pre><code>import pandas as pd\nimport argparse\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport logging\nimport os\nos.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n\n\ndef train_and_evaluate(**params):\n    pass\n    # will be updated as we go\n</code></pre> <p>Let\u2019s also write down our requirements in a dedicated <code>requirements.txt</code></p> <pre><code>transformers==4.40.1\ntorch==2.2.2\npandas==2.0.3\nscikit-learn==1.3.2\ngcsfs\n</code></pre> <p>We will now load 2 parts to train our model:</p> <ul> <li>The tokenizer, which will take care of splitting the text inputs into tokens that BERT has been trained with.</li> <li>The model itself.</li> </ul> <p>You can obtain both from Huggingface here. You can also download them to Cloud Storage. That is what I did, and will therefore load them with:</p> <pre><code># Load pretrained tokenizers and bert model\ntokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\nmodel = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n</code></pre> <p>Let\u2019s now add the following piece to our file:</p> <pre><code>class SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n        # Uncomment the below if you only want to retrain certain layers.\n        # self.bert_module.requires_grad_(False)\n        # for param in self.bert_module.encoder.parameters():\n        #     param.requires_grad = True\n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        # print(ids.size(), mask.size(), token_type_ids.size())\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n</code></pre> <p>A little break here. We have several options when it comes to reusing an existing model.</p> <ul> <li>Transfer learning : we freeze the weights of the model and use it as a \u201cfeature extractor\u201d. We can therefore append additional layers downstream. This is frequently used in Computer Vision where models like VGG, Xception, etc. can be reused to train a custom model on small datasets</li> <li>Fine-tuning : we unfreeze all or part of the weights of the model and retrain the model on a custom dataset. This is the preferred approach when training custom LLMs.</li> </ul> <p>More details on Transfer learning and Fine-tuning here:</p> <p>In the model, we have chosen to unfreeze all the model, but feel free to freeze one or more layers of the pretrained BERT module and see how it influences the performance.</p> <p>The key part here is to add a fully connected layer after the BERT module to \u201clink\u201d it to our classification task, hence the final layer with 3 units. This will allow us to reuse the pretrained BERT weights and adapt our model to our task.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#creating-the-dataloaders","title":"Creating the dataloaders","text":"<p>To create the dataloaders we will need the Tokenizer loaded above. The Tokenizer takes a string as input, and returns several outputs amongst which we can find the tokens (\u2018input_ids\u2019 in our case):</p> <p></p> <p>The BERT tokenizer is a bit special and will return several outputs, but the most important one is the <code>input_ids</code>: they are the tokens used to encode our sentence. They might be words, or parts or words. For example, the word \u201clooking\u201d might be made of 2 tokens, \u201clook\u201d and \u201c##ing\u201d.</p> <p>Let\u2019s now create a dataloader module which will handle our datasets :</p> <pre><code>class BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#writing-the-main-script-to-train-the-model","title":"Writing the main script to train the model","text":"<p>Let us define first and foremost two functions to handle the training and evaluation steps:</p> <pre><code>def train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n</code></pre> <p>We are getting closer to getting our main script up and running. Let\u2019s stitch pieces together. We have:</p> <ul> <li>A <code>BertDataset</code> class to handle the loading of the data</li> <li>A <code>SentimentBERT</code> model which takes our Tiny-BERT model and adds an additional layer for our custom use case</li> <li><code>train()</code> and <code>eval()</code> functions to handle those steps</li> <li>A <code>train_and_eval()</code> functions that bundles everything</li> </ul> <p>We will use <code>argparse</code> to be able to launch our script with arguments. Such arguments are typically the train/eval/test files to run our model with any datasets, the path where our model will be stored, and parameters related to the training.</p> <pre><code>import pandas as pd\nimport time\nimport torch.nn as nn\nimport torch\nimport logging\nimport numpy as np\nimport argparse\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\n\nlogging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG)\nlogging.getLogger().setLevel(logging.INFO)\n\n# --- CONSTANTS ---\nBERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n\nif torch.cuda.is_available():\n    logging.info(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n    DEVICE = torch.device('cuda')\nelse:\n    logging.info(\"No GPU available. Training will run on CPU.\")\n    DEVICE = torch.device('cpu')\n\n# --- Data preparation and tokenization ---\nclass BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n\n# --- Model definition ---\nclass SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n\n# --- Training loop ---\ndef train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Validation loop ---\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Main function ---\ndef train_and_evaluate(**params):\n    logging.info(\"running with the following params :\")\n    logging.info(params)\n    # Load pretrained tokenizers and bert model\n    # update the paths to whichever you are using\n    tokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\n    model = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n\n    # Training parameters\n    epochs = int(params.get('epochs'))\n    batch_size = int(params.get('batch_size'))\n    learning_rate = float(params.get('learning_rate'))\n\n    #  Load the data\n    df_train = pd.read_csv(params.get('training_file'))\n    df_eval = pd.read_csv(params.get('validation_file'))\n    df_test = pd.read_csv(params.get('testing_file'))\n    # Create dataloaders\n    train_ds = BertDataset(df_train, tokenizer, max_length=100)\n    train_loader = DataLoader(dataset=train_ds,batch_size=batch_size, shuffle=True)\n    eval_ds = BertDataset(df_eval, tokenizer, max_length=100)\n    eval_loader = DataLoader(dataset=eval_ds,batch_size=batch_size)\n    test_ds = BertDataset(df_test, tokenizer, max_length=100)\n    test_loader = DataLoader(dataset=test_ds,batch_size=batch_size)\n\n    # Create the model\n    classifier = SentimentBERT(bert_model=model).to(DEVICE)\n    total_parameters = sum([np.prod(p.size()) for p in classifier.parameters()])\n    model_parameters = filter(lambda p: p.requires_grad, classifier.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    logging.info(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\")\n\n    # Optimizer and loss functions\n    optimizer = torch.optim.Adam([p for p in classifier.parameters() if p.requires_grad], learning_rate)\n    loss_fn = nn.CrossEntropyLoss()\n    # If dry run we only\n    logging.info(f'Training model with {BERT_MODEL_NAME}')\n\n    if args.dry_run:\n        logging.info(\"Dry run mode\")\n        epochs = 1\n        steps_per_epoch = 1\n    else:\n        steps_per_epoch = None\n\n    # Action !\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train_metrics = train(epoch, classifier, train_loader, loss_fn=loss_fn, optimizer=optimizer, max_steps=steps_per_epoch)\n        eval_metrics = evaluate(classifier, eval_loader, loss_fn=loss_fn)\n\n        print(\"-\" * 59)\n        print(\n            \"End of epoch {:3d} - time: {:5.2f}s - loss: {:.4f} - accuracy: {:.4f} - valid_loss: {:.4f} - valid accuracy {:.4f} \".format(\n                epoch, time.time() - epoch_start_time, train_metrics['loss'], train_metrics['acc'], eval_metrics['loss'], eval_metrics['acc']\n            )\n        )\n        print(\"-\" * 59)\n\n    if args.dry_run:\n        # If dry run, we do not run the evaluation\n        return None\n\n    test_metrics = evaluate(classifier, test_loader, loss_fn=loss_fn)\n\n    metrics = {\n        'train': train_metrics,\n        'val': eval_metrics,\n        'test': test_metrics,\n    }\n    logging.info(metrics)\n\n    # save model and architecture to single file\n    if params.get('job_dir') is None:\n        logging.warning(\"No job dir provided, model will not be saved\")\n    else:\n        logging.info(\"Saving model to {} \".format(params.get('job_dir')))\n        torch.save(classifier.state_dict(), params.get('job_dir'))\n    logging.info(\"Bye bye\")\n\n\nif __name__ == '__main__':\n    # Create arguments here\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--training-file', required=True, type=str)\n    parser.add_argument('--validation-file', required=True, type=str)\n    parser.add_argument('--testing-file', type=str)\n    parser.add_argument('--job-dir', type=str)\n    parser.add_argument('--epochs', type=float, default=2)\n    parser.add_argument('--batch-size', type=float, default=1024)\n    parser.add_argument('--learning-rate', type=float, default=0.01)\n    parser.add_argument('--dry-run', action=\"store_true\")\n    # Parse them\n    args, _ = parser.parse_known_args()\n    # Execute training\n    train_and_evaluate(**vars(args))\n</code></pre> <p>This is great, but unfortunately, this model will take a long time to train. Indeed, with around 4.7M parameters to train, one step will take around 3s on a 16Gb Macbook Pro with Intel chip.</p> <p></p> <p>3s per step can be quite long when you have 1238 steps to go and 10 epochs to complete\u2026</p> <p>No GPU, no party.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#how-to-use-vertex-ai-and-start-the-party","title":"How to use Vertex AI and start the party?","text":"<p>Short answer : Docker and gcloud.</p> <p>If you do not have a powerful GPU on your laptop (as most of us do), and/or want to avoid burning your laptop\u2019s cooling fan, you may want to move your script on a Cloud platform such as Google Cloud (disclaimer: I use Google Cloud at my job).</p> <p>The nice thing about Google is it offers 300$ in credits when you open your own project with your Gmail account.</p> <p>And as always, when it comes to transferring your code to somewhere else, Docker is usually the go-to solution.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#dockerizing-the-script","title":"Dockerizing the script","text":"<p>Let\u2019s write a Docker image with GPU enabled. There are a lot of Docker images you can find on the official Docker repository, I chose the pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime as I use a Pytorch 2.2.2 version. Be sure to select a version with CUDA, otherwise you will have to install it yourself in your Dockerfile, and trust me, you don\u2019t want to do that, except if you really have to.</p> <p>This Dockerfile will preinstall necessary CUDA dependencies and drivers and ensure we can use them in a custom training job, and run your python <code>main.py</code> file with the arguments that you will pass once you call the image.</p> <pre><code>FROM pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime\nWORKDIR /src\nCOPY . .\nRUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\nENTRYPOINT [\"python\", \"main.py\"]\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#building-and-pushing-an-image-to-google-cloud","title":"Building and pushing an image to Google Cloud","text":"<p>Once our image is ready to be built, we need to build it and push it to a registry. It can be on any registry you like, but Google Cloud offers a service for that called Artefact Registry. You will therefore be able to store your images on Google Cloud very easily.</p> <p>Write this little file at the root of your directory, and be sure that the Dockerfile is at the same level:</p> <pre><code># build.sh\nexport PROJECT_ID=&lt;your-project-id&gt;\nexport IMAGE_REPO_NAME=pt_bert_sentiment\nexport IMAGE_TAG=dev\nexport IMAGE_URI=eu.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\ngcloud builds submit --tag $IMAGE_URI .\n</code></pre> <p>Run the <code>build.sh</code> file, and after waiting a couple of minutes for the image to build, you should see something like: eu.gcr.io//pt_bert_sentiment:dev SUCCESS"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#creating-a-job-on-vertex-ai","title":"Creating a job on Vertex AI","text":"<p>Once your image has been built and pushed to Artefact Registry, we will now be able to tell Vertex AI to run this image on any machine we want, including ones with powerful GPUs ! Google offers a $300 credit when you create your own GCP project, it will be largely sufficient to run our model.</p> <p>Costs are available here. In our case, we will take the n1-standard-4 machine at $0.24/hr, and attach a NVIDIA T4 GPU at $0.40/hr.</p> <p></p> <p>Create a <code>job.sh</code> file as follows, by specifying which region you are in and what kind of machine you use. Refer to the link above if you are in a different region as costs may vary.</p> <p>You\u2019ll also need to pass arguments to your training script. The syntax for the <code>gcloud ai custom-jobs create</code> consists of 2 parts:</p> <ul> <li> <p>the arguments related to the job itself : <code>--region</code> , <code>--display-name</code> , <code>--worker-pool-spec</code> , <code>--service-account</code> , and <code>--args</code></p> </li> <li> <p>the arguments related to the training : <code>--training-file</code> , <code>--epochs</code> , etc.</p> </li> </ul> <p>The latter needs to be preceded by the <code>--args</code> to indicate that all following arguments are related to the training Python script.</p> <p>Ex: supposing our script takes 2 arguments x and y, we would have: <code>--args=x=1,y=2</code></p> <pre><code># job.sh\nexport PROJECT_ID=&lt;your-project-id&gt;\nexport BUCKET=&lt;your-bucket-id&gt;\nexport REGION=\"europe-west4\"\nexport SERVICE_ACCOUNT=&lt;your-service-account&gt;\nexport JOB_NAME=\"pytorch_bert_training\"\nexport MACHINE_TYPE=\"n1-standard-4\"  # We can specify GPUs here\nexport ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\nexport IMAGE_URI=\"eu.gcr.io/$PROJECT_ID/pt_bert_sentiment:dev\"\ngcloud ai custom-jobs create \\\n--region=$REGION \\\n--display-name=$JOB_NAME \\\n--worker-pool-spec=machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=1,replica-count=1,container-image-uri=$IMAGE_URI \\\n--service-account=$SERVICE_ACCOUNT \\\n--args=\\\n--training-file=gs://$BUCKET/data/train.csv,\\\n--validation-file=gs://$BUCKET/data/eval.csv,\\\n--testing-file=gs://$BUCKET/data/test.csv,\\\n--job-dir=gs://$BUCKET/model/model.pt,\\\n--epochs=10,\\\n--batch-size=128,\\\n--learning-rate=0.0001\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#running-the-job-on-vertex-ai","title":"Running the job on Vertex AI","text":"<p>Launch the script, and navigate to your GCP project, in the Training section under the Vertex menu .</p> <p></p> <p>Launch the script, and navigate to the console. You should see the job status as \u201cPending\u201d, and then \u201cTraining\u201d.</p> <p>To ensure the GPU is being used, you can check the job and its ressources :</p> <p></p> <p>This indicates that we are training with a GPU, we should therefore expect a significant speed-up now ! Let\u2019s have a look at the logs:</p> <p></p> <p>Less than 10 minutes to run 1 epoch, vs 1hr/epoch on CPU ! We have offloaded the training to Vertex and accelerated the training process. We could decide to launch other jobs with different configurations, without overloading our laptop\u2019s capabilities.</p> <p>What about the final accuracy of the model ? Well after 10 epochs, it is around 94\u201395%. We could let it run even longer and see if the score improves (we can also add an early stopping callback to avoid overfitting)</p> <p></p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#how-does-our-model-perform","title":"How does our model perform ?","text":"<p>Time to party !</p>"},{"location":"2018/11/29/Attention%20Seq2Seq%20with%20PyTorch%3A%20learning%20to%20invert%20a%20sequence/","title":"Seq2seq with pytorch","text":"<p>Work in progress</p>"},{"location":"2019/02/09/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8AExponential%20Smoothing%20and%20ARIMA%20processes/","title":"Time series part1","text":"<p>Work in progress</p>"},{"location":"2019/02/15/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%202%3A%20Dealing%20with%20seasonal%20data/","title":"Time series part2","text":"<p>Work in progress</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/","title":"Time series part3","text":"<p>Work in progress</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/","title":"Transformers guide","text":"<p>An end-to-end implementation of a Pytorch Transformer, in which we will cover key concepts such as self-attention, encoders, decoders, and much more.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#writing-our-own","title":"Writing our own","text":"<p>When I decided to dig deeper into Transformer architectures, I often felt frustrated when reading or watching tutorials online as I felt they always missed something :</p> <ul> <li>Official tutorials from Tensorflow or Pytorch used their own APIs, thus staying high-level and forcing me to have to go in their codebase to see what was under the hood. Very time-consuming and not always easy to read 1000s of lines of code.</li> <li>Other tutorials with custom code I found (links at the end of the article) often oversimplified use cases and didn\u2019t tackle concepts such as masking of variable-length sequence batch handling.</li> </ul> <p>I therefore decided to write my own Transformer to make sure I understood the concepts and be able to use it with any dataset.</p> <p>During this article, we will therefore follow a methodical approach in which we will implement a transformer layer by layer and block by block.</p> <p>There are obviously a lot of different implementations as well as high-level APIs from Pytorch or Tensorflow already available off the shelf, with \u2014 I am sure \u2014 better performance than the model we will build.</p> <p>\u201cOk, but why not use the TF/Pytorch implementations then\u201d ?</p> <p>The purpose of this article is educational, and I have no pretention in beating Pytorch or Tensorflow implementations. I do believe that the theory and the code behind transformers is not straightforward, that is why I hope that going through this step-by-step tutorial will allow you to have a better grasp over these concepts and feel more comfortable when building your own code later.</p> <p>Another reasons to build your own transformer from scratch is that it will allow you to fully understand how to use the above APIs. If we look at the Pytorch implementation of the <code>forward()</code> method of the Transformer class, you will see a lot of obscure keywords like :</p> <p> </p> <p>If you are already familiar with these keywords, then you can happily skip this article.</p> <p>Otherwise, this article will walk you through each of these keywords with the underlying concepts.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#a-very-short-introduction-to-transformers","title":"A very short introduction to Transformers","text":"<p>If you already heard about ChatGPT or Gemini, then you already met a transformer before. Actually, the \u201cT\u201d of ChatGPT stands for Transformer.</p> <p>The architecture was first coined in 2017 by Google researchers in the \u201cAttention is All you need\u201d paper. It is quite revolutionary as previous models used to do sequence-to-sequence learning (machine translation, speech-to-text, etc\u2026) relied on RNNs which were computationnally expensive in the sense they had to process sequences step by step, whereas Transformers only need to look once at the whole sequence, moving the time complexity from O(n) to O(1).</p> <p> </p> <p>Applications of transformers are quite large in the domain of NLP, and include language translation, question answering, document summarization, text generation, etc.</p> <p>The overall architecture of a transformer is as below:</p> <p> </p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#multi-head-attention","title":"Multi-head attention","text":"<p>The first block we will implement is actually the most important part of a Transformer, and is called the Multi-head Attention. Let\u2019s see where it sits in the overall architecture</p> <p> </p> <p>Attention is a mechanism which is actually not specific to transformers, and which was already used in RNN sequence-to-sequence models.</p> <p> </p> <p> </p> <pre><code>import torch\nimport torch.nn as nn\nimport math\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim=256, num_heads=4):\n        \"\"\"\n        input_dim: Dimensionality of the input.\n        num_heads: The number of attention heads to split the input into.\n        \"\"\"\n        super(MultiHeadAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        assert hidden_dim % num_heads == 0, \"Hidden dim must be divisible by num heads\"\n        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output layer\n\n\n    def check_sdpa_inputs(self, x):\n        assert x.size(1) == self.num_heads, f\"Expected size of x to be ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}), got {x.size()}\"\n        assert x.size(3) == self.hidden_dim // self.num_heads\n\n\n    def scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n\n        \"\"\"\n        self.check_sdpa_inputs(query)\n        self.check_sdpa_inputs(key)\n        self.check_sdpa_inputs(value)\n\n\n        d_k = query.size(-1)\n        tgt_len, src_len = query.size(-2), key.size(-2)\n\n\n        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n\n        # Attention mask here\n        if attention_mask is not None:\n            if attention_mask.dim() == 2:\n                assert attention_mask.size() == (tgt_len, src_len)\n                attention_mask = attention_mask.unsqueeze(0)\n                logits = logits + attention_mask\n            else:\n                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n\n\n        # Key mask here\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n            logits = logits + key_padding_mask\n\n\n        attention = torch.softmax(logits, dim=-1)\n        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n\n        return output, attention\n\n\n    def split_into_heads(self, x, num_heads):\n        batch_size, seq_length, hidden_dim = x.size()\n        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n\n        return x.transpose(1, 2) # Final dim will be (batch_size, num_heads, seq_length, , hidden_dim // num_heads)\n\n    def combine_heads(self, x):\n        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n\n\n    def forward(\n            self, \n            q, \n            k, \n            v, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        q : tensor of shape (batch_size, query_sequence_length, hidden_dim)\n        k : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        v : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, self.num_heads)\n        k = self.split_into_heads(k, self.num_heads)\n        v = self.split_into_heads(v, self.num_heads)\n\n        # attn_values, attn_weights = self.multihead_attn(q, k, v, attn_mask=attention_mask)\n        attn_values, attn_weights  = self.scaled_dot_product_attention(\n            query=q, \n            key=k, \n            value=v, \n            attention_mask=attention_mask,\n            key_padding_mask=key_padding_mask,\n        )\n        grouped = self.combine_heads(attn_values)\n        output = self.Wo(grouped)\n\n        self.attention_weigths = attn_weights\n\n        return output\n</code></pre> <p>We need to explain a few concepts here.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#1-queries-keys-and-values","title":"1) Queries, Keys and Values.","text":"<p>The query is the information you are trying to match, The key and values are the stored information.</p> <p>Think of that as using a dictionary : whenever using a Python dictionary, if your query doesn\u2019t match the dictionary keys, you won\u2019t be returned anything. But what if we want our dictionary to return a blend of information which are quite close ? Like if we had :</p> <pre><code>d = {\"panther\": 1, \"bear\": 10, \"dog\":3}\nd[\"wolf\"] = 0.2*d[\"panther\"] + 0.7*d[\"dog\"] + 0.1*d[\"bear\"]\n</code></pre> <p>This is basically what attention is about : looking at different parts of your data, and blend them to obtain a synthesis as an answer to your query.</p> <p>The relevant part of the code is this one, where we compute the attention weights between the query and the keys</p> <pre><code>logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # we compute the weights of attention\n</code></pre> <p>And this one, where we apply the normalized weights to the values :</p> <pre><code>attention = torch.softmax(logits, dim=-1)\noutput = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#2-attention-masking-and-padding","title":"2) Attention masking and padding","text":"<p>When attending to parts of a sequential input, we do not want to include useless or forbidden information.</p> <p>Useless information is for example padding: padding symbols, used to align all sequences in a batch to the same sequence size, should be ignored by our model. We will come back to that in the last section</p> <p>Forbidden information is a bit more complex. When being trained, a model learns to encode the input sequence, and align targets to the inputs. However, as the inference process involves looking at previously emitted tokens to predict the next one (think of text generation in ChatGPT), we need to apply the same rules during training.</p> <p>This is why we apply a causal mask to ensure that the targets, at each time step, can only see information from the past. Here is the corresponding section where the mask is applied (computing the mask is covered at the end)</p> <pre><code>if attention_mask is not None:\n    if attention_mask.dim() == 2:\n        assert attention_mask.size() == (tgt_len, src_len)\n        attention_mask = attention_mask.unsqueeze(0)\n        logits = logits + attention_mask\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#positional-encoding","title":"Positional Encoding","text":"<p>It corresponds to the following part of the Transformer:</p> <p> </p> <p>When receiving and treating an input, a transformer has no sense of order as it looks at the sequence as a whole, in opposition to what RNNs do. We therefore need to add a hint of temporal order so that the transformer can learn dependencies.</p> <p>The specific details of how positional encoding works is out of scope for this article, but feel free to read the original paper to understand.</p> <pre><code># Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#encoders","title":"Encoders","text":"<p>We are getting close to having a full encoder working ! The encoder is the left part of the Transformer.</p> <p> </p> <p>We will add a small part to our code, which is the Feed Forward part :</p> <pre><code>class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n</code></pre> <p>Putting the pieces together, we get an Encoder module !</p> <pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(EncoderBlock, self).__init__()\n        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm2 = nn.LayerNorm(n_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, src_padding_mask=None):\n        assert x.ndim==3, \"Expected input to be 3-dim, got {}\".format(x.ndim)\n        att_output = self.mha(x, x, x, key_padding_mask=src_padding_mask)\n        x = x + self.dropout(self.norm1(att_output))\n\n        ff_output = self.ff(x)\n        output = x + self.norm2(ff_output)\n\n        return output\n</code></pre> <p>As shown in the diagram, the Encoder actually contains N Encoder blocks or layers, as well as an Embedding layer for our inputs. Let\u2019s therefore create an Encoder by adding the Embedding, the Positional Encoding and the Encoder blocks:</p> <pre><code>class Encoder(nn.Module):\n    def __init__(\n            self, \n            vocab_size: int, \n            n_dim: int, \n            dropout: float, \n            n_encoder_blocks: int,\n            n_heads: int):\n\n        super(Encoder, self).__init__()\n        self.n_dim = n_dim\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )    \n        self.encoder_blocks = nn.ModuleList([\n            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)\n        ])\n\n\n    def forward(self, x, padding_mask=None):\n        x = self.embedding(x) * math.sqrt(self.n_dim)\n        x = self.positional_encoding(x)\n        for block in self.encoder_blocks:\n            x = block(x=x, src_padding_mask=padding_mask)\n        return x\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#decoders","title":"Decoders","text":"<p>The decoder part is the part on the left and requires a bit more crafting.</p> <p> </p> <p>There is something called Masked Multi-Head Attention. Remember what we said before about causal mask ? Well this happens here. We will use the attention_mask parameter of our Multi-head attention module to represent this (more details about how we compute the mask at the end) :</p> <pre><code># Stuff before\n\nself.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\nmasked_att_output = self.self_attention(\n    q=tgt, \n    k=tgt, \n    v=tgt, \n    attention_mask=tgt_mask, &lt;-- HERE IS THE CAUSAL MASK\n    key_padding_mask=tgt_padding_mask)\n\n# Stuff after\n</code></pre> <p>The second attention is called cross-attention. It will uses the decoder\u2019s query to match with the encoder\u2019s key &amp; values ! Beware : they can have different lengths during training, so it is usually a good practice to define clearly the expected shapes of inputs as follows :</p> <pre><code>def scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n</code></pre> <p>And here is the part where we use the encoder\u2019s output, called memory, with our decoder input :</p> <pre><code># Stuff before\nself.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\ncross_att_output = self.cross_attention(\n        q=x1, \n        k=memory, \n        v=memory, \n        attention_mask=None,  &lt;-- NO CAUSAL MASK HERE\n        key_padding_mask=memory_padding_mask) &lt;-- WE NEED TO USE THE PADDING OF THE SOURCE\n# Stuff after\n</code></pre> <p>Putting the pieces together, we end up with this for the Decoder :</p> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(DecoderBlock, self).__init__()\n\n        # The first Multi-Head Attention has a mask to avoid looking at the future\n        self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n\n        # The second Multi-Head Attention will take inputs from the encoder as key/value inputs\n        self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm2 = nn.LayerNorm(n_dim)\n\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm3 = nn.LayerNorm(n_dim)\n        # self.dropout = nn.Dropout(dropout)\n\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n\n        masked_att_output = self.self_attention(\n            q=tgt, k=tgt, v=tgt, attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)\n        x1 = tgt + self.norm1(masked_att_output)\n\n        cross_att_output = self.cross_attention(\n            q=x1, k=memory, v=memory, attention_mask=None, key_padding_mask=memory_padding_mask)\n        x2 = x1 + self.norm2(cross_att_output)\n\n        ff_output = self.ff(x2)\n        output = x2 + self.norm3(ff_output)\n\n\n        return output\n\nclass Decoder(nn.Module):\n    def __init__(\n        self, \n        vocab_size: int, \n        n_dim: int, \n        dropout: float, \n        n_decoder_blocks: int,\n        n_heads: int):\n\n        super(Decoder, self).__init__()\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim,\n            padding_idx=0\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )\n\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)\n        ])\n\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n        x = self.embedding(tgt)\n        x = self.positional_encoding(x)\n\n        for block in self.decoder_blocks:\n            x = block(\n                x, \n                memory, \n                tgt_mask=tgt_mask, \n                tgt_padding_mask=tgt_padding_mask, \n                memory_padding_mask=memory_padding_mask)\n        return x\n</code></pre> <p>Padding &amp; Masking Remember the Multi-head attention section where we mentionned excluding certain parts of the inputs when doing attention.</p> <p>During training, we consider batches of inputs and targets, wherein each instance may have a variable length. Consider the following example where we batch 4 words : banana, watermelon, pear, blueberry. In order to process them as a single batch, we need to align all words to the length of the longest word (watermelon). We will therefore add an extra token, PAD, to each word so they all end up with the same length as watermelon.</p> <p>In the below picture, the upper table represents the raw data, the lower table the encoded version:</p> <p> </p> <p>In our case, we want to exclude padding indices from the attention weights being calculated. We can therefore compute a mask as follows, both for source and target data :</p> <pre><code>padding_mask = (x == PAD_IDX)\n</code></pre> <p>What about causal masks now ? Well if we want, at each time step, that the model can attend only steps in the past, this means that for each time step T, the model can only attend to each step t for t in 1\u2026T. It is a double for loop, we can therefore use a matrix to compute that :</p> <p> </p> <pre><code>def generate_square_subsequent_mask(size: int):\n      \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n      mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n      return mask\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#case-study-a-word-reverse-transformer","title":"Case study : a Word-Reverse Transformer","text":"<p>Let\u2019s now build our Transformer by bringing parts together !</p> <p>In our use case, we will use a very simple dataset to showcase how Transformers actually learn.</p> <p>\u201cBut why use a Transformer to reverse words ? I already know how to do that in Python with word[::-1] !\u201d</p> <p>The objective here is to see whether the Transformer attention mechanism works. What we expect is to see attention weights to move from right to left when given an input sequence. If so, this means our Transformer has learned a very simple grammar, which is just reading from right to left, and could generalize to more complex grammars when doing real-life language translation.</p> <p>Let\u2019s first begin with our custom Transformer class :</p> <pre><code>import torch\nimport torch.nn as nn\nimport math\n\nfrom .encoder import Encoder\nfrom .decoder import Decoder\n\n\nclass Transformer(nn.Module):\n    def __init__(self, **kwargs):\n        super(Transformer, self).__init__()\n\n        for k, v in kwargs.items():\n            print(f\" * {k}={v}\")\n\n        self.vocab_size = kwargs.get('vocab_size')\n        self.model_dim = kwargs.get('model_dim')\n        self.dropout = kwargs.get('dropout')\n        self.n_encoder_layers = kwargs.get('n_encoder_layers')\n        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n        self.n_heads = kwargs.get('n_heads')\n        self.batch_size = kwargs.get('batch_size')\n        self.PAD_IDX = kwargs.get('pad_idx', 0)\n\n        self.encoder = Encoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_encoder_layers, self.n_heads)\n        self.decoder = Decoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_decoder_layers, self.n_heads)\n        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n\n\n    @staticmethod    \n    def generate_square_subsequent_mask(size: int):\n            \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n            mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n            return mask\n\n\n    def encode(\n            self, \n            x: torch.Tensor, \n        ) -&gt; torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, S) with elements in (0, C) where C is num_classes\n        Output\n            (B, S, E) embedding\n        \"\"\"\n\n        mask = (x == self.PAD_IDX).float()\n        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        # (B, S, E)\n        encoder_output = self.encoder(\n            x, \n            padding_mask=encoder_padding_mask\n        )  \n\n        return encoder_output, encoder_padding_mask\n\n\n    def decode(\n            self, \n            tgt: torch.Tensor, \n            memory: torch.Tensor, \n            memory_padding_mask=None\n        ) -&gt; torch.Tensor:\n        \"\"\"\n        B = Batch size\n        S = Source sequence length\n        L = Target sequence length\n        E = Model dimension\n\n        Input\n            encoded_x: (B, S, E)\n            y: (B, L) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        mask = (tgt == self.PAD_IDX).float()\n        tgt_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        decoder_output = self.decoder(\n            tgt=tgt, \n            memory=memory, \n            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)), \n            tgt_padding_mask=tgt_padding_mask, \n            memory_padding_mask=memory_padding_mask,\n        )  \n        output = self.fc(decoder_output)  # shape (B, L, C)\n        return output\n\n\n\n    def forward(\n            self, \n            x: torch.Tensor, \n            y: torch.Tensor, \n        ) -&gt; torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, Sx) with elements in (0, C) where C is num_classes\n            y: (B, Sy) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        # Encoder output shape (B, S, E)\n        encoder_output, encoder_padding_mask = self.encode(x)  \n\n        # Decoder output shape (B, L, C)\n        decoder_output = self.decode(\n            tgt=y, \n            memory=encoder_output, \n            memory_padding_mask=encoder_padding_mask\n        )  \n\n        return decoder_output\n</code></pre>"},{"location":"2024/12/31/Ground%20your%20chatbot%20with%20function%20calling/","title":"Wip function calling rag","text":"<p>Work in progress</p>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"category/vertex/","title":"vertex","text":""},{"location":"category/gcp/","title":"gcp","text":""},{"location":"category/ai/","title":"ai","text":""},{"location":"category/bayesian/","title":"bayesian","text":""},{"location":"category/timeseries/","title":"timeseries","text":""}]}