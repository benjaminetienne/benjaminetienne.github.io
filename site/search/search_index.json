{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"notebooks/GenAI/Agents/","title":"Agents","text":"In\u00a0[24]: Copied! <pre>import requests\nimport vertexai\nimport os\n\nvertexai.init(project=os.environ.get('GCP_PROJECT_ID'))\n\nfrom google.cloud import bigquery\n\nfrom vertexai.generative_models import (\n    GenerativeModel,\n    GenerationConfig,\n    Part,\n    FunctionDeclaration\n)\nfrom vertexai.generative_models import Tool as VertexTool\n</pre> import requests import vertexai import os  vertexai.init(project=os.environ.get('GCP_PROJECT_ID'))  from google.cloud import bigquery  from vertexai.generative_models import (     GenerativeModel,     GenerationConfig,     Part,     FunctionDeclaration ) from vertexai.generative_models import Tool as VertexTool In\u00a0[2]: Copied! <pre>PROJECT_ID = 'bigquery-public-data'\nDATASET_ID = 'thelook_ecommerce'\n</pre> PROJECT_ID = 'bigquery-public-data' DATASET_ID = 'thelook_ecommerce' In\u00a0[5]: Copied! <pre>def get_exchange_rate_from_api(params):\n    url = f\"https://api.frankfurter.app/latest?from={params['currency_from']}&amp;to={params['currency_to']}\"\n    api_response = requests.get(url)\n    return api_response.text\n</pre> def get_exchange_rate_from_api(params):     url = f\"https://api.frankfurter.app/latest?from={params['currency_from']}&amp;to={params['currency_to']}\"     api_response = requests.get(url)     return api_response.text In\u00a0[6]: Copied! <pre>get_exchange_rate_from_api({'currency_from': 'USD', 'currency_to': 'EUR'})\n</pre> get_exchange_rate_from_api({'currency_from': 'USD', 'currency_to': 'EUR'}) Out[6]: <pre>'{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-22\",\"rates\":{\"EUR\":0.96043}}'</pre> In\u00a0[7]: Copied! <pre>def list_datasets(params):\n    client = bigquery.Client(project=params['project'])\n    datasets = list(client.list_datasets())\n    if datasets:\n        return [dataset.dataset_id for dataset in datasets]\n    else:\n        return \"{} project does not contain any datasets.\".format(params['project'])\n</pre> def list_datasets(params):     client = bigquery.Client(project=params['project'])     datasets = list(client.list_datasets())     if datasets:         return [dataset.dataset_id for dataset in datasets]     else:         return \"{} project does not contain any datasets.\".format(params['project']) In\u00a0[12]: Copied! <pre>list_datasets({'project': PROJECT_ID})[:10]\n</pre> list_datasets({'project': PROJECT_ID})[:10] Out[12]: <pre>['america_health_rankings',\n 'austin_311',\n 'austin_bikeshare',\n 'austin_crime',\n 'austin_incidents',\n 'austin_waste',\n 'baseball',\n 'bbc_news',\n 'bigqueryml_ncaa',\n 'bitcoin_blockchain']</pre> In\u00a0[13]: Copied! <pre>def list_tables(params):\n    client = bigquery.Client(project=params['project'])\n    try:\n        response = client.list_tables(params[\"dataset_id\"])\n        return [table.table_id for table in response]\n    except Exception as e:\n        return f\"The dataset {params['dataset_id']} is not found in the {params['project']} project, please specify the dataset and project\"\n</pre> def list_tables(params):     client = bigquery.Client(project=params['project'])     try:         response = client.list_tables(params[\"dataset_id\"])         return [table.table_id for table in response]     except Exception as e:         return f\"The dataset {params['dataset_id']} is not found in the {params['project']} project, please specify the dataset and project\" In\u00a0[14]: Copied! <pre>list_tables({'project': PROJECT_ID, 'dataset_id': DATASET_ID})\n</pre> list_tables({'project': PROJECT_ID, 'dataset_id': DATASET_ID}) Out[14]: <pre>['distribution_centers',\n 'events',\n 'inventory_items',\n 'order_items',\n 'orders',\n 'products',\n 'users']</pre> In\u00a0[15]: Copied! <pre># Function declarations\nget_exchange_rate_func = FunctionDeclaration(\n    name=\"get_exchange_rate\",\n    description=\"Get the exchange rate for currencies between countries\",\n    parameters={\n    \"type\": \"object\",\n    \"properties\": {\n        \"currency_date\": {\n            \"type\": \"string\",\n            \"description\": \"A date that must always be in YYYY-MM-DD format or the value 'latest' if a time period is not specified\"\n        },\n        \"currency_from\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert from in ISO 4217 format\"\n        },\n        \"currency_to\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert to in ISO 4217 format\"\n        }\n    },\n         \"required\": [\n            \"currency_from\",\n            \"currency_to\",\n      ]\n  },\n)\n\nlist_datasets_func = FunctionDeclaration(\n    name=\"list_datasets\",\n    description=\"Get a list of datasets in a project that will help answer the user's question\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"project\": {\n                \"type\": \"string\",\n                \"description\": \"Project ID to fetch tables from.\",\n            }\n        },\n    },\n)\n\nlist_tables_func = FunctionDeclaration(\n    name=\"list_tables\",\n    description=\"List tables in a dataset that will help answer the user's question\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"dataset_id\": {\n                \"type\": \"string\",\n                \"description\": \"Dataset ID to fetch tables from.\",\n            },\n            \"project\": {\n                \"type\": \"string\",\n                \"description\": \"Project ID to fetch tables from.\",\n            }\n        },\n        \"required\": [\n            \"dataset_id\",\n            \"project\"\n        ],\n    },\n)\n\ntool = VertexTool(\n    function_declarations=[\n        get_exchange_rate_func,\n        list_datasets_func,\n        list_tables_func\n    ]\n)\n\n# Mapping to map function name to function\nfunction_handler = {\n    \"get_exchange_rate\": get_exchange_rate_from_api,\n    \"list_datasets\": list_datasets,\n    \"list_tables\": list_tables,\n}\n</pre> # Function declarations get_exchange_rate_func = FunctionDeclaration(     name=\"get_exchange_rate\",     description=\"Get the exchange rate for currencies between countries\",     parameters={     \"type\": \"object\",     \"properties\": {         \"currency_date\": {             \"type\": \"string\",             \"description\": \"A date that must always be in YYYY-MM-DD format or the value 'latest' if a time period is not specified\"         },         \"currency_from\": {             \"type\": \"string\",             \"description\": \"The currency to convert from in ISO 4217 format\"         },         \"currency_to\": {             \"type\": \"string\",             \"description\": \"The currency to convert to in ISO 4217 format\"         }     },          \"required\": [             \"currency_from\",             \"currency_to\",       ]   }, )  list_datasets_func = FunctionDeclaration(     name=\"list_datasets\",     description=\"Get a list of datasets in a project that will help answer the user's question\",     parameters={         \"type\": \"object\",         \"properties\": {             \"project\": {                 \"type\": \"string\",                 \"description\": \"Project ID to fetch tables from.\",             }         },     }, )  list_tables_func = FunctionDeclaration(     name=\"list_tables\",     description=\"List tables in a dataset that will help answer the user's question\",     parameters={         \"type\": \"object\",         \"properties\": {             \"dataset_id\": {                 \"type\": \"string\",                 \"description\": \"Dataset ID to fetch tables from.\",             },             \"project\": {                 \"type\": \"string\",                 \"description\": \"Project ID to fetch tables from.\",             }         },         \"required\": [             \"dataset_id\",             \"project\"         ],     }, )  tool = VertexTool(     function_declarations=[         get_exchange_rate_func,         list_datasets_func,         list_tables_func     ] )  # Mapping to map function name to function function_handler = {     \"get_exchange_rate\": get_exchange_rate_from_api,     \"list_datasets\": list_datasets,     \"list_tables\": list_tables, } <p>Tools can also be created this way but this implies having docstrings in your functions</p> In\u00a0[25]: Copied! <pre>gemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n    # tools=[tool]\n)\nchat = gemini_model.start_chat()\n</pre> gemini_model = GenerativeModel(     \"gemini-1.5-flash\",     generation_config=GenerationConfig(temperature=0),     # tools=[tool] ) chat = gemini_model.start_chat() In\u00a0[27]: Copied! <pre>response = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\")\nresponse.candidates[0].content.parts[0].text\n</pre> response = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\") response.candidates[0].content.parts[0].text Out[27]: <pre>'I do not have access to real-time information, including live exchange rates. \\n\\nTo get the most up-to-date USD to EUR exchange rate, I recommend checking a reliable financial website or using a currency converter app. \\n\\nHere are some popular options:\\n\\n* **Google Finance:** Simply search \"USD to EUR\" on Google.\\n* **XE.com:** A dedicated currency converter website.\\n* **Bloomberg:** A financial news and data provider.\\n* **Yahoo Finance:** Another popular financial website.\\n\\nRemember that exchange rates fluctuate constantly, so the rate you see at one moment may be different just a few minutes later. \\n'</pre> <p>Conclusion : Without tool, no answer</p> In\u00a0[14]: Copied! <pre>gemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n    tools=[tool]\n)\nchat = gemini_model.start_chat()\n</pre> gemini_model = GenerativeModel(     \"gemini-1.5-flash\",     generation_config=GenerationConfig(temperature=0),     tools=[tool] ) chat = gemini_model.start_chat() In\u00a0[15]: Copied! <pre>response = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\")\n\n# Extract the function call response\nfunction_call = response.candidates[0].content.parts[0].function_call\nfunction_call\n</pre> response = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\")  # Extract the function call response function_call = response.candidates[0].content.parts[0].function_call function_call Out[15]: <pre>name: \"get_exchange_rate\"\nargs {\n  fields {\n    key: \"currency_to\"\n    value {\n      string_value: \"EUR\"\n    }\n  }\n  fields {\n    key: \"currency_from\"\n    value {\n      string_value: \"USD\"\n    }\n  }\n  fields {\n    key: \"currency_date\"\n    value {\n      string_value: \"latest\"\n    }\n  }\n}</pre> In\u00a0[16]: Copied! <pre>prompt = f\"What is the current exchange rate for USD vs EUR ?\"\n\nresponse = chat.send_message(prompt)\n\n# Extract the function call response\nfunction_call = response.candidates[0].content.parts[0].function_call\n\n# Check for a function call or a natural language response\nif function_call.name in function_handler.keys():\n    # Extract the function call name\n    function_name = function_call.name\n    print(\"#### Predicted function name\")\n    print(function_name, \"\\n\")\n    # msg.content = f'I think I need to use the `{function_name}` tool'\n    #\u00a0await msg.update()\n\n    # Extract the function call parameters\n    params = {key: value for key, value in function_call.args.items()}\n    print(\"#### Predicted function parameters\")\n    print(params, \"\\n\")\n\n    function_api_response = function_handler[function_name](params)\n    print(\"#### API response\")\n    print(function_api_response)\n    response = chat.send_message(\n        Part.from_function_response(\n            name=function_name,\n            response={\"content\": function_api_response},\n        ),\n    )   \n    print(\"\\n#### Final Answer\")\n    print(response.candidates[0].content.parts[0].text)\n</pre> prompt = f\"What is the current exchange rate for USD vs EUR ?\"  response = chat.send_message(prompt)  # Extract the function call response function_call = response.candidates[0].content.parts[0].function_call  # Check for a function call or a natural language response if function_call.name in function_handler.keys():     # Extract the function call name     function_name = function_call.name     print(\"#### Predicted function name\")     print(function_name, \"\\n\")     # msg.content = f'I think I need to use the `{function_name}` tool'     #\u00a0await msg.update()      # Extract the function call parameters     params = {key: value for key, value in function_call.args.items()}     print(\"#### Predicted function parameters\")     print(params, \"\\n\")      function_api_response = function_handler[function_name](params)     print(\"#### API response\")     print(function_api_response)     response = chat.send_message(         Part.from_function_response(             name=function_name,             response={\"content\": function_api_response},         ),     )        print(\"\\n#### Final Answer\")     print(response.candidates[0].content.parts[0].text) <pre>#### Predicted function name\nget_exchange_rate \n\n#### Predicted function parameters\n{'currency_from': 'USD', 'currency_date': 'latest', 'currency_to': 'EUR'} \n\n#### API response\n{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}\n</pre> <pre>/Users/benjamin.etienne/Library/Caches/pypoetry/virtualenvs/london-xwvFazzc-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.frankfurter.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n</pre> <pre>\n#### Final Answer\nThe current exchange rate for USD vs EUR is 0.94679. This means that 1 USD is equal to 0.94679 EUR. \n\n</pre> In\u00a0[37]: Copied! <pre>from langchain_google_vertexai import VertexAI\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.agents import Tool as LangchainTool\nfrom langchain_core.tools import tool\nfrom langchain.memory import ConversationBufferMemory\n</pre> from langchain_google_vertexai import VertexAI from langchain.agents import AgentType, initialize_agent from langchain.agents import Tool as LangchainTool from langchain_core.tools import tool from langchain.memory import ConversationBufferMemory In\u00a0[38]: Copied! <pre>@tool\ndef list_datasets(project: str) -&gt; list:\n    \"\"\"\n    Return a list of Bigquery datasets\n    Args:\n        project: GCP project id\n    \"\"\"\n    client = bigquery.Client(project=project)\n    datasets = list(client.list_datasets())\n    if datasets:\n        return [dataset.dataset_id for dataset in datasets]\n    else:\n        return \"{} project does not contain any datasets.\".format(project)\n\n@tool\ndef list_tables(project: str, dataset_id: str) -&gt; list:\n    \"\"\"\n    Return a list of Bigquery tables\n    Args:\n        project: GCP project id\n        dataset_id: ID of the dataset\n    \"\"\"\n    client = bigquery.Client(project=project)\n    try:\n        response = client.list_tables(dataset_id)\n        return [table.table_id for table in response]\n    except Exception as e:\n        return f\"The dataset {dataset_id} is not found in the {project} project, please specify the dataset and project\"\n\n@tool\ndef get_exchange_rate_from_api(currency_from: str, currency_to: str) -&gt; str:\n    \"\"\"\n    Return the exchange rate between currencies\n    Args:\n        currency_from: str\n        currency_to: str\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&amp;to={currency_to}\"\n    api_response = requests.get(url, verify=False)\n    return api_response.text\n</pre> @tool def list_datasets(project: str) -&gt; list:     \"\"\"     Return a list of Bigquery datasets     Args:         project: GCP project id     \"\"\"     client = bigquery.Client(project=project)     datasets = list(client.list_datasets())     if datasets:         return [dataset.dataset_id for dataset in datasets]     else:         return \"{} project does not contain any datasets.\".format(project)  @tool def list_tables(project: str, dataset_id: str) -&gt; list:     \"\"\"     Return a list of Bigquery tables     Args:         project: GCP project id         dataset_id: ID of the dataset     \"\"\"     client = bigquery.Client(project=project)     try:         response = client.list_tables(dataset_id)         return [table.table_id for table in response]     except Exception as e:         return f\"The dataset {dataset_id} is not found in the {project} project, please specify the dataset and project\"  @tool def get_exchange_rate_from_api(currency_from: str, currency_to: str) -&gt; str:     \"\"\"     Return the exchange rate between currencies     Args:         currency_from: str         currency_to: str     \"\"\"     url = f\"https://api.frankfurter.app/latest?from={currency_from}&amp;to={currency_to}\"     api_response = requests.get(url, verify=False)     return api_response.text In\u00a0[39]: Copied! <pre>#\u00a0langchain_tool = [\n#\u00a0    LangchainTool(\n#\u00a0        name='list_datasets',\n#\u00a0        func=list_datasets,\n#\u00a0        description=list_datasets.description,\n#\u00a0    ),\n#\u00a0    LangchainTool(\n#\u00a0        name='list_tables',\n#\u00a0        func=list_tables,\n#\u00a0        description=list_tables.description,\n#\u00a0    ),\n#\u00a0    LangchainTool(\n#\u00a0        name='get_exchange_rate',\n#\u00a0        func=get_exchange_rate_from_api,\n#\u00a0        description=get_exchange_rate_from_api.description\n#\u00a0    )\n#\u00a0]\n\nlangchain_tool = [\n    list_datasets,\n    list_tables,\n    get_exchange_rate_from_api\n]\n</pre> #\u00a0langchain_tool = [ #\u00a0    LangchainTool( #\u00a0        name='list_datasets', #\u00a0        func=list_datasets, #\u00a0        description=list_datasets.description, #\u00a0    ), #\u00a0    LangchainTool( #\u00a0        name='list_tables', #\u00a0        func=list_tables, #\u00a0        description=list_tables.description, #\u00a0    ), #\u00a0    LangchainTool( #\u00a0        name='get_exchange_rate', #\u00a0        func=get_exchange_rate_from_api, #\u00a0        description=get_exchange_rate_from_api.description #\u00a0    ) #\u00a0]  langchain_tool = [     list_datasets,     list_tables,     get_exchange_rate_from_api ] In\u00a0[6]: Copied! <pre>from langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_google_vertexai import ChatVertexAI\n</pre> from langchain.agents import AgentExecutor, create_tool_calling_agent from langchain_core.prompts import ChatPromptTemplate from langchain_google_vertexai import ChatVertexAI In\u00a0[7]: Copied! <pre>gemini_llm = ChatVertexAI(model=\"gemini-1.5-flash\")\n</pre> gemini_llm = ChatVertexAI(model=\"gemini-1.5-flash\") In\u00a0[36]: Copied! <pre>prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant\"),\n        (\"human\", \"{input}\"),\n        # Placeholders fill up a **list** of messages\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n\n\nagent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\nagent_executor.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"})\n</pre> prompt = ChatPromptTemplate.from_messages(     [         (\"system\", \"You are a helpful assistant\"),         (\"human\", \"{input}\"),         # Placeholders fill up a **list** of messages         (\"placeholder\", \"{agent_scratchpad}\"),     ] )   agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt) agent_executor = AgentExecutor(agent=agent, tools=langchain_tool) agent_executor.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"}) Out[36]: <pre>{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'output': 'I am sorry, I cannot find the dataset `thelook_ecommerce` in the project `gcp-project-id`. Please check if the dataset name and project id are correct. \\n'}</pre> In\u00a0[37]: Copied! <pre>agent_executor.invoke({\"input\": f\"Project id is {PROJECT_ID}\"})\n</pre> agent_executor.invoke({\"input\": f\"Project id is {PROJECT_ID}\"}) Out[37]: <pre>{'input': 'Project id is bigquery-public-data',\n 'output': 'OK. What else can I do for you? \\n'}</pre> In\u00a0[38]: Copied! <pre>agent_executor.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"})\n</pre> agent_executor.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"}) Out[38]: <pre>{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'output': 'I am sorry, I cannot find the dataset \"thelook_ecommerce\" in your project. Please double check the dataset name and project ID and try again. \\n'}</pre> In\u00a0[46]: Copied! <pre>from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n</pre> from langchain_core.chat_history import InMemoryChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory In\u00a0[9]: Copied! <pre>memory = InMemoryChatMessageHistory(session_id=\"foo\")\n</pre> memory = InMemoryChatMessageHistory(session_id=\"foo\") In\u00a0[10]: Copied! <pre>prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful assistant.\"),\n        # First put the history\n        (\"placeholder\", \"{chat_history}\"),\n        # Then the new input\n        (\"human\", \"{input}\"),\n        # Finally the scratchpad\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n</pre> prompt = ChatPromptTemplate.from_messages(     [         (\"system\", \"You are a helpful assistant.\"),         # First put the history         (\"placeholder\", \"{chat_history}\"),         # Then the new input         (\"human\", \"{input}\"),         # Finally the scratchpad         (\"placeholder\", \"{agent_scratchpad}\"),     ] ) In\u00a0[41]: Copied! <pre>agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    # This is needed because in most real world scenarios, a session id is needed\n    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\n</pre> agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt) agent_executor = AgentExecutor(agent=agent, tools=langchain_tool)  agent_with_chat_history = RunnableWithMessageHistory(     agent_executor,     # This is needed because in most real world scenarios, a session id is needed     # It isn't really used here because we are using a simple in memory ChatMessageHistory     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", )  config = {\"configurable\": {\"session_id\": \"foo\"}} In\u00a0[42]: Copied! <pre>agent_with_chat_history.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"}, config)\n</pre> agent_with_chat_history.invoke({\"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"}, config) Out[42]: <pre>{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'chat_history': [],\n 'output': 'The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. Please specify the correct dataset and project. \\n'}</pre> In\u00a0[13]: Copied! <pre>agent_with_chat_history.invoke({\"input\": f\"Project id is {PROJECT_ID}\"}, config)\n</pre> agent_with_chat_history.invoke({\"input\": f\"Project id is {PROJECT_ID}\"}, config) Out[13]: <pre>{'input': 'Project id is bigquery-public-data',\n 'chat_history': [HumanMessage(content='Which tables are available in the thelook_ecommerce dataset ?'),\n  AIMessage(content='I am sorry, I cannot find the dataset thelook_ecommerce in your project. Please check if the dataset name is correct and if the dataset is available in your project. \\n')],\n 'output': 'The tables available in the thelook_ecommerce dataset are: distribution_centers, events, inventory_items, order_items, orders, products, users. \\n'}</pre> <p>First we need to bind the tools to our LLM</p> In\u00a0[40]: Copied! <pre>gemini_with_tools = gemini_llm.bind_tools(langchain_tool)\n</pre> gemini_with_tools = gemini_llm.bind_tools(langchain_tool) <p>Then we create a chain, which will be wrapped in a Runnable</p> In\u00a0[45]: Copied! <pre>chain = prompt | gemini_with_tools\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    # Uses the get_by_session_id function defined in the example\n    # above.\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nresp = chain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\nresp\n</pre> chain = prompt | gemini_with_tools memory = InMemoryChatMessageHistory(session_id=\"foo\") config = {\"configurable\": {\"session_id\": \"foo\"}} chain_with_history = RunnableWithMessageHistory(     chain,     # Uses the get_by_session_id function defined in the example     # above.     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", )  resp = chain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config) resp Out[45]: <pre>AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_exchange_rate_from_api', 'arguments': '{\"currency_from\": \"CHF\", \"currency_to\": \"EUR\"}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_LOW'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 185, 'candidates_token_count': 17, 'total_token_count': 202}}, id='run-9ad6b7e3-1314-442b-be35-e3b6d9864c3c-0', tool_calls=[{'name': 'get_exchange_rate_from_api', 'args': {'currency_from': 'CHF', 'currency_to': 'EUR'}, 'id': '1db02e1a-bf04-4968-bc06-f32fe5eb8d0f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 185, 'output_tokens': 17, 'total_tokens': 202})</pre> <p>Are we done ? No. The LLM correcty guessed the function to call, but we need to execute the function now !</p> In\u00a0[47]: Copied! <pre>from langchain_core.messages import AIMessage\n\ndef call_tools(msg: AIMessage) -&gt; list[dict]:\n    \"\"\"Simple sequential tool calling helper.\"\"\"\n    tool_map = {tool.name: tool for tool in langchain_tool}\n    tool_calls = msg.tool_calls.copy()\n    for tool_call in tool_calls:\n        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n    return tool_calls\n\nchain = prompt | gemini_with_tools | call_tools\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    # Uses the get_by_session_id function defined in the example\n    # above.\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nchain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n</pre> from langchain_core.messages import AIMessage  def call_tools(msg: AIMessage) -&gt; list[dict]:     \"\"\"Simple sequential tool calling helper.\"\"\"     tool_map = {tool.name: tool for tool in langchain_tool}     tool_calls = msg.tool_calls.copy()     for tool_call in tool_calls:         tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])     return tool_calls  chain = prompt | gemini_with_tools | call_tools memory = InMemoryChatMessageHistory(session_id=\"foo\") chain_with_history = RunnableWithMessageHistory(     chain,     # Uses the get_by_session_id function defined in the example     # above.     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", )  chain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config) <pre>/Users/benjamin.etienne/Library/Caches/pypoetry/virtualenvs/london-xwvFazzc-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.frankfurter.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n</pre> Out[47]: <pre>[{'name': 'get_exchange_rate_from_api',\n  'args': {'currency_from': 'CHF', 'currency_to': 'EUR'},\n  'id': '81bc85ea-dfd4-4c01-85e8-f3ca592fff5b',\n  'type': 'tool_call',\n  'output': '{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'}]</pre> In\u00a0[50]: Copied! <pre>def human_approval(msg: AIMessage) -&gt; AIMessage:\n    \"\"\"Responsible for passing through its input or raising an exception.\n\n    Args:\n        msg: output from the chat model\n\n    Returns:\n        msg: original output from the msg\n    \"\"\"\n    for tool_call in msg.tool_calls:\n        print(f\"I want to use function [{tool_call.get('name')}] with the following parameters :\")\n        for k,v in tool_call.get('args').items():\n            print(\" {} = {}\".format(k, v))\n            \n    print(\"\")\n    input_msg = (\n        f\"Do you approve (Y|y)?\\n\\n\"\n        \"&gt;&gt;&gt;\"\n    )\n    resp = input(input_msg)\n    if resp.lower() not in (\"yes\", \"y\"):\n        raise ValueError(f\"Tool invocations not approved\")\n    return msg\n</pre> def human_approval(msg: AIMessage) -&gt; AIMessage:     \"\"\"Responsible for passing through its input or raising an exception.      Args:         msg: output from the chat model      Returns:         msg: original output from the msg     \"\"\"     for tool_call in msg.tool_calls:         print(f\"I want to use function [{tool_call.get('name')}] with the following parameters :\")         for k,v in tool_call.get('args').items():             print(\" {} = {}\".format(k, v))                  print(\"\")     input_msg = (         f\"Do you approve (Y|y)?\\n\\n\"         \"&gt;&gt;&gt;\"     )     resp = input(input_msg)     if resp.lower() not in (\"yes\", \"y\"):         raise ValueError(f\"Tool invocations not approved\")     return msg In\u00a0[52]: Copied! <pre>chain = prompt | gemini_with_tools | human_approval | call_tools\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    # Uses the get_by_session_id function defined in the example\n    # above.\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nchain_with_history.invoke({\"input\": \"What is the current USD to EUR exchange rate ?\"}, config)\n</pre>  chain = prompt | gemini_with_tools | human_approval | call_tools memory = InMemoryChatMessageHistory(session_id=\"foo\") chain_with_history = RunnableWithMessageHistory(     chain,     # Uses the get_by_session_id function defined in the example     # above.     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", )  chain_with_history.invoke({\"input\": \"What is the current USD to EUR exchange rate ?\"}, config) <pre>I want to use function [get_exchange_rate_from_api] with the following parameters :\n currency_from = USD\n currency_to = EUR\n\n</pre> <pre>Do you approve (Y|y)?\n\n&gt;&gt;&gt; y\n</pre> <pre>/Users/benjamin.etienne/Library/Caches/pypoetry/virtualenvs/london-xwvFazzc-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.frankfurter.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n</pre> Out[52]: <pre>[{'name': 'get_exchange_rate_from_api',\n  'args': {'currency_from': 'USD', 'currency_to': 'EUR'},\n  'id': '169200f6-9319-44ad-89ba-96dd7c2d893e',\n  'type': 'tool_call',\n  'output': '{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'}]</pre> In\u00a0[48]: Copied! <pre>memory = InMemoryChatMessageHistory()\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    # This is needed because in most real world scenarios, a session id is needed\n    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\nagent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n</pre> memory = InMemoryChatMessageHistory() agent_with_chat_history = RunnableWithMessageHistory(     agent_executor,     # This is needed because in most real world scenarios, a session id is needed     # It isn't really used here because we are using a simple in memory ChatMessageHistory     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", ) agent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config) Out[48]: <pre>{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"I am sorry, I do not have access to real-time information, including sports results. To find out the result of Rafael Nadal's latest game, I recommend checking a reputable sports website or news source. \\n\"}</pre> In\u00a0[56]: Copied! <pre>from langchain_community.utilities import GoogleSerperAPIWrapper\n\nsearch = GoogleSerperAPIWrapper(serper_api_key=os.environ.get('SERPER_API_KEY'))\n\n@tool\ndef google_search(query: str):\n    \"\"\"\n    Perform a search on Google\n    Args:\n        query: the information to be retrieved with google search\n    \"\"\"\n    return search.run(query)\n\nlangchain_tool = [\n    list_datasets,\n    list_tables,\n    get_exchange_rate_from_api,\n    google_search\n]\nagent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n\nmemory = InMemoryChatMessageHistory()\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    # This is needed because in most real world scenarios, a session id is needed\n    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n</pre> from langchain_community.utilities import GoogleSerperAPIWrapper  search = GoogleSerperAPIWrapper(serper_api_key=os.environ.get('SERPER_API_KEY'))  @tool def google_search(query: str):     \"\"\"     Perform a search on Google     Args:         query: the information to be retrieved with google search     \"\"\"     return search.run(query)  langchain_tool = [     list_datasets,     list_tables,     get_exchange_rate_from_api,     google_search ] agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt) agent_executor = AgentExecutor(agent=agent, tools=langchain_tool)  memory = InMemoryChatMessageHistory() agent_with_chat_history = RunnableWithMessageHistory(     agent_executor,     # This is needed because in most real world scenarios, a session id is needed     # It isn't really used here because we are using a simple in memory ChatMessageHistory     lambda session_id: memory,     input_messages_key=\"input\",     history_messages_key=\"chat_history\", ) In\u00a0[57]: Copied! <pre>agent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n</pre> agent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config) Out[57]: <pre>{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"Rafael Nadal's last match was a loss to Botic van de Zandschulp in the Davis Cup. Spain was eliminated by the Netherlands. \\n\"}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/GenAI/Agents/#agents","title":"Agents\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#apis","title":"APIs\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#gemini-function-calling","title":"Gemini Function Calling\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#create-the-tool","title":"Create the tool\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#function-declarations","title":"Function declarations\u00b6","text":"<pre>def get_exchange_rate_from_api_v2(currency_from: str, currency_to: str):\n    \"\"\"\n    Get the exchange rate for currencies\n    \n    Args:\n        currency_from (str): The currency to convert from in ISO 4217 format\n        currency_to (str): The currency to convert to in ISO 4217 format\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&amp;to={currency_to}\"\n    params = {'currency_to': currency_to, 'currency_from': currency_from}\n    api_response = requests.get(url, params=params, verify=False)\n    return api_response.text\n\n\nget_exchange_rate_func_v2 = FunctionDeclaration.from_func(get_exchange_rate_from_api_v2)\n\ntool = VertexTool(\n    function_declarations=[\n        get_exchange_rate_func_v2\n    ]\n)\n\n# Mapping to map function name to function\nfunction_handler = {\n    \"get_exchange_rate\": get_exchange_rate_from_api,\n    \"list_datasets\": list_datasets,\n    \"list_tables\": list_tables,\n}\n</pre>"},{"location":"notebooks/GenAI/Agents/#create-the-chat","title":"Create the Chat\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#add-a-tool","title":"Add a tool\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#langchain-agents","title":"LangChain Agents\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#decorate-our-apis-with-the-tool-decorator","title":"Decorate our apis with the <code>tool</code> decorator\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#build-the-tool","title":"Build the tool\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#instantiate-agentexecutor-new-way","title":"Instantiate AgentExecutor - New way\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#adding-memory","title":"Adding memory\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#chaining-method","title":"Chaining method\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#human-in-the-loop-validation","title":"Human in the loop validation\u00b6","text":""},{"location":"notebooks/GenAI/Agents/#adding-a-tool-from-langchain-community","title":"Adding a tool from langchain community\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/","title":"Optimisation","text":"In\u00a0[117]: Copied! <pre>import numpy as np\nimport itertools\nfrom ortools.linear_solver import pywraplp\n</pre> import numpy as np import itertools from ortools.linear_solver import pywraplp <ol> <li>Production Problem</li> <li>Dual Problem</li> <li>Mixed Integer Programming - Warehouse allocation</li> <li>Mixed Integer Programming - Water Network</li> </ol> <p>A biscuit factory produces cookies, cupcakes, and  brownies which are then sold at different prices. Each type of biscuits needs some flour, sugar, chocolate, vegetal oil, and eggs. For a given quantity of goods, the factory manager wants to maximize its revenue.</p> <p>Formulation of the problem</p> $w_{i,j}$ weight needed of ingredient $i$ to produce biscuit $j$ $c_j$ price for biscuit $j$ $s_i$ available stock for ingredient $i$ $x_j$ quantity of biscuit $j$ produced <p>\\begin{align*} \\max &amp;\\sum_j c_j * x_j &amp; \\\\ \\text{subject to:}&amp;&amp;\\\\ &amp;\\sum_j w_{i,j} * x_j \\leq s_i &amp; \\forall i \\\\ &amp;x_j \\geq 0 &amp; \\forall j \\end{align*}</p> In\u00a0[92]: Copied! <pre>BISCUITS = ['cookie', 'cupcake', 'brownie']\nN_BISCUITS = len(BISCUITS)\nGOODS = ['flour', 'sugar', 'chocolate', 'oil', 'eggs']\nN_GOODS = len(GOODS)\n\n# quantities\nw = np.array(\n    [[.1, .3, .05], # flour\n     [.15, .2, .3], # sugar\n     [.1, .05, .3], # chocolate\n     [.05, .1, .1], # oil\n     [0, 2, 3]]     # eggs\n)     \nc = np.array([2, 3, 5]) # selling price of biscuits\ns = np.array([25, 40, 30, 15, 300])  # stock of ingredients\n</pre> BISCUITS = ['cookie', 'cupcake', 'brownie'] N_BISCUITS = len(BISCUITS) GOODS = ['flour', 'sugar', 'chocolate', 'oil', 'eggs'] N_GOODS = len(GOODS)  # quantities w = np.array(     [[.1, .3, .05], # flour      [.15, .2, .3], # sugar      [.1, .05, .3], # chocolate      [.05, .1, .1], # oil      [0, 2, 3]]     # eggs )      c = np.array([2, 3, 5]) # selling price of biscuits s = np.array([25, 40, 30, 15, 300])  # stock of ingredients <p>We first start by instantiating the solver :</p> In\u00a0[93]: Copied! <pre>solver = pywraplp.Solver.CreateSolver('SCIP')\n</pre> solver = pywraplp.Solver.CreateSolver('SCIP') <p>We then craete the variables</p> In\u00a0[94]: Copied! <pre>x = {}\n\nfor n, biscuit in enumerate(BISCUITS):\n    x[n] = solver.NumVar(0, solver.infinity(), biscuit)\n</pre> x = {}  for n, biscuit in enumerate(BISCUITS):     x[n] = solver.NumVar(0, solver.infinity(), biscuit) In\u00a0[95]: Copied! <pre>x\n</pre> x Out[95]: <pre>{0: cookie, 1: cupcake, 2: brownie}</pre> <p>We then add the constraints to our model:</p> In\u00a0[96]: Copied! <pre>n_constraints = len(s)\n\nfor i in range(n_constraints):\n    solver.Add(sum([x[j]*w[i, j] for j in range(N_BISCUITS)]) &lt;= s[i])\n</pre> n_constraints = len(s)  for i in range(n_constraints):     solver.Add(sum([x[j]*w[i, j] for j in range(N_BISCUITS)]) &lt;= s[i]) <p>And finally we add the objective function</p> In\u00a0[97]: Copied! <pre>objective = sum([x[i]*c[i] for i in range(N_BISCUITS)])\nsolver.Maximize(objective)\n</pre> objective = sum([x[i]*c[i] for i in range(N_BISCUITS)]) solver.Maximize(objective) In\u00a0[98]: Copied! <pre># Run the solver\nstatus = solver.Solve()\nif status == pywraplp.Solver.OPTIMAL:\n    print('Objective value =', solver.Objective().Value())\n    for j in range(N_BISCUITS):\n        print(\" #{} {} = {}\".format(\n            j, x[j].name(), x[j].solution_value())\n        )\n        \n    print()\n    print('Problem solved in %f milliseconds' % solver.wall_time())\n    print('Problem solved in %d iterations' % solver.iterations())\n    print('Problem solved in %d branch-and-bound nodes' % solver.nodes())\nelse:\n    print(status)\n    print('The problem does not have an optimal solution.')\n</pre> # Run the solver status = solver.Solve() if status == pywraplp.Solver.OPTIMAL:     print('Objective value =', solver.Objective().Value())     for j in range(N_BISCUITS):         print(\" #{} {} = {}\".format(             j, x[j].name(), x[j].solution_value())         )              print()     print('Problem solved in %f milliseconds' % solver.wall_time())     print('Problem solved in %d iterations' % solver.iterations())     print('Problem solved in %d branch-and-bound nodes' % solver.nodes()) else:     print(status)     print('The problem does not have an optimal solution.') <pre>Objective value = 618.5185185185185\n #0 cookie = 66.66666666666664\n #1 cupcake = 44.44444444444442\n #2 brownie = 70.37037037037038\n\nProblem solved in 2102.000000 milliseconds\nProblem solved in 3 iterations\nProblem solved in 1 branch-and-bound nodes\n</pre> <p>The dual formulation is:</p> $w_{i,j}$ weight needed of good $i$ to produce biscuit $j$ $c_j$ price for biscuit $j$ $s_i$ available stock for good $i$ $y_i$ quantity of good $i$ used <p>\\begin{align*} \\min &amp;\\sum_i s_i * y_i &amp; \\\\ \\text{subject to:}&amp;&amp;\\\\ &amp;\\sum_i w_{j,i} * y_i \\geq c_j &amp; \\forall j \\\\ &amp;y_i \\geq 0 &amp; \\forall i \\end{align*}</p> <p>And it's implementation:</p> In\u00a0[99]: Copied! <pre>dual_model = pywraplp.Solver.CreateSolver('SCIP')\n</pre> dual_model = pywraplp.Solver.CreateSolver('SCIP') <p>In the dual problem, the variables are now the ingredients</p> In\u00a0[100]: Copied! <pre>y = {}\n\nfor n, good in enumerate(GOODS):\n    y[n] = dual_model.NumVar(0, solver.infinity(), good)\n</pre> y = {}  for n, good in enumerate(GOODS):     y[n] = dual_model.NumVar(0, solver.infinity(), good) In\u00a0[101]: Copied! <pre>y\n</pre> y Out[101]: <pre>{0: flour, 1: sugar, 2: chocolate, 3: oil, 4: eggs}</pre> In\u00a0[102]: Copied! <pre>n_constraints = len(c)\n\nfor j in range(n_constraints):\n    dual_model.Add(sum([w[i, j]*y[i] for i in range(N_GOODS)]) &gt;= c[j])\n</pre> n_constraints = len(c)  for j in range(n_constraints):     dual_model.Add(sum([w[i, j]*y[i] for i in range(N_GOODS)]) &gt;= c[j]) In\u00a0[103]: Copied! <pre>objective = sum([y[i]*s[i] for i in range(N_GOODS)])\ndual_model.Minimize(objective)\n</pre> objective = sum([y[i]*s[i] for i in range(N_GOODS)]) dual_model.Minimize(objective) In\u00a0[104]: Copied! <pre># Run the solver\nstatus = dual_model.Solve()\nprint(status)\nif status == pywraplp.Solver.OPTIMAL:\n    print('Objective value =', dual_model.Objective().Value())\n    for i in range(N_GOODS):\n        print(\" #{} {} = {}\".format(\n            i, y[i].name(), y[i].solution_value())\n        )\nelse:\n    print(status)\n    print('The problem does not have an optimal solution.')\n</pre> # Run the solver status = dual_model.Solve() print(status) if status == pywraplp.Solver.OPTIMAL:     print('Objective value =', dual_model.Objective().Value())     for i in range(N_GOODS):         print(\" #{} {} = {}\".format(             i, y[i].name(), y[i].solution_value())         ) else:     print(status)     print('The problem does not have an optimal solution.') <pre>0\nObjective value = 618.5185185185185\n #0 flour = 0.0\n #1 sugar = 11.85185185185185\n #2 chocolate = 2.2222222222222254\n #3 oil = 0.0\n #4 eggs = 0.259259259259259\n</pre> In\u00a0[105]: Copied! <pre>print('{:&lt;12} | {:^12} | {:^12}'.format('', 'Primal model', 'Dual model'))\nprint(''.join(['=']*50))\nprint('{:&lt;12} | {:^12} | {:^12}'.format('', 'Variable', 'Slack'))\nprint(''.join(['-']*50))\nfor j in range(N_BISCUITS):\n    print('{:&lt;12} | {:&gt;12.2f} | {:&gt;12.2f}'.format(\n        BISCUITS[j], x[j].solution_value() , c[j] - sum([y[i].solution_value()*w[i, j] for i in range(N_GOODS)])))\nprint(''.join(['-']*50))\nprint('{:&lt;12} | {:^12} | {:^12}'.format('', 'Slack', 'Variable'))\nprint(''.join(['-']*50))\nfor i in range(N_GOODS):\n    print('{:&lt;12} | {:&gt;12.2f} | {:&gt;12.2f}'.format(\n        GOODS[i],  s[i] - sum([x[j].solution_value()*w[i, j] for j in range(N_BISCUITS)]), y[i].solution_value()))\n</pre> print('{:&lt;12} | {:^12} | {:^12}'.format('', 'Primal model', 'Dual model')) print(''.join(['=']*50)) print('{:&lt;12} | {:^12} | {:^12}'.format('', 'Variable', 'Slack')) print(''.join(['-']*50)) for j in range(N_BISCUITS):     print('{:&lt;12} | {:&gt;12.2f} | {:&gt;12.2f}'.format(         BISCUITS[j], x[j].solution_value() , c[j] - sum([y[i].solution_value()*w[i, j] for i in range(N_GOODS)]))) print(''.join(['-']*50)) print('{:&lt;12} | {:^12} | {:^12}'.format('', 'Slack', 'Variable')) print(''.join(['-']*50)) for i in range(N_GOODS):     print('{:&lt;12} | {:&gt;12.2f} | {:&gt;12.2f}'.format(         GOODS[i],  s[i] - sum([x[j].solution_value()*w[i, j] for j in range(N_BISCUITS)]), y[i].solution_value())) <pre>             | Primal model |  Dual model \n==================================================\n             |   Variable   |    Slack    \n--------------------------------------------------\ncookie       |        66.67 |         0.00\ncupcake      |        44.44 |         0.00\nbrownie      |        70.37 |         0.00\n--------------------------------------------------\n             |    Slack     |   Variable  \n--------------------------------------------------\nflour        |         1.48 |         0.00\nsugar        |         0.00 |        11.85\nchocolate    |         0.00 |         2.22\noil          |         0.19 |         0.00\neggs         |         0.00 |         0.26\n</pre> Problem definition   We have several warehouses w and n customers to serve. We want to know which warehouse should serve which customers, given that :  <ul> <li>Each warehouse has a fixed capacity</li> <li>Serving a customer has a cost</li> </ul> Decision variables <ul> <li>decide whether a warehouse serves a customer<ul> <li>$y_{wc}$ = 1 if warehouse w serves customer c</li> </ul> </li> </ul> What are the constraints? <ul> <li>the warehouse cannot serve more customers than its capacity \\begin{align*} &amp;\\sum_c y_{wc} \\leq capa_{w} &amp; \\forall w \\\\\\end{align*}</li> <li>a customer must be served by exactly one warehouse \\begin{align*} &amp;\\sum_w y_{wc} = 1 &amp; \\forall c \\\\\\end{align*}</li> </ul> In\u00a0[148]: Copied! <pre>w_capacity = [1,4,1,3,2]  # the capacity of a warehouse\n\ntransportation_costs = np.array(\n    [[20, 28, 74,  2, 46, 42,  1, 10, 93, 47], # t_{w,s}\n     [24, 27, 97, 55, 96, 22,  5, 73, 35, 65],\n     [11, 82, 71, 73, 59, 29, 73, 13, 63, 55],\n     [25, 83, 96, 69, 83, 67, 59, 43, 85, 71],\n     [30, 74, 70, 61,  4, 59, 56, 96, 46, 95]])\n\n\nN_WAREHOUSES, N_STORES  = transportation_costs.shape\n</pre> w_capacity = [1,4,1,3,2]  # the capacity of a warehouse  transportation_costs = np.array(     [[20, 28, 74,  2, 46, 42,  1, 10, 93, 47], # t_{w,s}      [24, 27, 97, 55, 96, 22,  5, 73, 35, 65],      [11, 82, 71, 73, 59, 29, 73, 13, 63, 55],      [25, 83, 96, 69, 83, 67, 59, 43, 85, 71],      [30, 74, 70, 61,  4, 59, 56, 96, 46, 95]])   N_WAREHOUSES, N_STORES  = transportation_costs.shape In\u00a0[149]: Copied! <pre># --- Create the solver\nmip_model = pywraplp.Solver.CreateSolver(\"SAT\")\n\n# --- Create the variables\ny = {}\nfor w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\n\n\n# --- Create the constraints\nfor w in range(N_WAREHOUSES):\n    mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])\n    \nfor c in range(N_STORES):\n    mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)\n    \n# -- Define the objective function\nobjective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))])\nmip_model.Minimize(objective)\n\nprint(f\"Solving with {solver.SolverVersion()}\")\nstatus = mip_model.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print(\"Objective value =\", mip_model.Objective().Value())\n    solutions = np.zeros((N_WAREHOUSES, N_STORES))\n    for s, w in itertools.product(range(N_STORES), range(N_WAREHOUSES)):\n        solutions[w,s] = y[w,s].solution_value()\n        if solutions[w,s] == 1:\n            print(\"Store {} will be served by warehouse {}\".format(s,w))\n    print()\n    print(f\"Problem solved in {mip_model.wall_time():d} milliseconds\")\n    print(f\"Problem solved in {mip_model.iterations():d} iterations\")\n    print(f\"Problem solved in {mip_model.nodes():d} branch-and-bound nodes\")\n    # print(solutions)\nelse:\n    print(\"The problem does not have an optimal solution.\")\n</pre> # --- Create the solver mip_model = pywraplp.Solver.CreateSolver(\"SAT\")  # --- Create the variables y = {} for w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\") print(\"Number of variables =\", mip_model.NumVariables())   # --- Create the constraints for w in range(N_WAREHOUSES):     mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])      for c in range(N_STORES):     mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)      # -- Define the objective function objective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))]) mip_model.Minimize(objective)  print(f\"Solving with {solver.SolverVersion()}\") status = mip_model.Solve()  if status == pywraplp.Solver.OPTIMAL:     print(\"Objective value =\", mip_model.Objective().Value())     solutions = np.zeros((N_WAREHOUSES, N_STORES))     for s, w in itertools.product(range(N_STORES), range(N_WAREHOUSES)):         solutions[w,s] = y[w,s].solution_value()         if solutions[w,s] == 1:             print(\"Store {} will be served by warehouse {}\".format(s,w))     print()     print(f\"Problem solved in {mip_model.wall_time():d} milliseconds\")     print(f\"Problem solved in {mip_model.iterations():d} iterations\")     print(f\"Problem solved in {mip_model.nodes():d} branch-and-bound nodes\")     # print(solutions) else:     print(\"The problem does not have an optimal solution.\") <pre>Number of variables = 50\nSolving with SCIP 9.0.0 [LP solver: Glop 9.10]\nObjective value = 274.0\nStore 0 will be served by warehouse 3\nStore 1 will be served by warehouse 1\nStore 2 will be served by warehouse 4\nStore 3 will be served by warehouse 0\nStore 4 will be served by warehouse 4\nStore 5 will be served by warehouse 1\nStore 6 will be served by warehouse 1\nStore 7 will be served by warehouse 2\nStore 8 will be served by warehouse 1\nStore 9 will be served by warehouse 3\n\nProblem solved in 12 milliseconds\nProblem solved in 0 iterations\nProblem solved in 0 branch-and-bound nodes\n</pre> <p>Let's now consider the rental prices of your warehouses: The rents of the warehouses are :</p> <pre>rents = [20, 75, 18, 34, 22]\n</pre> <p>Your landlord for warehouse 2 is asking for a +50% increase, from 18 to 27k\u20ac per month. The question is :</p> <p>-&gt; Should you close store 2 or should you accept the raise?</p> <p>We will start by adding this term to our objective function : \\begin{align*} \\min &amp;\\qquad \\sum_w rent_w x_w + \\sum_{w ,s} t_{ws} y_{ws} &amp; \\\\ \\end{align*}</p> Decision variables <ul> <li>decide whether a warehouse serves a customer<ul> <li>$y_{wc}$ = 1 if warehouse w serves customer c</li> </ul> </li> <li>for each warehouse, decide whether to open it<ul> <li>$x_w$ = 1 if warehouse w is open</li></ul></li> </ul> What are the constraints? <ul> <li>the warehouse cannot serve more customers than its capacity \\begin{align*} &amp;\\sum_c y_{wc} \\leq capa_{w} &amp; \\forall w \\\\\\end{align*}</li> <li>a customer must be served by exactly one warehouse \\begin{align*} &amp;\\sum_w y_{wc} = 1 &amp; \\forall c \\\\\\end{align*}</li> <li>a warehouse can serve a customer only if it is open \\begin{align*} &amp;y_{wc} \\leq x_w &amp; \\forall w,c \\\\\\end{align*}</li> </ul> What is the objective function ? <p>We want to minimize all three:</p> <ul> <li>the cost of opening a warehouse</li> <li>the transportation cost between the customer and the warehouse</li> </ul> Problem definition  In\u00a0[161]: Copied! <pre># --- Create the solver\nmip_model = pywraplp.Solver.CreateSolver(\"SAT\")\nrents = [20, 75, 18, 34, 22]\n\n\n# --- Create the variables\ny = {}\nfor w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\nx = {}\nfor w in range(N_WAREHOUSES):\n    x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\n\n\n\n# --- Create the constraints\nfor w in range(N_WAREHOUSES):\n    mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])\n    \nfor c in range(N_STORES):\n    mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)\n    \nfor w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    mip_model.Add(y[w, c] &lt;= x[w])\n    \n# -- Define the objective function\nobjective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))])\nobjective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)])\nmip_model.Minimize(objective)\n\nprint(f\"Solving with {solver.SolverVersion()}\")\nstatus = mip_model.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print(\"Objective value with current rents =\", mip_model.Objective().Value())\nelse:\n    print(\"The problem does not have an optimal solution.\")\n</pre> # --- Create the solver mip_model = pywraplp.Solver.CreateSolver(\"SAT\") rents = [20, 75, 18, 34, 22]   # --- Create the variables y = {} for w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\") print(\"Number of variables =\", mip_model.NumVariables()) x = {} for w in range(N_WAREHOUSES):     x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\") print(\"Number of variables =\", mip_model.NumVariables())    # --- Create the constraints for w in range(N_WAREHOUSES):     mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])      for c in range(N_STORES):     mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)      for w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     mip_model.Add(y[w, c] &lt;= x[w])      # -- Define the objective function objective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))]) objective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)]) mip_model.Minimize(objective)  print(f\"Solving with {solver.SolverVersion()}\") status = mip_model.Solve()  if status == pywraplp.Solver.OPTIMAL:     print(\"Objective value with current rents =\", mip_model.Objective().Value()) else:     print(\"The problem does not have an optimal solution.\") <pre>Number of variables = 50\nNumber of variables = 55\nSolving with SCIP 9.0.0 [LP solver: Glop 9.10]\nObjective value with current rents = 443.0\n</pre> In\u00a0[164]: Copied! <pre># --- Create the solver\nmip_model = pywraplp.Solver.CreateSolver(\"SAT\")\nrents = [20, 75, 27, 34, 22]\n\n\n# --- Create the variables\ny = {}\nfor w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\nx = {}\nfor w in range(N_WAREHOUSES):\n    x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\n\n\n\n# --- Create the constraints\nfor w in range(N_WAREHOUSES):\n    mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])\n    \nfor c in range(N_STORES):\n    mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)\n    \nfor w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    mip_model.Add(y[w, c] &lt;= x[w])\n    \n# -- Define the objective function\nobjective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))])\nobjective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)])\nmip_model.Minimize(objective)\n\nprint(f\"Solving with {solver.SolverVersion()}\")\nstatus = mip_model.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print(\"Objective value with new rents =\", mip_model.Objective().Value())\nelse:\n    print(\"The problem does not have an optimal solution.\")\n</pre> # --- Create the solver mip_model = pywraplp.Solver.CreateSolver(\"SAT\") rents = [20, 75, 27, 34, 22]   # --- Create the variables y = {} for w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\") print(\"Number of variables =\", mip_model.NumVariables()) x = {} for w in range(N_WAREHOUSES):     x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\") print(\"Number of variables =\", mip_model.NumVariables())    # --- Create the constraints for w in range(N_WAREHOUSES):     mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])      for c in range(N_STORES):     mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)      for w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     mip_model.Add(y[w, c] &lt;= x[w])      # -- Define the objective function objective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))]) objective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)]) mip_model.Minimize(objective)  print(f\"Solving with {solver.SolverVersion()}\") status = mip_model.Solve()  if status == pywraplp.Solver.OPTIMAL:     print(\"Objective value with new rents =\", mip_model.Objective().Value()) else:     print(\"The problem does not have an optimal solution.\") <pre>Number of variables = 50\nNumber of variables = 55\nSolving with SCIP 9.0.0 [LP solver: Glop 9.10]\nObjective value with new rents = 452.0\n</pre> <p>With the rent increase for store 2, costs have raised from 443 to 452k\u20ac.</p> <p>How about closing one store to compensate for this raise ?</p> <p>We add an additional constraint :</p> <ul> <li>the warehouse cannot serve more customers than its capacity \\begin{align*} &amp;\\sum_c y_{wc} \\leq capa_{w} &amp; \\forall w \\\\\\end{align*}</li> <li>a customer must be served by exactly one warehouse \\begin{align*} &amp;\\sum_w y_{wc} = 1 &amp; \\forall c \\\\\\end{align*}</li> <li>a warehouse can serve a customer only if it is open \\begin{align*} &amp;y_{wc} \\leq x_w &amp; \\forall w,c \\\\\\end{align*}</li> <li>we can only open 4 warehouses \\begin{align*} &amp;\\sum_w x_{w} = 4  \\\\\\end{align*}</li> </ul> In\u00a0[172]: Copied! <pre># --- Create the solver\nmip_model = pywraplp.Solver.CreateSolver(\"SAT\")\nrents = [20, 75, 27, 34, 22]\n\n\n# --- Create the variables\ny = {}\nfor w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\nx = {}\nfor w in range(N_WAREHOUSES):\n    x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\")\nprint(\"Number of variables =\", mip_model.NumVariables())\n\n\n\n# --- Create the constraints\nfor w in range(N_WAREHOUSES):\n    mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])\n    \nfor c in range(N_STORES):\n    mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)\n    \nfor w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):\n    mip_model.Add(y[w, c] &lt;= x[w])\n    \nmip_model.Add(sum([x[w] for w in range(N_WAREHOUSES)]) &lt;= 4)\n\n# -- Define the objective function\nobjective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))])\nobjective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)])\nmip_model.Minimize(objective)\n\nprint(f\"Solving with {solver.SolverVersion()}\")\nstatus = mip_model.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print(\"Objective value with new rents =\", mip_model.Objective().Value())\n    for i in range(N_WAREHOUSES):\n        print(f\"Warehouse {i} -&gt; {'Keep' if x[i].solution_value()==1 else 'Close'}\")\nelse:\n    print(\"The problem does not have an optimal solution.\")\n</pre> # --- Create the solver mip_model = pywraplp.Solver.CreateSolver(\"SAT\") rents = [20, 75, 27, 34, 22]   # --- Create the variables y = {} for w, s in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     y[w, s] = mip_model.IntVar(0, 1, f\"y[{w, s}]\") print(\"Number of variables =\", mip_model.NumVariables()) x = {} for w in range(N_WAREHOUSES):     x[w] = mip_model.IntVar(0, 1, f\"x[{w}]\") print(\"Number of variables =\", mip_model.NumVariables())    # --- Create the constraints for w in range(N_WAREHOUSES):     mip_model.Add(sum([y[w, c] for c in range(N_STORES)]) &lt;= w_capacity[w])      for c in range(N_STORES):     mip_model.Add(sum([y[w, c] for w in range(N_WAREHOUSES)]) == 1)      for w, c in itertools.product(range(N_WAREHOUSES), range(N_STORES)):     mip_model.Add(y[w, c] &lt;= x[w])      mip_model.Add(sum([x[w] for w in range(N_WAREHOUSES)]) &lt;= 4)  # -- Define the objective function objective = sum([y[i, j]*transportation_costs[i, j] for i, j in itertools.product(range(N_WAREHOUSES), range(N_STORES))]) objective += sum([x[i] * rents[i] for i in range(N_WAREHOUSES)]) mip_model.Minimize(objective)  print(f\"Solving with {solver.SolverVersion()}\") status = mip_model.Solve()  if status == pywraplp.Solver.OPTIMAL:     print(\"Objective value with new rents =\", mip_model.Objective().Value())     for i in range(N_WAREHOUSES):         print(f\"Warehouse {i} -&gt; {'Keep' if x[i].solution_value()==1 else 'Close'}\") else:     print(\"The problem does not have an optimal solution.\") <pre>Number of variables = 50\nNumber of variables = 55\nSolving with SCIP 9.0.0 [LP solver: Glop 9.10]\nObjective value with new rents = 455.0\nWarehouse 0 -&gt; Keep\nWarehouse 1 -&gt; Keep\nWarehouse 2 -&gt; Close\nWarehouse 3 -&gt; Keep\nWarehouse 4 -&gt; Keep\n</pre> <p>Shutting down warehouse 2 would be 3k more expensive than to accept the rent raise.</p> Problem definition   Consider a water network that consists of a set of nodes. Each node has a demand for water and may produce some water. The goal is to decide where to produce the water to meet the demand and to determine the best way to transport the water from the production nodes to the consumption nodes through pipelines. Each pipeline has a capacity that cannot be exceeded. There is a transportation cost for shipping a unit of water through each pipeline and a penalty for each unit of demand that is not fulfilled by the production and transportation plan. The goal is to minimize the total cost.     Decision variables <ul> <li>decide whether a warehouse serves a customer<ul> <li>$v_{ij}$ = amount of water flowing from node $i$ to node $j$</li> <li>$p_i$ = amount of water produced at node $i$</li> <li>$d_i$ = amount of water demand at node $i$</li> </ul> </li> </ul> What are the constraints? <ul> <li>Each pipeline has a capacity that cannot be exceeded \\begin{align*} &amp;v_{ij} \\leq ca_{ij} &amp; \\forall i,j \\\\\\end{align*}</li> <li>The amount of water produced by node i cannot exceed the maximum production of this node \\begin{align*} &amp;p_{i} \\leq p_{max_i} &amp; \\forall i \\\\\\end{align*}</li> <li>The amount of water produced + received by node i is equal to what is consumed (demand) + what leaves \\begin{align*} &amp;\\sum_j v_{ji} + p_i = \\sum_j v_{ij} + d_{i} - z_{i} &amp; \\forall i \\\\\\end{align*}</li> </ul> In\u00a0[183]: Copied! <pre># consumption (demand)\nd = [ 0, 50, 95, 10, 73, 55, 125, 32, 40, 20 ]\n# production (maximum generation)\np_max = [ 500, 0, 0, 500, 0, 0, 500, 0, 0, 0 ]\n\nN_NODES = len(d)\n\n# capacity of the arcs\nca = [ [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n       [20, 30, 40, 50, 60, 70, 80, 90, 100, 10],\n       [30, 40, 50, 60, 70, 80, 90, 100, 10, 20],\n       [40, 50, 60, 70, 80, 90, 100, 10, 20, 30],\n       [50, 60, 70, 80, 90, 100, 10, 20, 30, 40],\n       [60, 70, 80, 90, 100, 10, 20, 30, 40, 50],\n       [70, 80, 90, 100, 10, 20, 30, 40, 50, 60],\n       [80, 90, 100, 10, 20, 30, 40, 50, 60, 70],\n       [90, 100, 10, 20, 30, 40, 50, 60, 70, 80],\n       [100, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n     ]\n\n# linear variable cost: the cost of transporting one unit of water\nvc = 1\n\n# unsatisfied demand: penalty for each unit of water which is not consumed or produced\npenalty = 1000\n</pre> # consumption (demand) d = [ 0, 50, 95, 10, 73, 55, 125, 32, 40, 20 ] # production (maximum generation) p_max = [ 500, 0, 0, 500, 0, 0, 500, 0, 0, 0 ]  N_NODES = len(d)  # capacity of the arcs ca = [ [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],        [20, 30, 40, 50, 60, 70, 80, 90, 100, 10],        [30, 40, 50, 60, 70, 80, 90, 100, 10, 20],        [40, 50, 60, 70, 80, 90, 100, 10, 20, 30],        [50, 60, 70, 80, 90, 100, 10, 20, 30, 40],        [60, 70, 80, 90, 100, 10, 20, 30, 40, 50],        [70, 80, 90, 100, 10, 20, 30, 40, 50, 60],        [80, 90, 100, 10, 20, 30, 40, 50, 60, 70],        [90, 100, 10, 20, 30, 40, 50, 60, 70, 80],        [100, 10, 20, 30, 40, 50, 60, 70, 80, 90]      ]  # linear variable cost: the cost of transporting one unit of water vc = 1  # unsatisfied demand: penalty for each unit of water which is not consumed or produced penalty = 1000 In\u00a0[191]: Copied! <pre>mip_model = pywraplp.Solver.CreateSolver(\"SAT\")\n\n\n# Create variables\nv = {}\nfor i,j in itertools.product(range(N_NODES), range(N_NODES)):\n    v[i,j] = mip_model.IntVar(0, mip_model.infinity(), f\"v[{i,j}]\") \n    \nz = {}\nfor i in range(N_NODES):\n    z[i] = mip_model.IntVar(0, mip_model.infinity(), f\"z[{i}]\") \n\np = {}\nfor i in range(N_NODES):\n    p[i] = mip_model.IntVar(0, p_max[i], f\"p[{i}]\") \n    \n# Create constraints\nfor i in range(N_NODES):\n    mip_model.Add(sum([v[j, i] for j in range(N_NODES)]) + p[i] == sum([v[i, j] for j in range(N_NODES)]) + d[i] - z[i])\n    \n\nfor i,j in itertools.product(range(N_NODES), range(N_NODES)):\n    mip_model.Add(v[i, j] &lt;= ca[i][j])\n    \n    \n# -- Define the objective function\nobjective = sum([v[i, j] * vc for i, j in itertools.product(range(N_NODES), range(N_NODES))])\nobjective += sum([z[i] * penalty for i in range(N_NODES)])\nmip_model.Minimize(objective)\n\nprint(f\"Solving with {solver.SolverVersion()}\")\nstatus = mip_model.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print(\"Objective value =\", mip_model.Objective().Value())\n    for i in range(N_NODES):\n        if p[i].solution_value() &gt; 0:\n            print(f\"Production at node {i} -&gt; {p[i].solution_value()}\")\n    for i,j in itertools.product(range(N_NODES), range(N_NODES)):\n        if v[i, j].solution_value() &gt; 0:\n            print(f\"Flow between {i} and {j} -&gt; {v[i, j].solution_value()}\")\nelse:\n    print(\"The problem does not have an optimal solution.\")\n</pre> mip_model = pywraplp.Solver.CreateSolver(\"SAT\")   # Create variables v = {} for i,j in itertools.product(range(N_NODES), range(N_NODES)):     v[i,j] = mip_model.IntVar(0, mip_model.infinity(), f\"v[{i,j}]\")       z = {} for i in range(N_NODES):     z[i] = mip_model.IntVar(0, mip_model.infinity(), f\"z[{i}]\")   p = {} for i in range(N_NODES):     p[i] = mip_model.IntVar(0, p_max[i], f\"p[{i}]\")       # Create constraints for i in range(N_NODES):     mip_model.Add(sum([v[j, i] for j in range(N_NODES)]) + p[i] == sum([v[i, j] for j in range(N_NODES)]) + d[i] - z[i])       for i,j in itertools.product(range(N_NODES), range(N_NODES)):     mip_model.Add(v[i, j] &lt;= ca[i][j])           # -- Define the objective function objective = sum([v[i, j] * vc for i, j in itertools.product(range(N_NODES), range(N_NODES))]) objective += sum([z[i] * penalty for i in range(N_NODES)]) mip_model.Minimize(objective)  print(f\"Solving with {solver.SolverVersion()}\") status = mip_model.Solve()  if status == pywraplp.Solver.OPTIMAL:     print(\"Objective value =\", mip_model.Objective().Value())     for i in range(N_NODES):         if p[i].solution_value() &gt; 0:             print(f\"Production at node {i} -&gt; {p[i].solution_value()}\")     for i,j in itertools.product(range(N_NODES), range(N_NODES)):         if v[i, j].solution_value() &gt; 0:             print(f\"Flow between {i} and {j} -&gt; {v[i, j].solution_value()}\") else:     print(\"The problem does not have an optimal solution.\") <pre>Solving with SCIP 9.0.0 [LP solver: Glop 9.10]\nObjective value = 365.0\nProduction at node 0 -&gt; 112.0\nProduction at node 3 -&gt; 113.0\nProduction at node 6 -&gt; 275.0\nFlow between 0 and 2 -&gt; 5.0\nFlow between 0 and 4 -&gt; 50.0\nFlow between 0 and 5 -&gt; 35.0\nFlow between 0 and 7 -&gt; 22.0\nFlow between 3 and 1 -&gt; 50.0\nFlow between 3 and 4 -&gt; 23.0\nFlow between 3 and 7 -&gt; 10.0\nFlow between 3 and 8 -&gt; 20.0\nFlow between 6 and 2 -&gt; 90.0\nFlow between 6 and 5 -&gt; 20.0\nFlow between 6 and 8 -&gt; 20.0\nFlow between 6 and 9 -&gt; 20.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Optimization/OR_tools_exercises/#optimisation","title":"Optimisation\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#0-summary","title":"0. Summary\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#1-linear-programming-production-problem","title":"1. Linear Programming : Production Problem\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#2-linear-programming-duality","title":"2. Linear Programming : Duality\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#3-mixed-integer-programming-warehouse-problem","title":"3. Mixed Integer Programming : WareHouse Problem\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#problem-formulation","title":"Problem Formulation\u00b6","text":"<p>\\begin{align*} \\min &amp;\\qquad \\sum_{w ,s} t_{wc} y_{wc} &amp; \\\\ \\text{subject to:} &amp;&amp;\\\\ &amp;\\sum_w y_{wc} = 1 &amp; \\forall c \\\\ &amp;x_w, y_{wc} \\in \\mathbb{B} &amp; \\forall w,c \\end{align*}</p>"},{"location":"notebooks/Optimization/OR_tools_exercises/#4-mixed-integer-programming-water-network-problem","title":"4. Mixed Integer Programming : Water Network Problem\u00b6","text":""},{"location":"notebooks/Optimization/OR_tools_exercises/#problem-formulation","title":"Problem Formulation\u00b6","text":"<p>\\begin{align*} \\min &amp;\\qquad \\sum_{i,j} vc * v_{ij} + \\sum_{i} p * z_{i} &amp; \\\\ \\end{align*}</p>"},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/","title":"BERT Sentiment Classifier with PyTorch","text":"In\u00a0[1]: Copied! <pre>import streamlit as st\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel\nfrom transformers import BertForSequenceClassification\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torchmetrics\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport lightning as pl\n\nfrom sklearn.model_selection import train_test_split\n</pre> import streamlit as st from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel from transformers import BertForSequenceClassification import torch import pandas as pd import numpy as np import torchmetrics  from torch.utils.data import Dataset, DataLoader import torch.nn as nn import lightning as pl  from sklearn.model_selection import train_test_split <pre>/Users/benjamin.etienne/Projects/CAR-DATA-TEAM/sentiment-analysis/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>%load_ext watermark\n</pre> %load_ext watermark In\u00a0[3]: Copied! <pre>%watermark -p streamlit,transformers,torch,pandas,lightning\n</pre> %watermark -p streamlit,transformers,torch,pandas,lightning <pre>streamlit   : 1.33.0\ntransformers: 4.40.1\ntorch       : 2.2.2\npandas      : 2.0.3\nlightning   : 2.2.4\n\n</pre> In\u00a0[18]: Copied! <pre># Data can be found in the csb-sentiment-analysis bucket\n\ndf=pd.concat([\n    pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),\n    pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\")\n])\n</pre> # Data can be found in the csb-sentiment-analysis bucket  df=pd.concat([     pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),     pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\") ]) In\u00a0[19]: Copied! <pre>df = df.dropna(subset=['sentiment'], axis=0)\ndf['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\ndf.head()\n</pre> df = df.dropna(subset=['sentiment'], axis=0) df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int) df.head() Out[19]: platform bodyText sentiment date country Target 0 Twitter @Kenichan I dived many times for the ball. Man... 0.4939 2009-04-06 NaN 2 1 Twitter @nationwideclass no, it's not behaving at all.... -0.4939 2009-04-06 NaN 0 2 Twitter Need a hug 0.4767 2009-04-06 NaN 2 3 Twitter @LOLTrish hey  long time no see! Yes.. Rains a... 0.6208 2009-04-06 NaN 2 4 Twitter @Tatiana_K nope they didn't have it 0.0000 2009-04-06 NaN 1 In\u00a0[20]: Copied! <pre>df_train, _df = train_test_split(df, stratify=df['Target'], test_size=0.2)\ndf_val, df_test = train_test_split(_df, stratify=_df['Target'], test_size=0.5)\n</pre> df_train, _df = train_test_split(df, stratify=df['Target'], test_size=0.2) df_val, df_test = train_test_split(_df, stratify=_df['Target'], test_size=0.5) In\u00a0[4]: Copied! <pre>#\u00a0Model can be downloaded https://hf-mirror.com/google/bert_uncased_L-2_H-128_A-2/tree/main\n# or from the csb-sentiment-analysis bucket\n\nPRETRAINED_MODEL_DIR = '../models/bert_uncased_L-2_H-128_A-2'\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\nmodel = BertModel.from_pretrained(PRETRAINED_MODEL_DIR)\n</pre> #\u00a0Model can be downloaded https://hf-mirror.com/google/bert_uncased_L-2_H-128_A-2/tree/main # or from the csb-sentiment-analysis bucket  PRETRAINED_MODEL_DIR = '../models/bert_uncased_L-2_H-128_A-2' tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR) model = BertModel.from_pretrained(PRETRAINED_MODEL_DIR) In\u00a0[28]: Copied! <pre>class BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        \n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n        \n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n            truncation=True\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n            }\n        y = torch.tensor(y, dtype=torch.long)\n        \n        return x, y\n</pre> class BertDataset(Dataset):     def __init__(self, df, tokenizer, max_length=100):         super(BertDataset, self).__init__()         self.df=df         self.tokenizer=tokenizer         self.target=self.df['Target']         self.max_length=max_length              def __len__(self):         return len(self.df)          def __getitem__(self, idx):                  X = self.df['bodyText'].values[idx]         y = self.target.values[idx]                  inputs = self.tokenizer.encode_plus(             X,             pad_to_max_length=True,             add_special_tokens=True,             return_attention_mask=True,             max_length=self.max_length,             truncation=True         )         ids = inputs[\"input_ids\"]         token_type_ids = inputs[\"token_type_ids\"]         mask = inputs[\"attention_mask\"]          x = {             'ids': torch.tensor(ids, dtype=torch.long),             'mask': torch.tensor(mask, dtype=torch.long),             'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)             }         y = torch.tensor(y, dtype=torch.long)                  return x, y In\u00a0[29]: Copied! <pre>train_ds= BertDataset(df_train, tokenizer, max_length=100)\ntrain_loader=DataLoader(dataset=train_ds, batch_size=512)\neval_ds= BertDataset(df_test, tokenizer, max_length=100)\neval_loader=DataLoader(dataset=eval_ds, batch_size=512)\n</pre> train_ds= BertDataset(df_train, tokenizer, max_length=100) train_loader=DataLoader(dataset=train_ds, batch_size=512) eval_ds= BertDataset(df_test, tokenizer, max_length=100) eval_loader=DataLoader(dataset=eval_ds, batch_size=512) In\u00a0[30]: Copied! <pre>sample_batch = next(iter(train_loader))\n</pre> sample_batch = next(iter(train_loader)) In\u00a0[40]: Copied! <pre>x, y = sample_batch\nfor k, v in x.items():\n    print(\"&gt;\", k, \":\")\n    print(\"-\"*len(k))\n    print(v[0, :])\nprint()\nprint(\"&gt; target\")\nprint(\"-\"*6)\nprint(y[0])\n</pre> x, y = sample_batch for k, v in x.items():     print(\"&gt;\", k, \":\")     print(\"-\"*len(k))     print(v[0, :]) print() print(\"&gt; target\") print(\"-\"*6) print(y[0]) <pre>&gt; ids :\n---\ntensor([  101,  7842,  4246,  4948,  4542,  4122,  7084,  2000,  2272,  2461,\n         2085,  1060,  1012,  8299,  1024,  1013,  1013,  4714,  3126,  2140,\n         1012,  4012,  1013,  1051, 10354,  2615, 28311,   102,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n&gt; mask :\n----\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])\n&gt; token_type_ids :\n--------------\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0])\n\n&gt; target\n------\ntensor(1)\n</pre> In\u00a0[41]: Copied! <pre># tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n#\u00a0model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n</pre> # tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment') #\u00a0model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment') In\u00a0[42]: Copied! <pre>tokenizer.encode_plus(\"I love pizza\",\n                      max_length = 10,           # Pad &amp; truncate all sentences.\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt')\n</pre> tokenizer.encode_plus(\"I love pizza\",                       max_length = 10,           # Pad &amp; truncate all sentences.                         padding = 'max_length',                         return_attention_mask = True,   # Construct attn. masks.                         return_tensors = 'pt') Out[42]: <pre>{'input_ids': tensor([[  101,  1045,  2293, 10733,   102,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}</pre> In\u00a0[8]: Copied! <pre>class SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n        \n        self.bert_module.requires_grad_(False)\n        for param in self.bert_module.encoder.parameters():\n            param.requires_grad = True\n        \n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        # print(ids.size(), mask.size(), token_type_ids.size())\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n</pre> class SentimentBERT(nn.Module):     def __init__(self, bert_model):         super().__init__()         self.bert_module = bert_model         self.dropout = nn.Dropout(0.1)         self.final = nn.Linear(in_features=128, out_features=3, bias=True)                   self.bert_module.requires_grad_(False)         for param in self.bert_module.encoder.parameters():             param.requires_grad = True              def forward(self, inputs):         ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']         # print(ids.size(), mask.size(), token_type_ids.size())         x = self.bert_module(ids, mask, token_type_ids)         x = self.dropout(x['pooler_output'])         out = self.final(x)         return out In\u00a0[45]: Copied! <pre>bert_model = SentimentBERT(model)\nprint(bert_model)\n</pre> bert_model = SentimentBERT(model) print(bert_model) <pre>SentimentBERT(\n  (bert_module): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (final): Linear(in_features=128, out_features=4, bias=True)\n)\n</pre> In\u00a0[46]: Copied! <pre>total_parameters = sum([np.prod(p.size()) for p in bert_model.parameters()])\nmodel_parameters = filter(lambda p: p.requires_grad, bert_model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\")\n</pre> total_parameters = sum([np.prod(p.size()) for p in bert_model.parameters()]) model_parameters = filter(lambda p: p.requires_grad, bert_model.parameters()) params = sum([np.prod(p.size()) for p in model_parameters]) print(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\") <pre>Total params : 4386436 - Trainable : 397060 (9.05199574324121% of total)\n</pre> In\u00a0[44]: Copied! <pre>import time\n\n\ndef train(model, dataloader, loss_fn, optimizer):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n        \n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n        \n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        \n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc / total_count\n</pre> import time   def train(model, dataloader, loss_fn, optimizer):     model.train()     total_acc, total_count = 0, 0     log_interval = 50     start_time = time.time()      for idx, (inputs, label) in enumerate(dataloader):         optimizer.zero_grad()         predicted_label = model(inputs)                  loss = loss_fn(predicted_label, label)         loss.backward()         optimizer.step()                  total_acc += (predicted_label.argmax(1) == label).sum().item()         total_count += label.size(0)                  if idx % log_interval == 0:             elapsed = time.time() - start_time             print(                 \"Epoch {:3d} | {:5d}/{:5d} batches \"                 \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(                     epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed                 )             )             total_acc, total_count = 0, 0             start_time = time.time()   def evaluate(model, dataloader, loss_fn):     model.eval()     total_acc, total_count = 0, 0      with torch.no_grad():         for idx, (inputs, label) in enumerate(dataloader):             predicted_label = model(inputs)             loss = loss_fn(predicted_label, label)             total_acc += (predicted_label.argmax(1) == label).sum().item()             total_count += label.size(0)     return total_acc / total_count In\u00a0[47]: Copied! <pre>EPOCHS = 2\nBATCH_SIZE=512\nLEARNING_RATE = 1e-3\n\noptimizer = torch.optim.Adam([p for p in bert_model.parameters() if p.requires_grad], LEARNING_RATE)\nloss_fn = nn.CrossEntropyLoss()\n\ntrain_ds= BertDataset(df_train, tokenizer, max_length=100)\ntrain_loader=DataLoader(dataset=train_ds,batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\neval_ds= BertDataset(df_test, tokenizer, max_length=100)\neval_loader=DataLoader(dataset=eval_ds,batch_size=BATCH_SIZE, pin_memory=True)\n</pre> EPOCHS = 2 BATCH_SIZE=512 LEARNING_RATE = 1e-3  optimizer = torch.optim.Adam([p for p in bert_model.parameters() if p.requires_grad], LEARNING_RATE) loss_fn = nn.CrossEntropyLoss()  train_ds= BertDataset(df_train, tokenizer, max_length=100) train_loader=DataLoader(dataset=train_ds,batch_size=BATCH_SIZE, pin_memory=True, shuffle=True) eval_ds= BertDataset(df_test, tokenizer, max_length=100) eval_loader=DataLoader(dataset=eval_ds,batch_size=BATCH_SIZE, pin_memory=True) In\u00a0[\u00a0]: Copied! <pre>for epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train(bert_model, train_loader, loss_fn=loss_fn, optimizer=optimizer)\n    accu_val = evaluate(bert_model, valid_loader, loss_fn=loss_fn, optimizer=optimizer)\n    \n    print(\"-\" * 59)\n    print(\n        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n        \"valid accuracy {:8.3f} \".format(\n            epoch, time.time() - epoch_start_time, accu_val\n        )\n    )\n    print(\"-\" * 59)\n</pre> for epoch in range(1, EPOCHS + 1):     epoch_start_time = time.time()     train(bert_model, train_loader, loss_fn=loss_fn, optimizer=optimizer)     accu_val = evaluate(bert_model, valid_loader, loss_fn=loss_fn, optimizer=optimizer)          print(\"-\" * 59)     print(         \"| end of epoch {:3d} | time: {:5.2f}s | \"         \"valid accuracy {:8.3f} \".format(             epoch, time.time() - epoch_start_time, accu_val         )     )     print(\"-\" * 59) <p>The first step is to build a Docker image from the Dockerfile</p> <p><code>Dockerfile here</code></p> <p>Then, we need to write a little <code>build.sh</code> script to build and push the image to Artefact Registry</p> <pre># build.sh\n\nexport PROJECT_ID=...\nexport IMAGE_REPO_NAME=pt_bert_sentiment\nexport IMAGE_TAG=dev\nexport IMAGE_URI=eu.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n\ngcloud builds submit --tag $IMAGE_URI .\n</pre> <p>We will then launch a job, with 2 options:</p> In\u00a0[\u00a0]: Copied! <pre>!gcloud ai custom-jobs stream-logs projects/1011434374459/locations/europe-west4/customJobs/8968484625693278208\n</pre> !gcloud ai custom-jobs stream-logs projects/1011434374459/locations/europe-west4/customJobs/8968484625693278208 In\u00a0[9]: Copied! <pre>from google.cloud import storage\n\nstorage_client = storage.Client()\nbucket = storage_client.bucket(\"csb-sentiment-analysis\")\nblob = bucket.blob(\"model/model.pt\")\nloaded_model = SentimentBERT(model)\n\nwith blob.open(\"rb\") as f:\n    loaded_model.load_state_dict(torch.load(f, map_location=torch.device('cpu')))\n    \nloaded_model.eval()\n</pre> from google.cloud import storage  storage_client = storage.Client() bucket = storage_client.bucket(\"csb-sentiment-analysis\") blob = bucket.blob(\"model/model.pt\") loaded_model = SentimentBERT(model)  with blob.open(\"rb\") as f:     loaded_model.load_state_dict(torch.load(f, map_location=torch.device('cpu')))      loaded_model.eval()  Out[9]: <pre>SentimentBERT(\n  (bert_module): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (final): Linear(in_features=128, out_features=3, bias=True)\n)</pre> In\u00a0[33]: Copied! <pre>def sentiment_score(comment):\n    mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n    inputs = tokenizer(comment, return_tensors='pt')\n    ids = inputs[\"input_ids\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n    mask = inputs[\"attention_mask\"]\n\n    x = {\n        'ids': ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids\n        }\n    result = loaded_model(x)\n    y = nn.Softmax()(result)\n    for n, x in enumerate(y[0]):\n        print(f\"{mapping[n]}: {100*x:.2f}%\")\n</pre> def sentiment_score(comment):     mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}     inputs = tokenizer(comment, return_tensors='pt')     ids = inputs[\"input_ids\"]     token_type_ids = inputs[\"token_type_ids\"]     mask = inputs[\"attention_mask\"]      x = {         'ids': ids,         'mask': mask,         'token_type_ids': token_type_ids         }     result = loaded_model(x)     y = nn.Softmax()(result)     for n, x in enumerate(y[0]):         print(f\"{mapping[n]}: {100*x:.2f}%\") In\u00a0[34]: Copied! <pre>sentiment_score(\"I hate watching this\")\n</pre> sentiment_score(\"I hate watching this\") <pre>Negative: 99.76%\nNeutral: 0.15%\nPositive: 0.09%\n</pre> In\u00a0[35]: Copied! <pre>sentiment_score(\"I really love this ring, it's so beautiful !\")\n</pre> sentiment_score(\"I really love this ring, it's so beautiful !\") <pre>Negative: 0.06%\nNeutral: 0.10%\nPositive: 99.84%\n</pre> In\u00a0[39]: Copied! <pre>sentiment_score(\"This place is a scam, i highly don't recommend\")\n</pre> sentiment_score(\"This place is a scam, i highly don't recommend\") <pre>Negative: 97.73%\nNeutral: 0.49%\nPositive: 1.77%\n</pre> In\u00a0[40]: Copied! <pre>sentiment_score(\"I don't know what to say\")\n</pre> sentiment_score(\"I don't know what to say\") <pre>Negative: 0.07%\nNeutral: 99.87%\nPositive: 0.06%\n</pre> In\u00a0[42]: Copied! <pre>sentiment_score(\"The sky is blue\")\n</pre> sentiment_score(\"The sky is blue\") <pre>Negative: 0.06%\nNeutral: 99.89%\nPositive: 0.06%\n</pre> In\u00a0[48]: Copied! <pre>sentiment_score(\"the cartier trinity is ugly bruh\")\n</pre> sentiment_score(\"the cartier trinity is ugly bruh\")  <pre>Negative: 96.94%\nNeutral: 2.93%\nPositive: 0.13%\n</pre> In\u00a0[49]: Copied! <pre>sentiment_score(\"I have no idea what wedding band to get for this - any ideas? for now I have the cartier trinity ring underneath which works surprisingly well but will be changed to the wedding band, once we get married in 2025!\")\n</pre> sentiment_score(\"I have no idea what wedding band to get for this - any ideas? for now I have the cartier trinity ring underneath which works surprisingly well but will be changed to the wedding band, once we get married in 2025!\")  <pre>Negative: 6.61%\nNeutral: 0.61%\nPositive: 92.78%\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#bert-sentiment-classifier-with-pytorch","title":"BERT Sentiment Classifier with PyTorch\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#local-training","title":"Local training\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#data-preprocessing","title":"Data preprocessing\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#load-pretrained-bert-model-and-tokenizer","title":"Load pretrained BERT model and tokenizer\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#create-dataloader","title":"Create dataloader\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#define-model","title":"Define model\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#training-and-evaluation-utils","title":"Training and evaluation utils\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#train-model-on-vertex","title":"Train model on Vertex\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#1-option-1-via-gcloud-create-a-little-jobsh-script-as-below","title":"1. Option 1: via gcloud : Create a little <code>job.sh</code> script as below\u00b6","text":"<pre># job.sh\n\nexport PROJECT_ID=...\nexport BUCKET=\"csb-sentiment-analysis\"\nexport REGION=\"europe-west4\"\nexport SERVICE_ACCOUNT=...\nexport JOB_NAME=\"pytorch_bert_training\"\nexport MACHINE_TYPE=\"n1-standard-8\"  # We can specify GPUs here\nexport ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\nexport IMAGE_URI=\"eu.gcr.io/$PROJECT_ID/pt_bert_sentiment:dev\"\n\n\ngcloud ai custom-jobs create \\\n--region=$REGION \\\n--display-name=$JOB_NAME \\\n--worker-pool-spec=machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=1,replica-count=1,container-image-uri=$IMAGE_URI \\\n--service-account=$SERVICE_ACCOUNT \\\n--args=\\\n--training-file=gs://$BUCKET/data/train.csv,\\\n--validation-file=gs://$BUCKET/data/eval.csv,\\\n--testing-file=gs://$BUCKET/data/test.csv,\\\n--job-dir=gs://$BUCKET/model/model.pt,\\\n--epochs=1,\\\n--batch-size=128,\\\n--learning-rate=0.0001\n</pre>"},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#option-2-via-the-python-client","title":"option 2: via the Python Client\u00b6","text":"<pre>from google.cloud import aiplatform\n\nPROJECT_ID=...\nBUCKET=\"csb-sentiment-analysis\"\n\nmy_job = aiplatform.CustomContainerTrainingJob(\n    display_name='pytorch_bert_training',\n    container_uri='eu.gcr.io/{PROJECT_ID}/pt_bert_sentiment:dev',\n    staging_bucket='gs://{BUCKET}')\n    \nmy_job.run(replica_count=1,\n           machine_type='n1-standard-8',\n           accelerator_type='NVIDIA_TESLA_T4',\n           accelerator_count=1)\n</pre>"},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#monitor-the-job","title":"Monitor the job\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20PyTorch/#inference-with-trained-model","title":"Inference with trained model\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20Tensorflow/","title":"BERT Sentiment Classifier with Tensorflow","text":"In\u00a0[158]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom sklearn.model_selection import train_test_split\n\ntf.get_logger().setLevel('ERROR')\n</pre> import pandas as pd import numpy as np  import tensorflow as tf import tensorflow_hub as hub import tensorflow_text as text  from sklearn.model_selection import train_test_split  tf.get_logger().setLevel('ERROR') In\u00a0[4]: Copied! <pre>%load_ext watermark\n</pre> %load_ext watermark In\u00a0[159]: Copied! <pre>%watermark -p tensorflow,tensorflow_hub,tensorflow_text,sklearn\n</pre> %watermark -p tensorflow,tensorflow_hub,tensorflow_text,sklearn <pre>tensorflow     : 2.13.1\ntensorflow_hub : 0.16.1\ntensorflow_text: 2.13.0\nsklearn        : 1.3.2\n\n</pre> In\u00a0[138]: Copied! <pre># Data downloaded from https://www.kaggle.com/datasets/farisdurrani/sentimentsearch\n\ndf=pd.concat([\n    pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),\n    pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\")\n])\n</pre> # Data downloaded from https://www.kaggle.com/datasets/farisdurrani/sentimentsearch  df=pd.concat([     pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),     pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\") ]) In\u00a0[149]: Copied! <pre>len(df)\n</pre> len(df) Out[149]: <pre>821081</pre> In\u00a0[150]: Copied! <pre>df.head()\n</pre> df.head() Out[150]: platform bodyText sentiment date country Target 0 Twitter @Kenichan I dived many times for the ball. Man... 0.4939 2009-04-06 NaN 1.0 1 Twitter @nationwideclass no, it's not behaving at all.... -0.4939 2009-04-06 NaN -1.0 2 Twitter Need a hug 0.4767 2009-04-06 NaN 1.0 3 Twitter @LOLTrish hey  long time no see! Yes.. Rains a... 0.6208 2009-04-06 NaN 1.0 4 Twitter @Tatiana_K nope they didn't have it 0.0000 2009-04-06 NaN 0.0 In\u00a0[151]: Copied! <pre>df = df.dropna(subset=['sentiment'], axis=0)\n</pre> df = df.dropna(subset=['sentiment'], axis=0) In\u00a0[154]: Copied! <pre>df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\ndf.head()\n</pre> df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int) df.head() <pre>/var/folders/k_/l97qqwps37qgq6txcj84v9b00000gn/T/ipykernel_10352/1955999727.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n</pre> Out[154]: platform bodyText sentiment date country Target 0 Twitter @Kenichan I dived many times for the ball. Man... 0.4939 2009-04-06 NaN 2 1 Twitter @nationwideclass no, it's not behaving at all.... -0.4939 2009-04-06 NaN 0 2 Twitter Need a hug 0.4767 2009-04-06 NaN 2 3 Twitter @LOLTrish hey  long time no see! Yes.. Rains a... 0.6208 2009-04-06 NaN 2 4 Twitter @Tatiana_K nope they didn't have it 0.0000 2009-04-06 NaN 1 In\u00a0[155]: Copied! <pre>df.Target.value_counts(normalize=True)\n</pre> df.Target.value_counts(normalize=True) Out[155]: <pre>Target\n2    0.476433\n1    0.262097\n0    0.261471\nName: proportion, dtype: float64</pre> In\u00a0[188]: Copied! <pre>df = df.sample(frac=0.5)\n</pre> df = df.sample(frac=0.5) In\u00a0[189]: Copied! <pre>X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2)\nX_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5)\n</pre> X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2) X_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5) In\u00a0[190]: Copied! <pre>print(f\"Train : ({X_train.shape, y_train.shape}) samples\")\nprint(f\"Val : ({X_val.shape, y_val.shape}) samples\")\nprint(f\"Test : ({X_test.shape, y_test.shape}) samples\")\n</pre> print(f\"Train : ({X_train.shape, y_train.shape}) samples\") print(f\"Val : ({X_val.shape, y_val.shape}) samples\") print(f\"Test : ({X_test.shape, y_test.shape}) samples\") <pre>Train : (((328355,), (328355,))) samples\nVal : (((41044,), (41044,))) samples\nTest : (((41045,), (41045,))) samples\n</pre> In\u00a0[191]: Copied! <pre>train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n</pre> train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().prefetch(buffer_size=tf.data.AUTOTUNE) test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().prefetch(buffer_size=tf.data.AUTOTUNE) In\u00a0[192]: Copied! <pre>for x, y in train_ds.batch(4).take(1):\n    print(x, y)\n</pre> for x, y in train_ds.batch(4).take(1):     print(x, y) <pre>tf.Tensor(\n[b'just back from a gig!, imadethis and Furlo rawked! met up with some old friends too &amp;gt;.&amp;lt; thanks for all the nice comments '\n b'@nerdist YAY FOR @COLINMELOY '\n b\"Wow am I tired..was up talking to my bffl Daisha til 12ish..she was talking some sense into me..Maybe I'll eat some cake to wake up \"\n b'has lost her voice, '], shape=(4,), dtype=string) tf.Tensor([2 2 2 0], shape=(4,), dtype=int64)\n</pre> <pre>2024-05-10 16:46:28.695647: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</pre> In\u00a0[193]: Copied! <pre>y_test.value_counts()\n</pre> y_test.value_counts() Out[193]: <pre>Target\n2    19561\n1    10786\n0    10698\nName: count, dtype: int64</pre> In\u00a0[32]: Copied! <pre>BERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\nTOKENIZER_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\nBERT_MODEL_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\ntokenizer = hub.KerasLayer(TOKENIZER_URL)\nbert_model = hub.KerasLayer(BERT_MODEL_URL)\n</pre> BERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2' TOKENIZER_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3' BERT_MODEL_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1' tokenizer = hub.KerasLayer(TOKENIZER_URL) bert_model = hub.KerasLayer(BERT_MODEL_URL) In\u00a0[33]: Copied! <pre>text_test = ['sometimes i wish i was a panda']\ntext_preprocessed = tokenizer(text_test)\n</pre> text_test = ['sometimes i wish i was a panda'] text_preprocessed = tokenizer(text_test) In\u00a0[34]: Copied! <pre>print(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :10]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :10]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :10]}')\n</pre> print(f'Keys       : {list(text_preprocessed.keys())}') print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}') print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :10]}') print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :10]}') print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :10]}') <pre>Keys       : ['input_mask', 'input_type_ids', 'input_word_ids']\nShape      : (1, 128)\nWord Ids   : [  101  2823  1045  4299  1045  2001  1037 25462   102     0]\nInput Mask : [1 1 1 1 1 1 1 1 1 0]\nType Ids   : [0 0 0 0 0 0 0 0 0 0]\n</pre> In\u00a0[35]: Copied! <pre>bert_model(text_preprocessed)\n</pre> bert_model(text_preprocessed) Out[35]: <pre>{'default': &lt;tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n array([[-0.9999753 ,  0.16285095, -0.9991155 ,  0.94176614, -0.99974805,\n          0.06312796, -0.998636  ,  0.4763149 ,  0.13516821, -0.02416325,\n         -0.6557846 , -0.04298883, -0.1316844 ,  1.        , -0.88311327,\n         -0.8912707 ,  0.90224254,  0.00613371, -0.8089084 ,  0.99620104,\n          0.9636094 ,  0.08133331,  0.99200433,  0.95425904, -0.999996  ,\n          0.05177542, -0.9996473 ,  0.9649326 ,  0.9903885 ,  0.07173721,\n          0.10204275,  0.09355631, -0.97235924, -0.15794337,  0.7986553 ,\n          0.9996173 , -0.6733732 , -0.155746  ,  0.8948879 , -0.99969214,\n          0.7239788 ,  0.9898049 , -0.9991481 ,  0.9894511 , -0.9999779 ,\n         -0.22439677, -0.99981123,  0.99636346,  0.9813393 ,  0.9832557 ,\n          0.9895193 , -0.43293884,  0.0314689 ,  0.9930125 ,  0.99891675,\n          0.999489  , -0.9907511 , -0.68675447,  0.5443378 , -0.77976435,\n         -0.05668417,  0.29758197, -0.84936285,  0.9744014 , -0.7764867 ,\n         -0.9999987 ,  0.7285832 ,  0.6136133 ,  0.8953313 ,  0.8456571 ,\n          0.9994263 , -0.03930187, -0.9997915 ,  0.10469574,  0.86636907,\n         -0.99544203, -0.26130468,  0.12262645, -0.91191435,  0.20496912,\n          0.6147858 , -0.22474639, -0.9962038 , -0.9999554 ,  0.99960154,\n         -0.8798603 ,  0.5636777 ,  0.557866  , -0.18760201,  0.6673624 ,\n         -0.9041123 ,  0.99915344, -0.94210184,  0.9994909 , -0.31144068,\n          0.40841162, -0.9936153 , -0.72045016, -0.9998872 , -0.9636848 ,\n         -0.9957678 ,  0.80706525, -0.9997106 , -0.9754736 , -0.99751425,\n          0.92712474, -0.9940275 , -0.9979612 , -0.46771565,  0.9972338 ,\n          0.9993334 ,  0.97802055, -0.7866818 ,  0.99930024, -0.9999989 ,\n          0.11323293,  0.95258844,  0.56515914,  0.21961828, -0.97894686,\n          0.26843572, -0.9999831 , -0.76925075,  0.90130097, -0.9996943 ,\n          0.93291706,  0.8831948 ,  0.99951047]], dtype=float32)&gt;,\n 'encoder_outputs': [&lt;tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n  array([[[-0.4104218 ,  0.7154593 , -6.4672914 , ..., -0.64346975,\n           -0.6155251 ,  0.51599675],\n          [-0.6587324 ,  1.3788172 ,  0.07930201, ..., -1.9233143 ,\n            0.69141245,  0.47687203],\n          [-2.4409764 ,  1.15798   ,  0.6488332 , ..., -4.0650206 ,\n           -0.51734275, -0.04621631],\n          ...,\n          [-1.394653  ,  0.4066436 , -0.25429088, ..., -1.3632857 ,\n            0.59795535,  1.0136952 ],\n          [-1.0741649 ,  0.6947189 , -0.23569326, ..., -1.1689848 ,\n            0.45016924,  0.924365  ],\n          [-0.7173106 ,  0.92680454, -0.17283653, ..., -0.9315816 ,\n            0.07558171,  0.79331887]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n  array([[[-1.1300321e+00, -9.9121027e-02, -3.1297979e+00, ...,\n            3.6399342e-02, -5.6742907e-01,  1.5096064e+00],\n          [-6.6179633e-01,  9.4794190e-01, -3.8296500e-01, ...,\n           -7.0248544e-04,  5.1630849e-01,  1.5306761e+00],\n          [-1.2262596e+00, -1.2711237e-01, -1.0036832e-01, ...,\n           -1.5480896e+00, -8.4113240e-01,  6.0122180e-01],\n          ...,\n          [-1.0829040e+00, -2.5349934e-02, -7.6939929e-01, ...,\n           -7.1985650e-01, -4.6738574e-01,  1.7646772e+00],\n          [-8.4669703e-01,  7.9183981e-02, -6.3098383e-01, ...,\n           -6.3131255e-01, -4.4220948e-01,  1.6185466e+00],\n          [-3.8328338e-01,  1.8087508e-01, -5.4923224e-01, ...,\n           -4.9273196e-01, -4.5769203e-01,  1.1787802e+00]]], dtype=float32)&gt;],\n 'pooled_output': &lt;tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n array([[-0.9999753 ,  0.16285095, -0.9991155 ,  0.94176614, -0.99974805,\n          0.06312796, -0.998636  ,  0.4763149 ,  0.13516821, -0.02416325,\n         -0.6557846 , -0.04298883, -0.1316844 ,  1.        , -0.88311327,\n         -0.8912707 ,  0.90224254,  0.00613371, -0.8089084 ,  0.99620104,\n          0.9636094 ,  0.08133331,  0.99200433,  0.95425904, -0.999996  ,\n          0.05177542, -0.9996473 ,  0.9649326 ,  0.9903885 ,  0.07173721,\n          0.10204275,  0.09355631, -0.97235924, -0.15794337,  0.7986553 ,\n          0.9996173 , -0.6733732 , -0.155746  ,  0.8948879 , -0.99969214,\n          0.7239788 ,  0.9898049 , -0.9991481 ,  0.9894511 , -0.9999779 ,\n         -0.22439677, -0.99981123,  0.99636346,  0.9813393 ,  0.9832557 ,\n          0.9895193 , -0.43293884,  0.0314689 ,  0.9930125 ,  0.99891675,\n          0.999489  , -0.9907511 , -0.68675447,  0.5443378 , -0.77976435,\n         -0.05668417,  0.29758197, -0.84936285,  0.9744014 , -0.7764867 ,\n         -0.9999987 ,  0.7285832 ,  0.6136133 ,  0.8953313 ,  0.8456571 ,\n          0.9994263 , -0.03930187, -0.9997915 ,  0.10469574,  0.86636907,\n         -0.99544203, -0.26130468,  0.12262645, -0.91191435,  0.20496912,\n          0.6147858 , -0.22474639, -0.9962038 , -0.9999554 ,  0.99960154,\n         -0.8798603 ,  0.5636777 ,  0.557866  , -0.18760201,  0.6673624 ,\n         -0.9041123 ,  0.99915344, -0.94210184,  0.9994909 , -0.31144068,\n          0.40841162, -0.9936153 , -0.72045016, -0.9998872 , -0.9636848 ,\n         -0.9957678 ,  0.80706525, -0.9997106 , -0.9754736 , -0.99751425,\n          0.92712474, -0.9940275 , -0.9979612 , -0.46771565,  0.9972338 ,\n          0.9993334 ,  0.97802055, -0.7866818 ,  0.99930024, -0.9999989 ,\n          0.11323293,  0.95258844,  0.56515914,  0.21961828, -0.97894686,\n          0.26843572, -0.9999831 , -0.76925075,  0.90130097, -0.9996943 ,\n          0.93291706,  0.8831948 ,  0.99951047]], dtype=float32)&gt;,\n 'sequence_output': &lt;tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n array([[[-1.1300321e+00, -9.9121027e-02, -3.1297979e+00, ...,\n           3.6399342e-02, -5.6742907e-01,  1.5096064e+00],\n         [-6.6179633e-01,  9.4794190e-01, -3.8296500e-01, ...,\n          -7.0248544e-04,  5.1630849e-01,  1.5306761e+00],\n         [-1.2262596e+00, -1.2711237e-01, -1.0036832e-01, ...,\n          -1.5480896e+00, -8.4113240e-01,  6.0122180e-01],\n         ...,\n         [-1.0829040e+00, -2.5349934e-02, -7.6939929e-01, ...,\n          -7.1985650e-01, -4.6738574e-01,  1.7646772e+00],\n         [-8.4669703e-01,  7.9183981e-02, -6.3098383e-01, ...,\n          -6.3131255e-01, -4.4220948e-01,  1.6185466e+00],\n         [-3.8328338e-01,  1.8087508e-01, -5.4923224e-01, ...,\n          -4.9273196e-01, -4.5769203e-01,  1.1787802e+00]]], dtype=float32)&gt;}</pre> In\u00a0[253]: Copied! <pre>class SentimentBERT(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = hub.KerasLayer(TOKENIZER_URL, name='tokenizer')\n        self.bert_model = hub.KerasLayer(BERT_MODEL_URL, trainable=True, name='bert_model')\n        self.dropout = tf.keras.layers.Dropout(0.1)\n        self.final = tf.keras.layers.Dense(3, activation=None)\n        \n    def call(self, inputs):\n        x = self.tokenizer(inputs)\n        x = self.bert_model(x)\n        x = self.dropout(x['pooled_output'])\n        out = self.final(x)\n        return out\n</pre> class SentimentBERT(tf.keras.Model):     def __init__(self):         super().__init__()         self.tokenizer = hub.KerasLayer(TOKENIZER_URL, name='tokenizer')         self.bert_model = hub.KerasLayer(BERT_MODEL_URL, trainable=True, name='bert_model')         self.dropout = tf.keras.layers.Dropout(0.1)         self.final = tf.keras.layers.Dense(3, activation=None)              def call(self, inputs):         x = self.tokenizer(inputs)         x = self.bert_model(x)         x = self.dropout(x['pooled_output'])         out = self.final(x)         return out In\u00a0[254]: Copied! <pre>classifier = SentimentBERT()\n</pre> classifier = SentimentBERT() In\u00a0[255]: Copied! <pre>bert_raw_result = classifier(tf.constant(text_test))\nprint(tf.keras.activations.softmax(bert_raw_result))\n</pre> bert_raw_result = classifier(tf.constant(text_test)) print(tf.keras.activations.softmax(bert_raw_result)) <pre>tf.Tensor([[0.14317669 0.43381873 0.42300454]], shape=(1, 3), dtype=float32)\n</pre> In\u00a0[256]: Copied! <pre>bert_raw_result = classifier(x)\nprint(tf.keras.activations.softmax(bert_raw_result))\n</pre> bert_raw_result = classifier(x) print(tf.keras.activations.softmax(bert_raw_result)) <pre>tf.Tensor(\n[[0.2786571  0.41875377 0.30258903]\n [0.28298354 0.28624907 0.43076733]\n [0.11631086 0.41331226 0.47037688]\n [0.11895362 0.3087614  0.572285  ]], shape=(4, 3), dtype=float32)\n</pre> In\u00a0[\u00a0]: Copied! <pre>tf.keras.utils.plot_model(classifier)\n</pre> tf.keras.utils.plot_model(classifier) In\u00a0[257]: Copied! <pre>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.metrics.SparseCategoricalAccuracy()\noptimizer = tf.keras.optimizers.Adam()\n</pre> loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) metric = tf.metrics.SparseCategoricalAccuracy() optimizer = tf.keras.optimizers.Adam() In\u00a0[258]: Copied! <pre>classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=metric)\n</pre> classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=metric) In\u00a0[259]: Copied! <pre>trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.trainable_weights])\nnon_trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.non_trainable_weights])\ntotal_params = trainable_params + non_trainable_params\n    \nprint(f\"Trainable params : {trainable_params}\")\nprint(f\"Total params : {total_params}\")\nprint(f\"% : {trainable_params/total_params*100:.4f}\")\n</pre> trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.trainable_weights]) non_trainable_params = np.sum([np.prod(v.get_shape()) for v in classifier.non_trainable_weights]) total_params = trainable_params + non_trainable_params      print(f\"Trainable params : {trainable_params}\") print(f\"Total params : {total_params}\") print(f\"% : {trainable_params/total_params*100:.4f}\") <pre>Trainable params : 4386307\nTotal params : 4386308.0\n% : 100.0000\n</pre> In\u00a0[260]: Copied! <pre>print(f'Training model with {BERT_MODEL_NAME}')\nhistory = classifier.fit(\n    x=X_train.values,\n    y=y_train.values,\n    validation_data=(X_val.values, y_val.values),\n    epochs=2,\n    batch_size=512)\n</pre> print(f'Training model with {BERT_MODEL_NAME}') history = classifier.fit(     x=X_train.values,     y=y_train.values,     validation_data=(X_val.values, y_val.values),     epochs=2,     batch_size=512) <pre>Training model with small_bert/bert_en_uncased_L-2_H-128_A-2\nEpoch 1/2\n642/642 [==============================] - 3013s 5s/step - loss: 0.3813 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.2611 - val_sparse_categorical_accuracy: 0.9037\nEpoch 2/2\n642/642 [==============================] - 2587s 4s/step - loss: 0.2471 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2170 - val_sparse_categorical_accuracy: 0.9204\n</pre> In\u00a0[262]: Copied! <pre>classifier.save(\"bert_classifier.tf\")\n</pre> classifier.save(\"bert_classifier.tf\") In\u00a0[263]: Copied! <pre>reverse_labels = {v: k for k, v in sentiment_to_target.items()}\n</pre> reverse_labels = {v: k for k, v in sentiment_to_target.items()} In\u00a0[264]: Copied! <pre>def sentiment_score(comment):\n    inp = tf.data.Dataset.from_tensors(comment).batch(1)\n    for x in inp.take(1):\n        print(x)\n    bert_raw_result = classifier(x)\n    y = tf.keras.activations.softmax(bert_raw_result)\n    print(y)\n    for n, x in enumerate(y[0]):\n        print(f\"{reverse_labels[n]}: {100*x:.2f}%\")\n</pre> def sentiment_score(comment):     inp = tf.data.Dataset.from_tensors(comment).batch(1)     for x in inp.take(1):         print(x)     bert_raw_result = classifier(x)     y = tf.keras.activations.softmax(bert_raw_result)     print(y)     for n, x in enumerate(y[0]):         print(f\"{reverse_labels[n]}: {100*x:.2f}%\") In\u00a0[265]: Copied! <pre>bert_raw_result = classifier(tf.constant(text_test))\nprint(tf.keras.activations.softmax(bert_raw_result))\n</pre> bert_raw_result = classifier(tf.constant(text_test)) print(tf.keras.activations.softmax(bert_raw_result)) <pre>tf.Tensor([[7.5321941e-04 1.0718697e-03 9.9817491e-01]], shape=(1, 3), dtype=float32)\n</pre> In\u00a0[266]: Copied! <pre>sentiment_score(\"I hate watching this\")\n</pre> sentiment_score(\"I hate watching this\") <pre>tf.Tensor([b'I hate watching this'], shape=(1,), dtype=string)\ntf.Tensor([[0.997603   0.00139782 0.00099914]], shape=(1, 3), dtype=float32)\nNegative: 99.76%\nNeutral: 0.14%\nPositive: 0.10%\n</pre> In\u00a0[267]: Copied! <pre>sentiment_score(\"I really love this ring, it's so beautiful !\")\n</pre> sentiment_score(\"I really love this ring, it's so beautiful !\") <pre>tf.Tensor([b\"I really love this ring, it's so beautiful !\"], shape=(1,), dtype=string)\ntf.Tensor([[4.4591504e-04 1.4268260e-03 9.9812728e-01]], shape=(1, 3), dtype=float32)\nNegative: 0.04%\nNeutral: 0.14%\nPositive: 99.81%\n</pre> In\u00a0[268]: Copied! <pre>sentiment_score(\"This place is a scam, i highly disrecommend\")\n</pre> sentiment_score(\"This place is a scam, i highly disrecommend\") <pre>tf.Tensor([b'This place is a scam, i highly disrecommend'], shape=(1,), dtype=string)\ntf.Tensor([[0.68084687 0.03579305 0.28336   ]], shape=(1, 3), dtype=float32)\nNegative: 68.08%\nNeutral: 3.58%\nPositive: 28.34%\n</pre> In\u00a0[269]: Copied! <pre>sentiment_score(\"I don't know what to say\")\n</pre> sentiment_score(\"I don't know what to say\") <pre>tf.Tensor([b\"I don't know what to say\"], shape=(1,), dtype=string)\ntf.Tensor([[7.085762e-04 9.987644e-01 5.271097e-04]], shape=(1, 3), dtype=float32)\nNegative: 0.07%\nNeutral: 99.88%\nPositive: 0.05%\n</pre> <pre># %%writefile ../app/   msrc/models/tf_bert.py\nimport pandas as pd\nimport numpy as np\nimport argparse\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport logging\n\nfrom sklearn.model_selection import train_test_split\n\nlogging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG)\nlogging.getLogger().setLevel(logging.INFO)\n\nBERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\nTOKENIZER_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\nBERT_MODEL_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n\n\nclass SentimentBERT(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = hub.KerasLayer(TOKENIZER_URL, name='tokenizer')\n        self.bert_model = hub.KerasLayer(BERT_MODEL_URL, trainable=True, name='bert_model')\n        self.dropout = tf.keras.layers.Dropout(0.1)\n        self.final = tf.keras.layers.Dense(3, activation=None)\n        \n    def call(self, inputs):\n        x = self.tokenizer(inputs)\n        x = self.bert_model(x)\n        x = self.dropout(x['pooled_output'])\n        out = self.final(x)\n        return out\n    \n    \ndef train_and_evaluate(**params):\n    \n    epochs = int(params.get('epochs'))\n    batch_size = int(params.get('batch_size'))\n    learning_rate = float(params.get('learning_rate'))\n    \n    df=pd.concat([\n        pd.read_csv(\"data/farisdurrani/twitter_filtered.csv\"),\n        pd.read_csv(\"data/farisdurrani/facebook_filtered.csv\")\n    ])\n    df = df.dropna(subset=['sentiment'], axis=0)\n    df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n\n    X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2)\n    X_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5)\n    \n    logging.info(f\"Train : ({X_train.shape, y_train.shape}) samples\")\n    logging.info(f\"Val : ({X_val.shape, y_val.shape}) samples\")\n    logging.info(f\"Test : ({X_test.shape, y_test.shape}) samples\")\n    \n    classifier = SentimentBERT()\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    metric = tf.metrics.SparseCategoricalAccuracy()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    \n    classifier.compile(optimizer=optimizer, loss=loss_fn, metrics=metric)\n    \n    stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', \n        min_delta=0, \n        patience=10, \n        verbose=2, \n        mode='min',\n        baseline=None, \n        restore_best_weights=True\n    )\n\n    logging.info(f'Training model with {BERT_MODEL_NAME}')\n    if args.dry_run:\n        logging.info(\"Dry run mode\")\n        epochs = 1\n        steps_per_epoch = 1\n    else:\n        steps_per_epoch = None\n        \n    history = classifier.fit(\n        x=X_train.values,\n        y=y_train.values,\n        validation_data=(X_val.values, y_val.values),\n        epochs=epochs,\n        batch_size=batch_size,\n        steps_per_epoch=steps_per_epoch,\n        callbacks=[stopping])\n    \n    if args.dry_run:\n        # If dry run, we do not run the evaluation\n        return None\n    \n    res = classifier.evaluate(\n        x=X_test.values, \n        y=y_test.values,\n        verbose=2\n    )\n    \n    metrics = {\n        'train_acc': history.history['accuracy'],\n        'val_acc': history.history['val_accuracy'],\n        'test_acc': res[-1],\n    }\n    logging.info(metrics)\n    \n    # save model and architecture to single file\n    if params.get('job_dir') is None:\n        logging.warning(\"No job dir provided, model will not be saved\")\n    else:\n        logging.info(\"Saving model to {} \".format(params.get('job_dir')))\n        classifier.save(params.get('job_dir'))\n    logging.info(\"Bye bye\")\n    \n    \nif __name__ == '__main__':\n    # Create arguments here\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--job-dir', required=True)\n    parser.add_argument('--epochs', type=float, default=2)\n    parser.add_argument('--batch-size', type=float, default=1024)\n    parser.add_argument('--learning-rate', type=float, default=0.01)\n    parser.add_argument('--dry-run', action=\"store_true\")\n\n    # Parse them\n    args, _ = parser.parse_known_args()\n\n    # Execute training\n    train_and_evaluate(\n        job_dir=args.job_dir,\n        batch_size=args.batch_size,\n        learning_rate=args.learning_rate,\n        epochs=args.epochs\n    )\n</pre> In\u00a0[273]: Copied! <pre>%%writefile ../app/src/models/tensorflow_bert/requirements.txt\n\ntensorflow==2.13.1\ntensorflow_hub==0.16.1\ntensorflow_text==2.13.0\nscikit-learn==1.3.2\n</pre> %%writefile ../app/src/models/tensorflow_bert/requirements.txt  tensorflow==2.13.1 tensorflow_hub==0.16.1 tensorflow_text==2.13.0 scikit-learn==1.3.2 <pre>Writing ../app/src/models/tensorflow_bert/requirements.txt\n</pre> In\u00a0[276]: Copied! <pre>PROJECT_ID = ...\nIMAGE_NAME=f'bert_tf_sentiment'\nIMAGE_TAG='latest'\nIMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n\n!gcloud builds submit --tag $IMAGE_URI ../app/src/Dockerfile\n</pre> PROJECT_ID = ... IMAGE_NAME=f'bert_tf_sentiment' IMAGE_TAG='latest' IMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)  !gcloud builds submit --tag $IMAGE_URI ../app/src/Dockerfile <pre>/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'oauth2.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\nReauthentication required.\n/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'oauth2.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/Users/benjamin.etienne/google-cloud-sdk/lib/third_party/urllib3/connectionpool.py:1060: InsecureRequestWarning: Unverified HTTPS request is being made to host 'reauth.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\nERROR: (gcloud.builds.submit) There was a problem refreshing your current auth tokens: Reauthentication failed. Please run `gcloud auth login` to complete reauthentication with SAML.\nPlease run:\n\n  $ gcloud auth login\n\nto obtain new credentials.\n\nIf you have already logged in with a different account, run:\n\n  $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Sentiment%20Analysis/BERT%20Sentiment%20Classifier%20with%20Tensorflow/#bert-sentiment-classifier-with-tensorflow","title":"BERT Sentiment Classifier with Tensorflow\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/CatBoost%20Sentiment%20Classifier/","title":"CatBoost Sentiment Classifier","text":"In\u00a0[11]: Copied! <pre>import pandas as pd\nimport catboost as cb\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n</pre> import pandas as pd import catboost as cb import numpy as np import seaborn as sns  from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split In\u00a0[7]: Copied! <pre># https://towardsdatascience.com/unconventional-sentiment-analysis-bert-vs-catboost-90645f2437a9\n</pre> # https://towardsdatascience.com/unconventional-sentiment-analysis-bert-vs-catboost-90645f2437a9 In\u00a0[53]: Copied! <pre>%load_ext watermark\n</pre> %load_ext watermark In\u00a0[54]: Copied! <pre>%watermark -p catboost,pandas,numpy,sklearn\n</pre> %watermark -p catboost,pandas,numpy,sklearn <pre>catboost: 1.0.4\npandas  : 1.4.2\nnumpy   : 1.22.4\nsklearn : 0.0\n\n</pre> In\u00a0[8]: Copied! <pre># Data downloaded from https://www.kaggle.com/datasets/farisdurrani/sentimentsearch\n\ndf=pd.concat([\n    pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),\n    pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\")\n])\n</pre> # Data downloaded from https://www.kaggle.com/datasets/farisdurrani/sentimentsearch  df=pd.concat([     pd.read_csv(\"../data/farisdurrani/twitter_filtered.csv\"),     pd.read_csv(\"../data/farisdurrani/facebook_filtered.csv\") ]) In\u00a0[9]: Copied! <pre>df = df.dropna(subset=['sentiment'], axis=0)\n</pre> df = df.dropna(subset=['sentiment'], axis=0) In\u00a0[12]: Copied! <pre>df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\ndf.head()\n</pre> df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int) df.head() Out[12]: platform bodyText sentiment date country Target 0 Twitter @Kenichan I dived many times for the ball. Man... 0.4939 2009-04-06 NaN 2 1 Twitter @nationwideclass no, it's not behaving at all.... -0.4939 2009-04-06 NaN 0 2 Twitter Need a hug 0.4767 2009-04-06 NaN 2 3 Twitter @LOLTrish hey  long time no see! Yes.. Rains a... 0.6208 2009-04-06 NaN 2 4 Twitter @Tatiana_K nope they didn't have it 0.0000 2009-04-06 NaN 1 In\u00a0[13]: Copied! <pre>X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2)\nX_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5)\n</pre> X_train, _X, y_train, _y = train_test_split(df['bodyText'], df['Target'], stratify=df['Target'], test_size=0.2) X_val, X_test, y_val, y_test = train_test_split(_X, _y, stratify=_y, test_size=0.5) In\u00a0[18]: Copied! <pre>X_val\n</pre> X_val Out[18]: <pre>105526                @lilgary take what back? Im lost  lol\n392442    my throat is sore, my hair is a mess and i can...\n477580    uploaded more photos pheww  www.mileymandyfan8...\n160495                              Plz. I want Fun Ranch. \n86988     @graceamy I was being childish lol cos one of ...\n                                ...                        \n293470    Sytycd I love the ChBABE, but that didn't look...\n519842    @Miss_Melbourne thank you for the #ff recco mu...\n636815              @boxdog looks like the matrix in blue! \n476164    Watching never back down..threw powder in my b...\n221148    Everyone gets to lay out by the pool except fo...\nName: bodyText, Length: 82089, dtype: object</pre> In\u00a0[20]: Copied! <pre>train_pool = cb.Pool(\n    data=X_train.to_frame('content'),\n    label=y_train,\n    text_features=['content']\n)\nvalid_pool = cb.Pool(\n    data=X_val.to_frame('content'), \n    label=y_val,\n    text_features=['content']\n)\n</pre> train_pool = cb.Pool(     data=X_train.to_frame('content'),     label=y_train,     text_features=['content'] ) valid_pool = cb.Pool(     data=X_val.to_frame('content'),      label=y_val,     text_features=['content'] ) In\u00a0[45]: Copied! <pre>model = cb.CatBoostClassifier(\n    iterations = 500,\n    learning_rate = 0.1,\n    eval_metric=\"Accuracy\",\n    tokenizers=[\n        {\n            'tokenizer_id': 'Sense',\n            'separator_type': 'BySense',\n            'lowercasing': 'True',\n            'token_types':['Word', 'Number', 'SentenceBreak'],\n            'sub_tokens_policy':'SeveralTokens'\n        }      \n    ],\n    dictionaries = [\n        {\n            'dictionary_id': 'Word',\n            'max_dictionary_size': '50000'\n        }\n    ],\n    feature_calcers = [\n        'BoW:top_tokens_count=10000'\n    ]\n)\n</pre> model = cb.CatBoostClassifier(     iterations = 500,     learning_rate = 0.1,     eval_metric=\"Accuracy\",     tokenizers=[         {             'tokenizer_id': 'Sense',             'separator_type': 'BySense',             'lowercasing': 'True',             'token_types':['Word', 'Number', 'SentenceBreak'],             'sub_tokens_policy':'SeveralTokens'         }           ],     dictionaries = [         {             'dictionary_id': 'Word',             'max_dictionary_size': '50000'         }     ],     feature_calcers = [         'BoW:top_tokens_count=10000'     ] ) In\u00a0[46]: Copied! <pre>model.fit(\n        train_pool,\n        eval_set=valid_pool,\n        verbose=10,\n        plot=True,\n        use_best_model=True)\n</pre> model.fit(         train_pool,         eval_set=valid_pool,         verbose=10,         plot=True,         use_best_model=True) <pre>MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))</pre> <pre>0:\tlearn: 0.4903656\ttest: 0.4901875\tbest: 0.4901875 (0)\ttotal: 5.54s\tremaining: 46m 6s\n10:\tlearn: 0.5222808\ttest: 0.5222502\tbest: 0.5222502 (10)\ttotal: 1m\tremaining: 45m 10s\n20:\tlearn: 0.5347627\ttest: 0.5343834\tbest: 0.5343834 (20)\ttotal: 2m 4s\tremaining: 47m 27s\n30:\tlearn: 0.5463553\ttest: 0.5452618\tbest: 0.5452618 (30)\ttotal: 2m 57s\tremaining: 44m 46s\n40:\tlearn: 0.6569901\ttest: 0.6558735\tbest: 0.6558735 (40)\ttotal: 3m 53s\tremaining: 43m 37s\n50:\tlearn: 0.6830229\ttest: 0.6824057\tbest: 0.6824057 (50)\ttotal: 4m 53s\tremaining: 43m 3s\n60:\tlearn: 0.6965236\ttest: 0.6968900\tbest: 0.6968900 (60)\ttotal: 5m 47s\tremaining: 41m 43s\n70:\tlearn: 0.7183155\ttest: 0.7188052\tbest: 0.7188052 (70)\ttotal: 6m 48s\tremaining: 41m 6s\n80:\tlearn: 0.7291681\ttest: 0.7299882\tbest: 0.7299882 (80)\ttotal: 7m 43s\tremaining: 39m 55s\n90:\tlearn: 0.7396613\ttest: 0.7404890\tbest: 0.7404890 (90)\ttotal: 8m 36s\tremaining: 38m 42s\n100:\tlearn: 0.7469598\ttest: 0.7479078\tbest: 0.7489067 (99)\ttotal: 9m 34s\tremaining: 37m 48s\n110:\tlearn: 0.7544807\ttest: 0.7559234\tbest: 0.7559234 (110)\ttotal: 10m 42s\tremaining: 37m 31s\n120:\tlearn: 0.7618401\ttest: 0.7633179\tbest: 0.7633179 (120)\ttotal: 11m 45s\tremaining: 36m 49s\n130:\tlearn: 0.7676600\ttest: 0.7693235\tbest: 0.7693235 (130)\ttotal: 12m 45s\tremaining: 35m 55s\n140:\tlearn: 0.7709720\ttest: 0.7726127\tbest: 0.7726370 (139)\ttotal: 13m 32s\tremaining: 34m 28s\n150:\tlearn: 0.7776096\ttest: 0.7800436\tbest: 0.7800436 (150)\ttotal: 14m 18s\tremaining: 33m 5s\n160:\tlearn: 0.7802272\ttest: 0.7827236\tbest: 0.7828698 (158)\ttotal: 15m 5s\tremaining: 31m 45s\n170:\tlearn: 0.7848898\ttest: 0.7871213\tbest: 0.7871213 (170)\ttotal: 15m 53s\tremaining: 30m 33s\n180:\tlearn: 0.7880495\ttest: 0.7906540\tbest: 0.7906784 (179)\ttotal: 16m 46s\tremaining: 29m 33s\n190:\tlearn: 0.7908209\ttest: 0.7935290\tbest: 0.7935290 (190)\ttotal: 17m 33s\tremaining: 28m 23s\n200:\tlearn: 0.7940491\ttest: 0.7965379\tbest: 0.7966110 (198)\ttotal: 18m 6s\tremaining: 26m 56s\n210:\tlearn: 0.7973215\ttest: 0.7998027\tbest: 0.7998027 (210)\ttotal: 18m 46s\tremaining: 25m 43s\n220:\tlearn: 0.7997487\ttest: 0.8028481\tbest: 0.8028481 (220)\ttotal: 19m 23s\tremaining: 24m 28s\n230:\tlearn: 0.8034642\ttest: 0.8063200\tbest: 0.8063200 (230)\ttotal: 20m 15s\tremaining: 23m 35s\n240:\tlearn: 0.8052793\ttest: 0.8078671\tbest: 0.8078914 (239)\ttotal: 20m 52s\tremaining: 22m 26s\n250:\tlearn: 0.8069285\ttest: 0.8090609\tbest: 0.8092436 (248)\ttotal: 21m 29s\tremaining: 21m 19s\n260:\tlearn: 0.8094608\ttest: 0.8115216\tbest: 0.8115216 (260)\ttotal: 21m 56s\tremaining: 20m 5s\n270:\tlearn: 0.8111952\ttest: 0.8132880\tbest: 0.8132880 (270)\ttotal: 22m 18s\tremaining: 18m 51s\n280:\tlearn: 0.8136392\ttest: 0.8153589\tbest: 0.8153589 (280)\ttotal: 22m 39s\tremaining: 17m 39s\n290:\tlearn: 0.8150188\ttest: 0.8169913\tbest: 0.8169913 (290)\ttotal: 22m 58s\tremaining: 16m 30s\n300:\tlearn: 0.8166573\ttest: 0.8188551\tbest: 0.8188551 (300)\ttotal: 23m 17s\tremaining: 15m 23s\n310:\tlearn: 0.8181024\ttest: 0.8202682\tbest: 0.8202682 (310)\ttotal: 23m 35s\tremaining: 14m 20s\n320:\tlearn: 0.8194378\ttest: 0.8216935\tbest: 0.8216935 (320)\ttotal: 23m 56s\tremaining: 13m 20s\n330:\tlearn: 0.8212971\ttest: 0.8233015\tbest: 0.8233015 (330)\ttotal: 24m 17s\tremaining: 12m 24s\n340:\tlearn: 0.8235842\ttest: 0.8256648\tbest: 0.8256648 (340)\ttotal: 24m 45s\tremaining: 11m 32s\n350:\tlearn: 0.8249745\ttest: 0.8269196\tbest: 0.8269196 (349)\ttotal: 25m 22s\tremaining: 10m 46s\n360:\tlearn: 0.8257678\ttest: 0.8277358\tbest: 0.8277358 (360)\ttotal: 26m 8s\tremaining: 10m 3s\n370:\tlearn: 0.8267698\ttest: 0.8289296\tbest: 0.8289296 (370)\ttotal: 26m 28s\tremaining: 9m 12s\n380:\tlearn: 0.8286078\ttest: 0.8306229\tbest: 0.8306229 (380)\ttotal: 26m 47s\tremaining: 8m 22s\n390:\tlearn: 0.8292701\ttest: 0.8314756\tbest: 0.8314756 (390)\ttotal: 27m 7s\tremaining: 7m 33s\n400:\tlearn: 0.8305797\ttest: 0.8322918\tbest: 0.8322918 (400)\ttotal: 27m 33s\tremaining: 6m 48s\n410:\tlearn: 0.8315984\ttest: 0.8334491\tbest: 0.8334491 (410)\ttotal: 28m\tremaining: 6m 3s\n420:\tlearn: 0.8328136\ttest: 0.8348987\tbest: 0.8349353 (419)\ttotal: 28m 22s\tremaining: 5m 19s\n430:\tlearn: 0.8340089\ttest: 0.8355565\tbest: 0.8357271 (427)\ttotal: 28m 44s\tremaining: 4m 36s\n440:\tlearn: 0.8354098\ttest: 0.8370914\tbest: 0.8370914 (440)\ttotal: 29m 3s\tremaining: 3m 53s\n450:\tlearn: 0.8361331\ttest: 0.8380904\tbest: 0.8381147 (449)\ttotal: 29m 22s\tremaining: 3m 11s\n460:\tlearn: 0.8370148\ttest: 0.8390162\tbest: 0.8390162 (460)\ttotal: 29m 44s\tremaining: 2m 30s\n470:\tlearn: 0.8378858\ttest: 0.8397715\tbest: 0.8397836 (469)\ttotal: 30m 4s\tremaining: 1m 51s\n480:\tlearn: 0.8391345\ttest: 0.8407704\tbest: 0.8407704 (480)\ttotal: 30m 24s\tremaining: 1m 12s\n490:\tlearn: 0.8398639\ttest: 0.8417693\tbest: 0.8417693 (490)\ttotal: 30m 44s\tremaining: 33.8s\n499:\tlearn: 0.8403039\ttest: 0.8423662\tbest: 0.8423662 (499)\ttotal: 31m 1s\tremaining: 0us\n\nbestTest = 0.8423662123\nbestIteration = 499\n\n</pre> Out[46]: <pre>&lt;catboost.core.CatBoostClassifier at 0x137dd7610&gt;</pre> In\u00a0[28]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[47]: Copied! <pre>test_pool = cb.Pool(\n    data=X_test.to_frame('content'), \n    label=y_test,\n    text_features=['content']\n)\ny_hat = model.predict(test_pool)\n</pre> test_pool = cb.Pool(     data=X_test.to_frame('content'),      label=y_test,     text_features=['content'] ) y_hat = model.predict(test_pool) In\u00a0[48]: Copied! <pre>cf_matrix = confusion_matrix(y_test, y_hat)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n</pre> cf_matrix = confusion_matrix(y_test, y_hat) sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues') Out[48]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[49]: Copied! <pre>model.predict_proba([\"I will never go back to this place\"])\n</pre> model.predict_proba([\"I will never go back to this place\"]) Out[49]: <pre>array([0.16020057, 0.63135258, 0.20844686])</pre> In\u00a0[50]: Copied! <pre>model.predict_proba([\"This place is a scam, i highly disrecommend\"])\n</pre> model.predict_proba([\"This place is a scam, i highly disrecommend\"]) Out[50]: <pre>array([0.18762034, 0.5675432 , 0.24483646])</pre> In\u00a0[51]: Copied! <pre>model.predict_proba([\"no, it's not allright to say such things\"])\n</pre> model.predict_proba([\"no, it's not allright to say such things\"]) Out[51]: <pre>array([0.68571312, 0.1558603 , 0.15842659])</pre> In\u00a0[52]: Copied! <pre>model.predict_proba([\"I really love this ring, it's so beautiful !\"])\n</pre> model.predict_proba([\"I really love this ring, it's so beautiful !\"]) Out[52]: <pre>array([0.01407426, 0.00883489, 0.97709085])</pre>"},{"location":"notebooks/Sentiment%20Analysis/Simple%20LSTM/","title":"Simple LSTM Classifier for Radarly Sentiment Analysis","text":"In\u00a0[113]: Copied! <pre>import pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom typing import List\n</pre> import pandas as pd import tensorflow as tf import matplotlib.pyplot as plt import numpy as np  from typing import List In\u00a0[4]: Copied! <pre>%load_ext watermark\n</pre> %load_ext watermark In\u00a0[101]: Copied! <pre>%watermark  --packages tensorflow,pandas,matplotlib\n</pre> %watermark  --packages tensorflow,pandas,matplotlib <pre>tensorflow: 2.13.1\npandas    : 2.0.3\nmatplotlib: 3.7.5\n\n</pre> In\u00a0[36]: Copied! <pre>df_train=pd.read_csv(\"../data/twitter_training.csv\")\ndf_test=pd.read_csv(\"../data/twitter_validation.csv\")\n\ndf_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)\n</pre> df_train=pd.read_csv(\"../data/twitter_training.csv\") df_test=pd.read_csv(\"../data/twitter_validation.csv\")  df_train.dropna(inplace=True) df_test.dropna(inplace=True) In\u00a0[37]: Copied! <pre>df_train.columns = df_test.columns = ['ID', 'entity','sentiment','content']\n</pre> df_train.columns = df_test.columns = ['ID', 'entity','sentiment','content'] In\u00a0[38]: Copied! <pre>df_train.head(10)\n</pre> df_train.head(10) Out[38]: ID entity sentiment content 0 2401 Borderlands Positive I am coming to the borders and I will kill you... 1 2401 Borderlands Positive im getting on borderlands and i will kill you ... 2 2401 Borderlands Positive im coming on borderlands and i will murder you... 3 2401 Borderlands Positive im getting on borderlands 2 and i will murder ... 4 2401 Borderlands Positive im getting into borderlands and i can murder y... 5 2402 Borderlands Positive So I spent a few hours making something for fu... 6 2402 Borderlands Positive So I spent a couple of hours doing something f... 7 2402 Borderlands Positive So I spent a few hours doing something for fun... 8 2402 Borderlands Positive So I spent a few hours making something for fu... 9 2402 Borderlands Positive 2010 So I spent a few hours making something f... In\u00a0[39]: Copied! <pre>df_train.sentiment.value_counts(normalize=True)\n</pre> df_train.sentiment.value_counts(normalize=True) Out[39]: <pre>sentiment\nNegative      0.302156\nPositive      0.279127\nNeutral       0.244719\nIrrelevant    0.173998\nName: proportion, dtype: float64</pre> In\u00a0[40]: Copied! <pre>df_train.entity.value_counts(normalize=True)\n</pre> df_train.entity.value_counts(normalize=True) Out[40]: <pre>entity\nMaddenNFL                            0.032124\nLeagueOfLegends                      0.032124\nCallOfDuty                           0.032110\nVerizon                              0.031962\nTomClancysRainbowSix                 0.031948\nFacebook                             0.031921\nMicrosoft                            0.031908\nDota2                                0.031881\nWorldOfCraft                         0.031854\nApexLegends                          0.031799\nNBA2K                                0.031664\nCallOfDutyBlackopsColdWar            0.031664\nFIFA                                 0.031408\njohnson&amp;johnson                      0.031408\nTomClancysGhostRecon                 0.031367\nBattlefield                          0.031299\nOverwatch                            0.031299\nGrandTheftAuto(GTA)                  0.030989\nHomeDepot                            0.030975\nPlayStation5(PS5)                    0.030962\nHearthstone                          0.030894\nCS-GO                                0.030867\nXbox(Xseries)                        0.030853\nBorderlands                          0.030799\nAmazon                               0.030759\nGoogle                               0.030732\nNvidia                               0.030691\nCyberpunk2077                        0.030570\nRedDeadRedemption(RDR)               0.030394\nFortnite                             0.030394\nPlayerUnknownsBattlegrounds(PUBG)    0.030191\nAssassinsCreed                       0.030191\nName: proportion, dtype: float64</pre> <p>Look at a couple of examples of Positive &amp; Negative reviews</p> In\u00a0[41]: Copied! <pre>df_train.query(\"sentiment == 'Positive'\").sample(10)\n</pre> df_train.query(\"sentiment == 'Positive'\").sample(10) Out[41]: ID entity sentiment content 72566 8831 Nvidia Positive Better performance than anything wrong 34118 6659 Fortnite Positive I like my meals with a big round of Fortnite. 36331 8235 Microsoft Positive After 10 years without trying it, I set it up.... 57057 11388 TomClancysRainbowSix Positive I'm loving seeing all that the R6 driving and ... 47189 5697 HomeDepot Positive I\u2019m t h r i v i n g, thanks. 22805 4308 CS-GO Positive Help me win this awesome CS:GO giveaway from I... 2773 1682 CallOfDutyBlackopsColdWar Positive who tryna buy me this shit im willing to do an... 10463 13001 Xbox(Xseries) Positive I'm extremely happy to upgrade to the Xbox Ser... 69231 3860 Cyberpunk2077 Positive Who needs AI? A few precisely programmed text ... 49829 6155 FIFA Positive Had the most fun on FIFA since launch. Watchin... In\u00a0[42]: Copied! <pre>df_train.query(\"sentiment == 'Negative'\").sample(10)\n</pre> df_train.query(\"sentiment == 'Negative'\").sample(10) Out[42]: ID entity sentiment content 42429 10079 PlayerUnknownsBattlegrounds(PUBG) Negative Miss U S Pubg.... 71696 11082 TomClancysGhostRecon Negative RhandlerR Why before the update I don't have l... 65570 6829 johnson&amp;johnson Negative Are they infecting everyone with Venom Powder? 2579 1650 CallOfDutyBlackopsColdWar Negative Call Of Duty: Black Ops Cold War beta cheats a... 63539 7690 MaddenNFL Negative @EAMaddenNFL i don't know how you guys still m... 45656 11837 Verizon Negative Idk how people across Laredo say that @Verizon... 39258 5539 Hearthstone Negative Holy shit. Arena sucks now. Brings back all th... 58782 3277 Facebook Negative All my success in the past has come from peopl... 39639 1205 Battlefield Negative In multiplayer mode, I can't see even any one ... 35696 8129 Microsoft Negative Today I found out that Windows' Cortana actual... <p>Clean the data</p> In\u00a0[60]: Copied! <pre>sentiment_to_target = {'Positive': 2, 'Neutral': 1, 'Negative': 0, 'Irrelevant': 3}\ndf_train['Target'] = df_train['sentiment'].map(sentiment_to_target)\ndf_test['Target'] = df_test['sentiment'].map(sentiment_to_target)\n</pre> sentiment_to_target = {'Positive': 2, 'Neutral': 1, 'Negative': 0, 'Irrelevant': 3} df_train['Target'] = df_train['sentiment'].map(sentiment_to_target) df_test['Target'] = df_test['sentiment'].map(sentiment_to_target) In\u00a0[61]: Copied! <pre>train_ds = tf.data.Dataset.from_tensor_slices((df_train['content'].values, df_train['Target'].values))\ntest_ds = tf.data.Dataset.from_tensor_slices((df_test['content'].values, df_test['Target'].values))\n</pre> train_ds = tf.data.Dataset.from_tensor_slices((df_train['content'].values, df_train['Target'].values)) test_ds = tf.data.Dataset.from_tensor_slices((df_test['content'].values, df_test['Target'].values)) <p>To handle text content, we use the <code>tf.keras.layers.TextVectorization</code>.</p> <p>This layer will accept a string and maps text features to integer sequences. We need to be careful when initializing the layer to pass only the vocabulary of the training dataset.</p> <p>Quick example below :</p> <pre>&gt;&gt;&gt;layer = tf.keras.layers.TextVectorization(output_sequence_length=5)\n&gt;&gt;&gt;layer.adapt(['I like pizza', 'do you like fries'])\n&gt;&gt;&gt;layer(['I want pizza and fries'])\n&lt;tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[5, 1, 4, 1, 6, 0]])&gt;\n&gt;&gt;&gt;{i: v for i, v in enumerate(layer.get_vocabulary())}\n{0: '',\n 1: '[UNK]',\n 2: 'like',\n 3: 'you',\n 4: 'pizza',\n 5: 'i',\n 6: 'fries',\n 7: 'do'}\n</pre> <p>Token 0 is reserved for padding Token 1 is reserved for out-of-vocabulary tokens</p> In\u00a0[62]: Copied! <pre>for x, y in train_ds.batch(1).take(1):\n    print(x, y)\n</pre> for x, y in train_ds.batch(1).take(1):     print(x, y) <pre>tf.Tensor([b'I am coming to the borders and I will kill you all,'], shape=(1,), dtype=string) tf.Tensor([2], shape=(1,), dtype=int64)\n</pre> In\u00a0[87]: Copied! <pre>class LSTMClassification(tf.keras.Model):\n    def __init__(self, text: List):\n        super().__init__()\n        \n        # We create a text vectorization layer of max length 100, meaning any longer string will be cut.\n        # Shorter strings will be padded\n        self.text_vectorizer = tf.keras.layers.TextVectorization(\n            output_sequence_length=100\n        )\n        print(\"Fitting vocabulary in text vectorization layer\")\n        self.text_vectorizer.adapt(text)\n        \n        # We use embeddings with zero masking\n        self.embedding = tf.keras.layers.Embedding(\n            input_dim=len(self.text_vectorizer.get_vocabulary()),\n            output_dim=64,\n            mask_zero=True\n        )\n        \n        # Bi-directional LSTM\n        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))\n        \n        self.dense = tf.keras.layers.Dense(256)\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.out = tf.keras.layers.Dense(4, activation='softmax')\n\n    def call(self, inputs):\n        x = self.text_vectorizer(inputs)\n        x = self.embedding(x)\n        x = self.lstm(x)\n        x = self.dense(x)\n        x = self.dropout(x)\n        return self.out(x)\n\n    \nmodel = LSTMClassification(df_train['content'])\n</pre> class LSTMClassification(tf.keras.Model):     def __init__(self, text: List):         super().__init__()                  # We create a text vectorization layer of max length 100, meaning any longer string will be cut.         # Shorter strings will be padded         self.text_vectorizer = tf.keras.layers.TextVectorization(             output_sequence_length=100         )         print(\"Fitting vocabulary in text vectorization layer\")         self.text_vectorizer.adapt(text)                  # We use embeddings with zero masking         self.embedding = tf.keras.layers.Embedding(             input_dim=len(self.text_vectorizer.get_vocabulary()),             output_dim=64,             mask_zero=True         )                  # Bi-directional LSTM         self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))                  self.dense = tf.keras.layers.Dense(256)         self.dropout = tf.keras.layers.Dropout(0.5)         self.out = tf.keras.layers.Dense(4, activation='softmax')      def call(self, inputs):         x = self.text_vectorizer(inputs)         x = self.embedding(x)         x = self.lstm(x)         x = self.dense(x)         x = self.dropout(x)         return self.out(x)       model = LSTMClassification(df_train['content']) <pre>Fitting vocabulary in text vectorization layer\n</pre> In\u00a0[90]: Copied! <pre>model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    metrics=['accuracy']\n)\n</pre> model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),     optimizer=tf.keras.optimizers.Adam(1e-4),     metrics=['accuracy'] ) In\u00a0[91]: Copied! <pre>batched_train = train_ds.batch(128).prefetch(128)\nbatched_test = test_ds.batch(128).prefetch(128)\n\nhistory = model.fit(\n    batched_train, \n    epochs=10,\n    validation_data=batched_test\n)\n</pre> batched_train = train_ds.batch(128).prefetch(128) batched_test = test_ds.batch(128).prefetch(128)  history = model.fit(     batched_train,      epochs=10,     validation_data=batched_test ) <pre>Epoch 1/10\n579/579 [==============================] - 145s 226ms/step - loss: 1.3623 - accuracy: 0.3222 - val_loss: 1.3124 - val_accuracy: 0.3884\nEpoch 2/10\n579/579 [==============================] - 120s 208ms/step - loss: 1.1852 - accuracy: 0.4982 - val_loss: 0.9614 - val_accuracy: 0.6276\nEpoch 3/10\n579/579 [==============================] - 123s 213ms/step - loss: 0.9825 - accuracy: 0.5977 - val_loss: 0.7834 - val_accuracy: 0.6827\nEpoch 4/10\n579/579 [==============================] - 124s 213ms/step - loss: 0.8319 - accuracy: 0.6660 - val_loss: 0.6399 - val_accuracy: 0.7357\nEpoch 5/10\n579/579 [==============================] - 129s 223ms/step - loss: 0.6885 - accuracy: 0.7272 - val_loss: 0.5276 - val_accuracy: 0.7808\nEpoch 6/10\n579/579 [==============================] - 127s 219ms/step - loss: 0.5583 - accuracy: 0.7867 - val_loss: 0.4254 - val_accuracy: 0.8539\nEpoch 7/10\n579/579 [==============================] - 148s 256ms/step - loss: 0.4499 - accuracy: 0.8338 - val_loss: 0.3657 - val_accuracy: 0.8969\nEpoch 8/10\n579/579 [==============================] - 142s 245ms/step - loss: 0.3645 - accuracy: 0.8687 - val_loss: 0.3555 - val_accuracy: 0.9099\nEpoch 9/10\n579/579 [==============================] - 156s 269ms/step - loss: 0.3008 - accuracy: 0.8920 - val_loss: 0.3670 - val_accuracy: 0.9259\nEpoch 10/10\n579/579 [==============================] - 123s 213ms/step - loss: 0.2571 - accuracy: 0.9078 - val_loss: 0.3533 - val_accuracy: 0.9219\n</pre> In\u00a0[108]: Copied! <pre>fig, ax = plt.subplots(ncols=2, figsize=(15, 5))\nax[0].plot(history.history['loss'], label='train_loss')\nax[0].plot(history.history['val_loss'], label='validation_loss')\nax[1].plot(history.history['accuracy'], label='train_accuracy')\nax[1].plot(history.history['val_accuracy'], label='validation_accuracy')\nax[0].set_title(\"Loss\")\nax[1].set_title(\"Accuracy\")\nplt.legend()\n</pre> fig, ax = plt.subplots(ncols=2, figsize=(15, 5)) ax[0].plot(history.history['loss'], label='train_loss') ax[0].plot(history.history['val_loss'], label='validation_loss') ax[1].plot(history.history['accuracy'], label='train_accuracy') ax[1].plot(history.history['val_accuracy'], label='validation_accuracy') ax[0].set_title(\"Loss\") ax[1].set_title(\"Accuracy\") plt.legend() Out[108]: <pre>&lt;matplotlib.legend.Legend at 0x12a6cd190&gt;</pre> In\u00a0[127]: Copied! <pre>reverse_labels = {v: k for k, v in sentiment_to_target.items()}\n\ndef predict_sentence(s):\n    y = model.predict(s)\n    for n, x in enumerate(y[0]):\n        print(f\"{reverse_labels[n]}: {100*x:.2f}%\")\n</pre> reverse_labels = {v: k for k, v in sentiment_to_target.items()}  def predict_sentence(s):     y = model.predict(s)     for n, x in enumerate(y[0]):         print(f\"{reverse_labels[n]}: {100*x:.2f}%\")      In\u00a0[128]: Copied! <pre>predict_sentence(['I like my meals with a big mac'])\n</pre> predict_sentence(['I like my meals with a big mac']) <pre>1/1 [==============================] - 0s 35ms/step\nNegative: 0.77%\nNeutral: 0.84%\nPositive: 98.24%\nIrrelevant: 0.15%\n</pre> In\u00a0[130]: Copied! <pre>predict_sentence(['I hate when someone eats my big mac'])\n</pre> predict_sentence(['I hate when someone eats my big mac']) <pre>1/1 [==============================] - 0s 40ms/step\nNegative: 3.65%\nNeutral: 85.97%\nPositive: 1.34%\nIrrelevant: 9.03%\n</pre> In\u00a0[131]: Copied! <pre>predict_sentence(['Windows 95 really sucks'])\n</pre> predict_sentence(['Windows 95 really sucks']) <pre>1/1 [==============================] - 0s 40ms/step\nNegative: 99.96%\nNeutral: 0.04%\nPositive: 0.00%\nIrrelevant: 0.00%\n</pre> In\u00a0[133]: Copied! <pre>predict_sentence(['I am both happy and angry'])\n</pre> predict_sentence(['I am both happy and angry']) <pre>1/1 [==============================] - 0s 152ms/step\nNegative: 17.27%\nNeutral: 5.53%\nPositive: 73.70%\nIrrelevant: 3.50%\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/Sentiment%20Analysis/Simple%20LSTM/#simple-lstm-classifier-for-radarly-sentiment-analysis","title":"Simple LSTM Classifier for Radarly Sentiment Analysis\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/Simple%20LSTM/#load-the-data","title":"Load the data\u00b6","text":""},{"location":"notebooks/Sentiment%20Analysis/Simple%20LSTM/#create-a-simple-model","title":"Create a simple model\u00b6","text":"<p>We map the outputs to integers, maybe <code>Neutral</code> and <code>Irrelevant</code> could be merged in one class ?</p>"},{"location":"notebooks/Sentiment%20Analysis/Simple%20LSTM/#train-it","title":"Train it !\u00b6","text":""},{"location":"2019/08/21/Bayesian%20Basketball%20%3A%20were%20the%20Toronto%20Raptors%20really%20the%20best%20team%20during%20NBA%202019%20season%20%3F/","title":"Bayesian basketball","text":"<p>Work in progress</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/","title":"From notebooks to pipelines","text":""},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#advocating-for-ai","title":"Advocating for AI","text":"<p>There is a misunderstanding (not to say fantasy) which keeps coming back in companies whenever it comes to AI and Machine Learning. People often misjudge the complexity and the skills needed to bring Machine Learning projects to production, either because they do not understand the job, or (even worse) because they think they understand it, whereas they don\u2019t.</p> <p>Published in Towards Data Science</p> <p>Their first reaction when discovering AI might be something like \u201cAI is actually pretty simple, I just need a Jupyter Notebook, copy paste code from here and there \u2014 or ask Copilot \u2014 and boom. No need to hire Data Scientists after all\u2026\u201d And the story always end badly, with bitterness, disappointment and a feeling that AI is a scam: difficulty to move to production, data drift, bugs, unwanted behavior.</p> <p>So let\u2019s write it down once and for all: AI/Machine Learning/any data-related job, is a real job, not a hobby. It requires skills, craftsmanship, and tools. If you think you can do ML in production with notebooks, you are wrong.</p> <p>This article aims at showing, with a simple example, all the effort, skills and tools, it takes to move from a notebook to a real pipeline in production. Because ML in production is, mostly, about being able to automate the run of your code on a regular basis, with automation and monitoring.</p> <p>And for those who are looking for an end-to-end \u201cnotebook to vertex pipelines\u201d tutorial, you might find this helpful.</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#a-simple-use-case","title":"A simple use case","text":"<p>Let\u2019s imagine you are a Data Scientist working at an e-commerce company. Your company is selling clothes online, and the marketing team asks for your help: they are preparing a special offer for specific products, and they would like to efficiently target customers by tailoring email content that will be pushed to them to maximize conversion. Your job is therefore simple: each customer should be assigned a score which represents the probability he/she purchases a product from the special offer.</p> <p>The special offer will specifically target those brands, meaning that the marketing team wants to know which customers will buy their next product from the below brands:</p> <p>Allegra K, Calvin Klein, Carhartt, Hanes, Volcom, Nautica, Quiksilver, Diesel, Dockers, Hurley</p> <p>We will, for this article, use a publicly available dataset from Google, the <code>thelook_ecommerce</code> dataset. It contains fake data with transactions, customer data, product data, everything we would have at our disposal when working at an online fashion retailer.</p> <p>To follow this notebook, you will need access to Google Cloud Platform, but the logic can be replicated to other Cloud providers or third-parties like Neptune, MLFlow, etc.</p> <p>As a respectable Data Scientist, you start by creating a notebook which will help us in exploring the data.</p> <p>We first import libraries which we will use during this article:</p> <p><pre><code>import catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport datetime as dt\n\nfrom dataclasses import dataclass\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\n%load_ext watermark\n%watermark --packages catboost,pandas,sklearn,numpy,google.cloud.bigquery\n</code></pre> catboost             : 1.0.4 pandas               : 1.4.2 numpy                : 1.22.4 google.cloud.bigquery: 3.2.0</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#before-production","title":"Before Production","text":""},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#getting-and-preparing-the-data","title":"Getting and preparing the data","text":"<p>We will then load the data from BigQuery using the Python Client. Be sure to use your own project id:</p> <p>{% highlight python %} query = \"\"\"     SELECT        transactions.user_id,       products.brand,       products.category,       products.department,       products.retail_price,       users.gender,       users.age,       users.created_at,       users.country,       users.city,       transactions.created_at     FROM <code>bigquery-public-data.thelook_ecommerce.order_items</code> as transactions     LEFT JOIN <code>bigquery-public-data.thelook_ecommerce.users</code> as users       ON transactions.user_id = users.id     LEFT JOIN <code>bigquery-public-data.thelook_ecommerce.products</code> as products       ON transactions.product_id = products.id     WHERE status &lt;&gt; 'Cancelled' \"\"\"</p> <p>client = bigquery.Client() df = client.query(query).to_dataframe() {% endhighlight %} You should see something like that when looking at the dataframe:</p> <p> </p> <p>These represent the transactions / purchases made by the customers, enriched with customer and product information.</p> <p>Given our objective is to predict which brand customers will buy in their next purchase, we will proceed as follows:</p> <ol> <li>Group purchases chronologically for each customer</li> <li>If a customer has N purchases, we consider the Nth purchase as the target, and the N-1 as our features.</li> <li>exclude customers with only 1 purchase</li> </ol> <p>Let\u2019s put that into code:</p> <pre><code># Compute recurrent customers\nrecurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n# Merge with dataset and filter those with more than 1 purchase\ndf = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\ndf = df.query('n_purchases &gt; 1')\n\n# Fill missing values\ndf.fillna('NA', inplace=True)\n\ntarget_brands = [\n    'Allegra K', \n    'Calvin Klein', \n    'Carhartt', \n    'Hanes', \n    'Volcom', \n    'Nautica', \n    'Quiksilver', \n    'Diesel',\n    'Dockers', \n    'Hurley'\n]\n\naggregation_columns = ['brand', 'department', 'category']\n\n# Group purchases by user chronologically\ndf_agg = (df.sort_values('created_at')\n          .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n          .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n         )\n\n# Create the target\ndf_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\ndf_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\ndf_agg['age'] = df_agg['age'].astype(float)\n\n# Remove last item of sequence features to avoid target leakage :\nfor col in aggregation_columns:\n    df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n</code></pre> <p>Notice how we removed the last item in the sequence features: this is very important as otherwise we get what we call a \u201cdata leakeage\u201d: the target is part of the features, the model is given the answer when learning.</p> <p>We now get this new df_agg dataframe:</p> <p> </p> <p>Comparing with the original dataframe, we see that user_id 2 has indeed purchased IZOD, Parke &amp; Ronen, and finally Orvis which is not in the target brands.</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#splitting-into-train-validation-and-test","title":"Splitting into train, validation and test","text":"<p>As a seasoned Data Scientist, you will now split your data into different sets, as you obviously know that all three are required to perform some rigorous Machine Learning. (Cross-validation is out of the scope for today folks, let\u2019s keep it simple.)</p> <p>One key thing when splitting the data is to use the not-so-well-known <code>stratify</code> parameter from the scikit-learn <code>train_test_split()</code> method. The reason for that is because of class-imbalance: if the target distribution (% of 0 and 1 in our case) differs between training and testing, we might get frustrated with poor results when deploying the model. </p> <p>ML 101 kids: keep you data distributions as similar as possible between training data and test data.</p> <pre><code># Remove unecessary features\n\ndf_agg.drop('last_purchase_category', axis=1, inplace=True)\ndf_agg.drop('last_purchase_brand', axis=1, inplace=True)\ndf_agg.drop('user_id', axis=1, inplace=True)\n\n# Split the data into train and eval\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\")\n\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\") \n# 30950 samples in train\n\ndf_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\nprint(f\"{len(df_val)} samples in val\")\nprint(f\"{len(df_test)} samples in test\")\n# 3869 samples in train\n# 3869 samples in test\nNow this is done, we will gracefully split our dataset between features and targets:\n\nX_train, y_train = df_train.iloc[:, :-1], df_train['target']\nX_val, y_val = df_val.iloc[:, :-1], df_val['target']\nX_test, y_test = df_test.iloc[:, :-1], df_test['target']\n</code></pre> <p>Among the feature are different types. We usually separate those between:</p> <ul> <li>numerical features: they are continuous, and reflect a measurable, or ordered, quantity.</li> <li>categorical features: they are usually discrete, and are often represented as strings (ex: a country, a color, etc\u2026)</li> <li>text features: they are usually sequences of words. Of course there can be more like image, video, audio, etc.</li> </ul>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#the-model-introducing-catboost","title":"The model: introducing CatBoost","text":"<p>For our classification problem (you already knew we were in a classification framework, didn\u2019t you?), we will use a simple yet very powerful library: CatBoost. It is built and maintained by Yandex, and provides a high-level API to easily play with boosted trees. It is close to XGBoost, though it does not work exactly the same under the hood.</p> <p>CatBoost offers a nice wrapper to deal with features from different kinds. In our case, some features can be considered as \u201ctext\u201d as they are the concatenation of words, such as \u201cCalvin Klein;BCBGeneration;Hanes\u201d. Dealing with this type of features can sometimes be painful as you need to handle them with text splitters, tokenizers, lemmatizers, etc. Hopefully, CatBoost can manage everything for us!</p> <pre><code># Define features\nfeatures = {\n    'numerical': ['retail_price', 'age'],\n    'static': ['gender', 'country', 'city'],\n    'dynamic': ['brand', 'department', 'category']\n}\n\n# Build CatBoost \"pools\", which are datasets\ntrain_pool = cb.Pool(\n    X_train,\n    y_train,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\nvalidation_pool = cb.Pool(\n    X_val,\n    y_val,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\n# Specify text processing options to handle our text features\ntext_processing_options = {\n    \"tokenizers\": [\n        {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n    ],\n    \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n    \"feature_processing\": {\n        \"default\": [\n            {\n                \"dictionaries_names\": [\"Word\"],\n                \"feature_calcers\": [\"BoW\"],\n                \"tokenizers_names\": [\"SemiColon\"],\n            }\n        ],\n    },\n}\n</code></pre> <p>We are now ready to define and train our model. Going through each and every parameter is out of today\u2019s scope as the number of parameters is quite impressive, but feel free to check the API yourself.</p> <p>And for brevity, we will not perform hyperparameter tuning today, but this is obviously a large part of the Data Scientist\u2019s job!</p> <pre><code># Train the model\nmodel = cb.CatBoostClassifier(\n    iterations=200,\n    loss_function=\"Logloss\",\n    random_state=42,\n    verbose=1,\n    auto_class_weights=\"SqrtBalanced\",\n    use_best_model=True,\n    text_processing=text_processing_options,\n    eval_metric='AUC'\n)\n\nmodel.fit(\n    train_pool, \n    eval_set=validation_pool, \n    verbose=10\n)\n</code></pre> <p>And voila, our model is trained. Are we done?</p> <p>No. We need to check that our model\u2019s performance between training and testing is consistent. A huge gap between training and testing means our model is overfitting (i.e. \u201clearning the training data by heart and not good at predicting unseen data\u201d).</p> <p>For our model evaluation, we will use the ROC-AUC score. Not deep-diving on this one either, but from my own experience this is a generally quite robust metric and way better than accuracy.</p> <p>A quick side note on accuracy: I usually do not recommend using this as your evaluation metric. Think of an imbalanced dataset where you have 1% of positives and 99% of negatives. What would be the accuracy of a very dumb model predicting 0 all the time? 99%. So accuracy not helpful here.</p> <p><pre><code>from sklearn.metrics import roc_auc_score\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=model.predict(X_train)):.2f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=model.predict(X_val)):.2f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=model.predict(X_test)):.2f}\")\n</code></pre> ROC-AUC for train set      : 0.612 ROC-AUC for validation set : 0.586 ROC-AUC for test set       : 0.622</p> <p>To be honest, 0.62 AUC is not great at all and a little bit disappointing for the expert Data Scientist you are. Our model definitely needs a little bit of parameter tuning here, and maybe we should also perform feature engineering more seriously.</p> <p>But it is already better than random predictions (phew):</p> <p><pre><code># random predictions\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=np.random.rand(len(y_train))):.3f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=np.random.rand(len(y_val))):.3f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=np.random.rand(len(y_test))):.3f}\")\n</code></pre> ROC-AUC for train set      : 0.501 ROC-AUC for validation set : 0.499 ROC-AUC for test set       : 0.501</p> <p>Let\u2019s assume we are satisfied for now with our model and our notebook. This is where amateur Data Scientists would stop. So how do we make the next step and become production ready?</p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#moving-to-production","title":"Moving to Production","text":""},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#meet-docker","title":"Meet Docker","text":"<p>Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. This being said, think of Docker as code which can run everywhere, and allowing you to avoid the \u201cworks on your machine but not on mine\u201d situation.</p> <p>Why use Docker? Because among cool things such as being able to share your code, keep versions of it and ensure its easy deployment everywhere, it can also be used to build pipelines. Bear with me and you will understand as we go.</p> <p>The first step to building a containerized application is to refactor and clean up our messy notebook. We are going to define 2 files, <code>preprocess.py</code> and <code>train.py</code> for our very simple example, and put them in a src directory. We will also include our <code>requirements.txt</code> file with everything in it.</p> <p>(Blog post to come on poetry since requirements is so 2020)</p> <pre><code># src/preprocess.py\n\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\ndef create_dataset_from_bq():\n    query = \"\"\"\n        SELECT \n          transactions.user_id,\n          products.brand,\n          products.category,\n          products.department,\n          products.retail_price,\n          users.gender,\n          users.age,\n          users.created_at,\n          users.country,\n          users.city,\n          transactions.created_at\n        FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users\n          ON transactions.user_id = users.id\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products\n          ON transactions.product_id = products.id\n        WHERE status &lt;&gt; 'Cancelled'\n    \"\"\"\n    client = bigquery.Client(project='&lt;replace_with_your_project_id&gt;')\n    df = client.query(query).to_dataframe()\n    print(f\"{len(df)} rows loaded.\")\n\n    # Compute recurrent customers\n    recurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n    # Merge with dataset and filter those with more than 1 purchase\n    df = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\n    df = df.query('n_purchases &gt; 1')\n\n    # Fill missing value\n    df.fillna('NA', inplace=True)\n\n    target_brands = [\n        'Allegra K', \n        'Calvin Klein', \n        'Carhartt', \n        'Hanes', \n        'Volcom', \n        'Nautica', \n        'Quiksilver', \n        'Diesel',\n        'Dockers', \n        'Hurley'\n    ]\n\n    aggregation_columns = ['brand', 'department', 'category']\n\n    # Group purchases by user chronologically\n    df_agg = (df.sort_values('created_at')\n              .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n              .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n             )\n\n    # Create the target\n    df_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\n    df_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\n    df_agg['age'] = df_agg['age'].astype(float)\n\n    # Remove last item of sequence features to avoid target leakage :\n    for col in aggregation_columns:\n        df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n\n    df_agg.drop('last_purchase_category', axis=1, inplace=True)\n    df_agg.drop('last_purchase_brand', axis=1, inplace=True)\n    df_agg.drop('user_id', axis=1, inplace=True)\n    return df_agg\n\n\ndef make_data_splits(df_agg):\n\n    df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\n    print(f\"{len(df_train)} samples in train\")\n\n    df_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\n    print(f\"{len(df_val)} samples in val\")\n    print(f\"{len(df_test)} samples in test\")\n\n    return df_train, df_val, df_test\n</code></pre> <pre><code># src/train.py\n\nimport catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport argparse\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef train_and_evaluate(\n        train_path: str,\n        validation_path: str,\n        test_path: str\n    ):\n    df_train = pd.read_csv(train_path)\n    df_val = pd.read_csv(validation_path)\n    df_test = pd.read_csv(test_path)\n\n    df_train.fillna('NA', inplace=True)\n    df_val.fillna('NA', inplace=True)\n    df_test.fillna('NA', inplace=True)\n\n    X_train, y_train = df_train.iloc[:, :-1], df_train['target']\n    X_val, y_val = df_val.iloc[:, :-1], df_val['target']\n    X_test, y_test = df_test.iloc[:, :-1], df_test['target']\n\n    features = {\n        'numerical': ['retail_price', 'age'],\n        'static': ['gender', 'country', 'city'],\n        'dynamic': ['brand', 'department', 'category']\n    }\n\n    train_pool = cb.Pool(\n        X_train,\n        y_train,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    validation_pool = cb.Pool(\n        X_val,\n        y_val,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    test_pool = cb.Pool(\n        X_test,\n        y_test,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n\n    params = CatBoostParams()\n\n    text_processing_options = {\n        \"tokenizers\": [\n            {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n        ],\n        \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n        \"feature_processing\": {\n            \"default\": [\n                {\n                    \"dictionaries_names\": [\"Word\"],\n                    \"feature_calcers\": [\"BoW\"],\n                    \"tokenizers_names\": [\"SemiColon\"],\n                }\n            ],\n        },\n    }\n\n    # Train the model\n    model = cb.CatBoostClassifier(\n        iterations=200,\n        loss_function=\"Logloss\",\n        random_state=42,\n        verbose=1,\n        auto_class_weights=\"SqrtBalanced\",\n        use_best_model=True,\n        text_processing=text_processing_options,\n        eval_metric='AUC'\n    )\n\n\n    model.fit(\n        train_pool, \n        eval_set=validation_pool, \n        verbose=10\n    )\n\n    roc_train = roc_auc_score(y_true=y_train, y_score=model.predict(X_train))\n    roc_eval  = roc_auc_score(y_true=y_val, y_score=model.predict(X_val))\n    roc_test  = roc_auc_score(y_true=y_test, y_score=model.predict(X_test))\n    print(f\"ROC-AUC for train set      : {roc_train:.2f}\")\n    print(f\"ROC-AUC for validation set : {roc_eval:.2f}\")\n    print(f\"ROC-AUC for test.      set : {roc_test:.2f}\")\n\n    return {\"model\": model, \"scores\": {\"train\": roc_train, \"eval\": roc_eval, \"test\": roc_test}}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--train-path\", type=str)\n    parser.add_argument(\"--validation-path\", type=str)\n    parser.add_argument(\"--test-path\", type=str)\n    parser.add_argument(\"--output-dir\", type=str)\n    args, _ = parser.parse_known_args()\n    _ = train_and_evaluate(\n        args.train_path,\n        args.validation_path,\n        args.test_path)\n</code></pre> <p>Much cleaner now. You can actually launch your script from the command line now!</p> <p><code>$ python train.py --train-path xxx --validation-path yyy</code> etc. We are now ready to build our Docker image. For that we need to write a Dockerfile at the root of the project:</p> <p><pre><code># Dockerfile\n\nFROM python:3.8-slim\nWORKDIR /\nCOPY requirements.txt /requirements.txt\nCOPY src /src\nRUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\nENTRYPOINT [ \"bash\" ]\n</code></pre> This will take our requirements, copy the src folder and its contents, and install the requirements with pip when the image will build.</p> <p>To build and deploy this image to a container registry, we can use the Google Cloud SDK and the gcloud commands:</p> <p><pre><code>PROJECT_ID = ...\nIMAGE_NAME=f'thelook_training_demo'\nIMAGE_TAG='latest'\nIMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n\n!gcloud builds submit --tag $IMAGE_URI .\n</code></pre> If everything goes well, you should see something like that:</p> <p> </p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#vertex-pipelines-the-move-to-production","title":"Vertex Pipelines, the move to production","text":"<p>Docker images are the first step to doing some serious Machine Learning in production. The next step is building what we call \u201cpipelines\u201d. Pipelines are a series of operations orchestrated by a framework called Kubeflow. Kubeflow can run on Vertex AI on Google Cloud.</p> <p>The reasons for preferring pipelines over notebooks in production can be debatable, but I will give you three based on my experience:</p> <ul> <li>Monitoring and reproducibility: each pipeline is stored with its artefacts (datasets, models, metrics), meaning you can compare runs, re-run them, and audit them. Each time you re-run a notebook, you lose the history (or you have to manage artefacts yourself as weel as the logs. Good luck.)</li> <li>Costs: Running a notebook implies having a machine on which it runs. \u2014 This machine has a cost, and for large models or huge datasets you will need virtual machines with heavy specs.<ul> <li>You have to remember to switch it off when you don\u2019t use it.</li> <li>Or you may simply crash your local machine if you choose not to use a virtual machine and have other applications running.</li> <li>Vertex AI pipelines is a serverless service, meaning you do not have to manage the underlying infrastructure, and only pay for what you use, meaning the execution time.</li> </ul> </li> <li>Scalability: Good luck when running dozens of experiments on your local laptop simultaneously. You will roll back to using a VM, and scale that VM, and re-read the bullet point above. The last reason to prefer pipelines over notebooks is subjective and highly debatable as well, but in my opinion notebooks are simply not designed for running workloads on a schedule. They are great though for exploration.</li> </ul> <p>Use a cron job with a Docker image at least, or pipelines if you want to do things the right way, but never, ever, run a notebook in production.</p> <p>Without further ado, let\u2019s write the components of our pipeline:</p> <p><pre><code># IMPORT REQUIRED LIBRARIES\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (Artifact,\n                        Dataset,\n                        Input,\n                        Model,\n                        Output,\n                        Metrics,\n                        Markdown,\n                        HTML,\n                        component, \n                        OutputPath, \n                        InputPath)\nfrom kfp.v2 import compiler\nfrom google.cloud.aiplatform import pipeline_jobs\n\n%watermark --packages kfp,google.cloud.aiplatform\n</code></pre> kfp                    : 2.7.0 google.cloud.aiplatform: 1.50.0</p> <p>The first component will download the data from Bigquery and store it as a CSV file.</p> <p>The BASE_IMAGE we use is the image we build previously! We can use it to import modules and functions we defined in our Docker image src folder:</p> <p><pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"get_data.yaml\"\n)\ndef create_dataset_from_bq(\n    output_dir: Output[Dataset],\n):\n\n    from src.preprocess import create_dataset_from_bq\n\n    df = create_dataset_from_bq()\n\n    df.to_csv(output_dir.path, index=False)\n</code></pre> Next step: split data</p> <pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_test_split.yaml\",\n)\ndef make_data_splits(\n    dataset_full: Input[Dataset],\n    dataset_train: Output[Dataset],\n    dataset_val: Output[Dataset],\n    dataset_test: Output[Dataset]):\n\n    import pandas as pd\n    from src.preprocess import make_data_splits\n\n    df_agg = pd.read_csv(dataset_full.path)\n\n    df_agg.fillna('NA', inplace=True)\n\n    df_train, df_val, df_test = make_data_splits(df_agg)\n    print(f\"{len(df_train)} samples in train\")\n    print(f\"{len(df_val)} samples in train\")\n    print(f\"{len(df_test)} samples in test\")\n\n    df_train.to_csv(dataset_train.path, index=False)\n    df_val.to_csv(dataset_val.path, index=False)\n    df_test.to_csv(dataset_test.path, index=False)\n</code></pre> <p>Next step: model training. We will save the model scores to display them in the next step:</p> <pre><code>@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_model.yaml\",\n)\ndef train_model(\n    dataset_train: Input[Dataset],\n    dataset_val: Input[Dataset],\n    dataset_test: Input[Dataset],\n    model: Output[Model]\n):\n\n    import json\n    from src.train import train_and_evaluate\n\n    outputs = train_and_evaluate(\n        dataset_train.path,\n        dataset_val.path,\n        dataset_test.path\n    )\n    cb_model = outputs['model']\n    scores = outputs['scores']\n\n\n    model.metadata[\"framework\"] = \"catboost\" \n    # Save the model as an artifact\n    with open(model.path, 'w') as f: \n        json.dump(scores, f)\n</code></pre> <p>The last step is computing the metrics (which are actually computed in the training of the model). It is merely necessary but is nice to show you how easy it is to build lightweight components. Notice how in this case we don\u2019t build the component from the BASE_IMAGE (which can be quite large sometimes), but only build a lightweight image with necessary components:</p> <p><pre><code>@component(\n    base_image=\"python:3.9\",\n    output_component_file=\"compute_metrics.yaml\",\n)\ndef compute_metrics(\n    model: Input[Model],\n    train_metric: Output[Metrics],\n    val_metric: Output[Metrics],\n    test_metric: Output[Metrics]\n):\n\n    import json\n\n    file_name = model.path\n    with open(file_name, 'r') as file:  \n        model_metrics = json.load(file)\n\n    train_metric.log_metric('train_auc', model_metrics['train'])\n    val_metric.log_metric('val_auc', model_metrics['eval'])\n    test_metric.log_metric('test_auc', model_metrics['test'])\n</code></pre> There are usually other steps which we can include, like if we want to deploy our model as an API endpoint, but this is more advanced-level and requires crafting another Docker image for the serving of the model. To be covered next time.</p> <p>Let\u2019s now glue the components together:</p> <pre><code># USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\nTIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\nDISPLAY_NAME = 'pipeline-thelook-demo-{}'.format(TIMESTAMP)\nPIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n\n# Define the pipeline. Notice how steps reuse outputs from previous steps\n@dsl.pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=\"pipeline-demo\"   \n)\n\ndef pipeline(\n    project: str = PROJECT_ID,\n    region: str = REGION, \n    display_name: str = DISPLAY_NAME\n):\n\n    load_data_op = create_dataset_from_bq()\n    train_test_split_op = make_data_splits(\n        dataset_full=load_data_op.outputs[\"output_dir\"]\n    )\n    train_model_op = train_model(\n        dataset_train=train_test_split_op.outputs[\"dataset_train\"], \n        dataset_val=train_test_split_op.outputs[\"dataset_val\"],\n        dataset_test=train_test_split_op.outputs[\"dataset_test\"],\n        )\n    model_evaluation_op = compute_metrics(\n        model=train_model_op.outputs[\"model\"]\n    )\n\n# Compile the pipeline as JSON\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path='thelook_pipeline.json'\n)\n\n# Start the pipeline\nstart_pipeline = pipeline_jobs.PipelineJob(\n    display_name=\"thelook-demo-pipeline\",\n    template_path=\"thelook_pipeline.json\",\n    enable_caching=False,\n    location=REGION,\n    project=PROJECT_ID\n)\n\n# Run the pipeline\nstart_pipeline.run(service_account=&lt;your_service_account_here&gt;)\n</code></pre> <p>If everything works well, you will now see your pipeline in the Vertex UI:</p> <p> </p> <p>You can click on it and see the different steps:</p> <p> </p>"},{"location":"2024/05/11/From%20Notebooks%20to%20Pipelines/#conclusion","title":"Conclusion","text":"<p>Data Science, despite all the no-code/low-code enthusiasts telling you you don\u2019t need to be a developer to do Machine Learning, is a real job. Like every job, it requires skills, concepts and tools which go beyond notebooks.</p> <p>And for those who aspire to become Data Scientists, here is the reality of the job.</p> <p>Happy coding.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/","title":"Build your Personal Assistant with Agents and Tools","text":"<p>LLMs alone suffer from not being able to access external or real-time data. Learn how to build your personal assistant using LangChain agents and Gemini by grounding it in external sources.</p> <p>Published in Towards Data Science</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#summary","title":"Summary:","text":"<ol> <li>The problem with LLMs</li> <li>What are Agents, Tools and Chains ?</li> <li>Creating a simple chat without Tools</li> <li>Adding Tools to our chat: The Google way with Function Calling</li> <li>Adding Tools to our chat : The Langchain way with Agents</li> <li>Adding Memory to our Agent</li> <li>Creating a Chain with a Human Validation step</li> <li>Using search tools</li> </ol>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#1-the-problem-with-llms","title":"1. The problem with LLMs","text":"<p>So you have your favorite chatbot, and you use it for your daily job to boost your productivity. It can translate text, write nice emails, tell jokes, etc. And then comes the day when your colleague comes to you and asks :</p> <p>\u201cDo you know the current exchange rate between USD and EUR ? I wonder if I should sell my EUR\u2026\u201d</p> <p>You ask your favorite chatbot, and the answer pops :</p> <pre><code>I am sorry, I cannot fulfill this request. \nI do not have access to real-time information, including financial data \nlike exchange rates.\n</code></pre> <p>What is the problem here ?</p> <p>The problem is that you have stumbled on one of the shortcomings of LLMs. Large Language Models (LLMs) are powerful at solving many types of problems, such as problem solving, text summarization, generation, etc.</p> <p>However, they are constrained by the following limitations:</p> <ul> <li>They are frozen after training, leading to stale knowledge.</li> <li>They can\u2019t query or modify external data.</li> </ul> <p>Same way as we are using search engines every day, reading books and documents or querying databases, we would ideally want to provide this knowledge to our LLM to make it more efficient.</p> <p>Fortunately, there is a way to do that: Tools and Agents.</p> <p>Foundational models, despite their impressive text and image generation, remain constrained by their inability to interact with the outside world. Tools bridge this gap, empowering agents to interact with external data and services while unlocking a wider range of actions beyond that of the underlying model alone</p> <p>(source : Google Agents whitepaper)</p> <p>Using agents and tools, we could then be able to, from our chat interface:</p> <ul> <li>retrieve data from our own documents</li> <li>read / send emails</li> <li>interact with internal databases</li> <li>perform real time Google searches</li> <li>etc.</li> </ul>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#2-what-are-agents-tools-and-chains","title":"2. What are Agents, Tools and Chains ?","text":"<p>An agent is an application which attempts to achieve a goal (or a task) by having at its disposal a set of tools and taking decisions based on its observations of the environment.</p> <p>A good example of an agent could be you, for example: if you need to compute a complex mathematical operation (goal), you could use a calculator (tool #1), or a programming language (tool #2). Maybe you would choose the calculator to do a simple addition, but choose tool #2 for more complex algorithms.</p> <p>Agents are therefore made of :</p> <ul> <li>A model : The brain in our agent is the LLM. It will understand the query (the goal), and browse through its tools available to select the best.</li> <li>One or more tools : These are functions, or APIs, that are responsible for performing a specific action (ie: retrieving the current currency rate for USD vs EUR, adding numbers, etc.)</li> <li>An orchestration process: this is how the model will behave when asked to solve a task. It is a cognitive process that defines how the model will analyze the problem, refine inputs, choose a tool, etc. Examples of such processes are ReAct, CoT (Chain of Thought), ToT (Tree-of-Thought)</li> </ul> <p>Here is below a workflow explanation</p> <p></p> <p>Chains are somehow different. Whereas agents can \u2018decide\u2019 by themselves what to do and which steps to take, chains are just a sequence of predefined steps. They can still rely on tools though, meaning that they can include a step in which they need to select from available tools. We\u2019ll cover that later.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#3-creating-a-simple-chat-without-tools","title":"3. Creating a simple chat without Tools","text":"<p>To illustrate our point, we will first of all see how our LLM performs as-is, without any help.</p> <p>Let\u2019s install the needed libraries :</p> <pre><code>vertexai==1.65.0\nlangchain==0.2.16\nlangchain-community==0.2.16\nlangchain-core==0.2.38\nlangchain-google-community==1.0.8\nlangchain-google-vertexai==1.0.6\n</code></pre> <p>And create our very simple chat using Google\u2019s Gemini LLM:</p> <pre><code>from vertexai.generative_models import (\n    GenerativeModel,\n    GenerationConfig,\n    Part\n)\ngemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n)\nchat = gemini_model.start_chat()\n</code></pre> <p>If you run this simple chat and ask a question about the current exchange rate, you might probably get a similar answer:</p> <pre><code>response = chat.send_message(\"What is the current exchange rate for USD vs EUR ?\")\nanswer = response.candidates[0].content.parts[0].text\n--- OUTPUT ---\n\"I am sorry, I cannot fulfill this request. I do not have access to real-time information, including financial data like exchange rates.\"\n</code></pre> <p>Not surprising, as we know LLMs do not have access to real-time data.</p> <p>Let\u2019s add a tool for that. Our tool will be little function that calls an API to retrieve exchange rate data in real time.</p> <pre><code>def get_exchange_rate_from_api(params):\n    url = f\"https://api.frankfurter.app/latest?from={params['currency_from']}&amp;to={params['currency_to']}\"\n    print(url)\n    api_response = requests.get(url)\n    return api_response.text\n\n\n# Try it out !\nget_exchange_rate_from_api({'currency_from': 'USD', 'currency_to': 'EUR'})\n---\n'{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'\n</code></pre> <p>Now we know how our tools works, we would like to tell our chat LLM to use this function to answer our question. We will therefore create a mono-tool agent. To do that, we have several options which I will list here:</p> <ul> <li>Use Google\u2019s Gemini chat API with Function Calling</li> <li>Use LangChain\u2019s API with Agents and Tools</li> </ul> <p>Both have their advantages and drawbacks. The purpose of this article is also to show you the possibilities and let you decide which one you prefer.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#4-adding-tools-to-our-chat-the-google-way-with-function-calling","title":"4. Adding Tools to our chat: The Google way with Function Calling","text":"<p>There are basically two ways of creating a tool out of a function.</p> <p>The 1st one is a \u201cdictionary\u201d approach where you specify inputs and description of the function in the Tool. The imporant parameters are:</p> <ul> <li>Name of the function (be explicit)</li> <li>Description : be verbose here, as a solid and exhaustive description will help the LLM select the right tool</li> <li>Parameters : this is where you specify your arguments (type and description). Again, be verbose in the description of your arguments to help the LLM know how to pass value to your function</li> </ul> <pre><code>import requests\nfrom vertexai.generative_models import FunctionDeclaration\n\n\nget_exchange_rate_func = FunctionDeclaration(\n    name=\"get_exchange_rate\",\n    description=\"Get the exchange rate for currencies between countries\",\n    parameters={\n    \"type\": \"object\",\n    \"properties\": {\n        \"currency_from\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert from in ISO 4217 format\"\n        },\n        \"currency_to\": {\n            \"type\": \"string\",\n            \"description\": \"The currency to convert to in ISO 4217 format\"\n        }\n    },\n        \"required\": [            \n            \"currency_from\",\n            \"currency_to\",\n      ]\n  },\n)\n</code></pre> <p>The 2nd way of adding a tool using Google\u2019s SDK is with a <code>from_func</code> instantiation. This requires editing our original function to be more explicit, with a docstring, etc. Instead of being verbose in the Tool creation, we are being verbose in the function creation.</p> <pre><code># Edit our function\ndef get_exchange_rate_from_api(currency_from: str, currency_to: str):\n    \"\"\"\n    Get the exchange rate for currencies   \n\n    Args:\n        currency_from (str): The currency to convert from in ISO 4217 format\n        currency_to (str): The currency to convert to in ISO 4217 format\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&amp;to={currency_to}\"\n    api_response = requests.get(url)\n    return api_response.text\n\n# Create the tool\nget_exchange_rate_func = FunctionDeclaration.from_func(\n    get_exchange_rate_from_api\n)\n</code></pre> <p>The next step is really about creating the tool. For that, we will add our FunctionDeclaration to a list to create our Tool object:</p> <pre><code>from vertexai.generative_models import Tool as VertexTool\n\n\ntool = VertexTool(\n    function_declarations=[        \n        get_exchange_rate_func,\n        # add more functions here !\n    ]\n)\n</code></pre> <p>Let\u2019s now pass that to our chat and see if it now can answer our query about exchange rates ! Remember, without tools, our chat answered:</p> <p></p> <p>Let\u2019s try Google\u2019s Function calling tool and see if this helps ! First, let\u2019s send our query to the chat:</p> <pre><code>from vertexai.generative_models import GenerativeModel\n\n\ngemini_model = GenerativeModel(\n    \"gemini-1.5-flash\",\n    generation_config=GenerationConfig(temperature=0),\n    tools=[tool] #We add the tool here !\n)\nchat = gemini_model.start_chat()\nresponse = chat.send_message(prompt)\n\n# Extract the function call response\nresponse.candidates[0].content.parts[0].function_call\n--- OUTPUT ---\n\"\"\"\nname: \"get_exchange_rate\"\nargs {\n  fields {\n    key: \"currency_to\"\n    value {\n      string_value: \"EUR\"\n    }\n  }\n  fields {\n    key: \"currency_from\"\n    value {\n      string_value: \"USD\"\n    }\n  }\n  fields {\n    key: \"currency_date\"\n    value {\n      string_value: \"latest\"\n    }\n  }\n}\"\"\"\n</code></pre> <p>The LLM correctly guessed it needed to use the <code>get_exchange_rate</code> function, and also correctly guessed the 2 parameters were <code>USD</code> and <code>EUR</code> .</p> <p>But this is not enough. What we want now is to actually run this function to get our results!</p> <pre><code># mapping dictionnary to map function names and function\nfunction_handler = {\n    \"get_exchange_rate\": get_exchange_rate_from_api,\n}\n# Extract the function call name\nfunction_name = function_call.name\nprint(\"#### Predicted function name\")\nprint(function_name, \"\\n\")\n\n# Extract the function call parameters\nparams = {key: value for key, value in function_call.args.items()}\nprint(\"#### Predicted function parameters\")\nprint(params, \"\\n\")\n\nfunction_api_response = function_handler[function_name](params)\nprint(\"#### API response\")\nprint(function_api_response)\n\nresponse = chat.send_message(\n    Part.from_function_response(\n        name=function_name,\n        response={\"content\": function_api_response},\n    ),\n)   \nprint(\"\\n#### Final Answer\")\nprint(response.candidates[0].content.parts[0].text)\n--- OUTPUT ---\n\"\"\"\n#### Predicted function name\nget_exchange_rate \n#### Predicted function parameters\n{'currency_from': 'USD', 'currency_date': 'latest', 'currency_to': 'EUR'} \n#### API response\n{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}\n#### Final Answer\nThe current exchange rate for USD vs EUR is 0.94679. This means that 1 USD is equal to 0.94679 EUR. \n\"\"\"\n</code></pre> <p>We can now see our chat is able to answer our question! It:</p> <ul> <li>Correctly guessed to function to call, <code>get_exchange_rate</code></li> <li>Correctly assigned the parameters to call the function <code>{\u2018currency_from\u2019: \u2018USD\u2019, \u2018currency_to\u2019: \u2018EUR\u2019}</code></li> <li>Got results from the API</li> <li>And nicely formatted the answer to be human-readable!</li> </ul> <p>Let\u2019s now see another way of doing with LangChain.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#5-adding-tools-to-our-chat-the-langchain-way-with-agents","title":"5. Adding Tools to our chat: The Langchain way with Agents","text":"<p>LangChain is a composable framework to build with LLMs. It is the orchestration framework for controllable agentic workflows.</p> <p>Similar to what we did before the \u201cGoogle\u201d way, we will build tools in the Langchain way. Let\u2019s begin with defining our functions. Same as for Google, we need to be exhaustive and verbose in the docstrings:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef get_exchange_rate_from_api(currency_from: str, currency_to: str) -&gt; str:\n    \"\"\"\n    Return the exchange rate between currencies\n    Args:\n        currency_from: str\n        currency_to: str\n    \"\"\"\n    url = f\"https://api.frankfurter.app/latest?from={currency_from}&amp;to={currency_to}\"\n    api_response = requests.get(url)\n    return api_response.text\n</code></pre> <p>In order to spice things up, I will add another tool which can list tables in a BigQuery dataset. Here is the code:</p> <pre><code>@tool\ndef list_tables(project: str, dataset_id: str) -&gt; list:\n    \"\"\"\n    Return a list of Bigquery tables\n    Args:\n        project: GCP project id\n        dataset_id: ID of the dataset\n    \"\"\"\n    client = bigquery.Client(project=project)\n    try:\n        response = client.list_tables(dataset_id)\n        return [table.table_id for table in response]\n\n    except Exception as e:\n        return f\"The dataset {dataset_id} is not found in the {project} project, please specify the dataset and project\"\n</code></pre> <p>Add once done, we add our functions to our LangChain toolbox !</p> <pre><code>langchain_tool = [    \n    list_tables,\n    get_exchange_rate_from_api\n]\n</code></pre> <p>To build our agent, we will use the <code>AgentExecutor</code>object from LangChain. This object will basically take 3 components, which are the ones we defined earlier :</p> <ul> <li>A LLM</li> <li>A prompt</li> <li>And tools.</li> </ul> <p>Let\u2019s first choose our LLM:</p> <pre><code>from langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_google_vertexai import ChatVertexAI\n\n\ngemini_llm = ChatVertexAI(model=\"gemini-1.5-flash\")\n</code></pre> <p>Then we create a prompt to manage the conversation:</p> <pre><code>prompt = ChatPromptTemplate.from_messages(\n    [   \n      (\"system\", \"You are a helpful assistant\"),\n      (\"human\", \"{input}\"),\n        # Placeholders fill up a **list** of messages\n      (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n</code></pre> <p>And finally, we create the <code>AgentExecutor</code> and run a query:</p> <pre><code>agent = create_tool_calling_agent(gemini_llm, langchain_tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tools)\nagent_executor.invoke({\n    \"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"\n})\n--- OUTPUT ---\n\"\"\"\n{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'output': 'The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. \n            Please specify the correct dataset and project. \\n'}\n\"\"\"\n</code></pre> <p>Hmmm. Seems like the agent is missing one argument, or at least asking for more information\u2026Let\u2019s reply by giving this information:</p> <pre><code>agent_executor.invoke({\"input\": f\"Project id is bigquery-public-data\"})\n--- OUPTUT ---\n\"\"\"\n{'input': 'Project id is bigquery-public-data',\n 'output': 'OK. What else can I do for you? \\n'}\n\"\"\"\n</code></pre> <p>Well, seems we\u2019re back to square one. The LLM has been told the project id but forgot about the question. Our agent seems to be lacking memory to remember previous questions and answers. Maybe we should think of\u2026</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#6-adding-memory-to-our-agent","title":"6. Adding Memory to our Agent","text":"<p>Memory is another concept in Agents, which basically helps the system to remember the conversation history and avoid endless loops like above. Think of memory as being a notepad where the LLM keeps track of previous questions and answers to build context around the conversation.</p> <p>We will modify our prompt (instructions) to the model to include memory:</p> <pre><code>from langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n\n# Different types of memory can be found in Langchain\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nprompt = ChatPromptTemplate.from_messages(\n    [        \n        (\"system\", \"You are a helpful assistant.\"),\n        # First put the history\n        (\"placeholder\", \"{chat_history}\"),\n        # Then the new input\n        (\"human\", \"{input}\"),\n        # Finally the scratchpad\n        (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n\n# Remains unchanged\nagent = create_tool_calling_agent(gemini_llm, langchain_tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tools)\n\n# We add the memory part and the chat history\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    lambda session_id: memory, #&lt;-- NEW\n    input_messages_key=\"input\", \n    history_messages_key=\"chat_history\", #&lt;-- NEW\n)\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\n</code></pre> <p>We will now rerun our query from the beginning:</p> <pre><code>agent_with_chat_history.invoke({\n    \"input\": \"Which tables are available in the thelook_ecommerce dataset ?\"\n    }, \n    config\n)\n--- OUTPUT ---\n\"\"\"\n{'input': 'Which tables are available in the thelook_ecommerce dataset ?',\n 'chat_history': [],\n 'output': 'The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. Please specify the correct dataset and project. \\n'}\n\"\"\"\n</code></pre> <p>With an empty chat history, the model still asks for the project id. Pretty consistent with what we had before with a memoryless agent. Let\u2019s reply to the agent and add the missing information:</p> <pre><code>reply = \"Project id is bigquery-public-data\"\nagent_with_chat_history.invoke({\"input\": reply}, config)\n--- OUTPUT ---\n\"\"\"\n{'input': 'Project id is bigquery-public-data',\n 'chat_history': [HumanMessage(content='Which tables are available in the thelook_ecommerce dataset ?'),\n  AIMessage(content='The dataset `thelook_ecommerce` is not found in the `gcp-project-id` project. Please specify the correct dataset and project. \\n')],\n 'output': 'The following tables are available in the `thelook_ecommerce` dataset:\\n- distribution_centers\\n- events\\n- inventory_items\\n- order_items\\n- orders\\n- products\\n- users \\n'}\n\"\"\"\n</code></pre> <p>Notice how, in the output:</p> <ul> <li>The <code>chat history</code> keeps track of the previous Q&amp;A</li> <li>The output now returns the list of the tables!</li> </ul> <pre><code>'output': 'The following tables are available in the `thelook_ecommerce` dataset:\\n- distribution_centers\\n- events\\n- inventory_items\\n- order_items\\n- orders\\n- products\\n- users \\n'}\n</code></pre> <p>In some use cases however, certain actions might require special attention because of their nature (ie deleting an entry in a database, editing information, sending an email, etc.). Full automation without control might leads to situations where the agent takes wrong decisions and creates damage.</p> <p>One way to secure our workflows is to add a human-in-the-loop step.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#7-creating-a-chain-with-a-human-validation-step","title":"7. Creating a Chain with a Human Validation step","text":"<p>A chain is somehow different from an agent. Whereas the agent can decide to use or not to use tools, a chain is more static. It is a sequence of steps, for which we can still include a step where the LLM will choose from a set of tools.</p> <p>To build chains in LangChain, we use LCEL. LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. Chains in LangChain use the pipe <code>|</code> operator to indicate the orders in which steps have to be executed, such as <code>step 1 | step 2 | step 3 etc.</code> The difference with Agents is that Chains will always follow those steps, whereas Agents can \u201cdecide\u201d by themselves and are autonomous in their decision-making process.</p> <p>In our case, we will proceed as follows to build a simple <code>prompt | llm</code> chain.</p> <pre><code># define the prompt with memory\nprompt = ChatPromptTemplate.from_messages(\n    [   \n      (\"system\", \"You are a helpful assistant.\"),\n        # First put the history\n      (\"placeholder\", \"{chat_history}\"),\n        # Then the new input\n      (\"human\", \"{input}\"),\n        # Finally the scratchpad\n      (\"placeholder\", \"{agent_scratchpad}\"),\n    ]\n)\n# bind the tools to the LLM\ngemini_with_tools = gemini_llm.bind_tools(langchain_tool)\n# build the chain\nchain = prompt | gemini_with_tools\n</code></pre> <p>Remember how in the previous step we passed an agent to our <code>RunnableWithMessageHistory</code>? Well, we will do the same here, but...</p> <pre><code># With AgentExecutor\n# agent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\n# agent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n# agent_with_chat_history = RunnableWithMessageHistory(\n#     agent_executor,\n#     lambda session_id: memory,\n#     input_messages_key=\"input\",\n#     history_messages_key=\"chat_history\",\n# )\nconfig = {\"configurable\": {\"session_id\": \"foo\"}}\n\n# With Chains\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\nresponse = chain_with_history.invoke(\n    {\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n--- OUTPUT\n\"\"\"\ncontent='', \nadditional_kwargs={\n    'function_call': {\n        'name': 'get_exchange_rate_from_api', \n        'arguments': '{\"currency_from\": \"CHF\", \"currency_to\": \"EUR\"}'\n    }\n}\n\"\"\"\n</code></pre> <p>Unlike the agent, a chain does not provide the answer unless we tell it to. In our case, it stopped at the step where the LLM returns the function that needs to be called.</p> <p>We need to add an extra step to actually call the tool. Let\u2019s add another function to call the tools:</p> <pre><code>from langchain_core.messages import AIMessage\n\ndef call_tools(msg: AIMessage) -&gt; list[dict]:\n    \"\"\"Simple sequential tool calling helper.\"\"\"\n    tool_map = {tool.name: tool for tool in langchain_tool}\n    tool_calls = msg.tool_calls.copy()\n    for tool_call in tool_calls:\n        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n    return tool_calls\n\nchain = prompt | gemini_with_tools | call_tools #&lt;-- Extra step\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n# Rerun the chain \nchain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n</code></pre> <p>We now get the following output, which shows the API has been successfully called:</p> <pre><code>[{'name': 'get_exchange_rate_from_api',\n  'args': {'currency_from': 'CHF', 'currency_to': 'EUR'},\n  'id': '81bc85ea-dfd4-4c01-85e8-f3ca592fff5b',\n  'type': 'tool_call',\n  'output': '{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2024-11-20\",\"rates\":{\"EUR\":0.94679}}'\n}]\n</code></pre> <p>Now we understood how to chain steps, let\u2019s add our human-in-the-loop step ! We want this step to check that the LLM has understood our requests and will make the right call to an API. If the LLM has misunderstood the request or will use the function incorrectly, we can decide to interrupt the process.</p> <pre><code>def human_approval(msg: AIMessage) -&gt; AIMessage:\n    \"\"\"Responsible for passing through its input or raising an exception.\n    Args:\n        msg: output from the chat model\n    Returns:\n        msg: original output from the msg\n    \"\"\"\n    for tool_call in msg.tool_calls:\n        print(f\"I want to use function [{tool_call.get('name')}] with the following parameters :\")\n        for k,v in tool_call.get('args').items():\n            print(\" {} = {}\".format(k, v))\n\n    print(\"\")\n    input_msg = (\n        f\"Do you approve (Y|y)?\\n\\n\"\n        \"&gt;&gt;&gt;\"\n    )\n    resp = input(input_msg)\n\n    if resp.lower() not in (\"yes\", \"y\"):\n        raise NotApproved(f\"Tool invocations not approved:\\n\\n{tool_strs}\")\n\n    return msg\n</code></pre> <p>Next, add this step to the chain before the function call:</p> <pre><code>chain = prompt | gemini_with_tools | human_approval | call_tools\nmemory = InMemoryChatMessageHistory(session_id=\"foo\")\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n\nchain_with_history.invoke({\"input\": \"What is the current CHF EUR exchange rate ?\"}, config)\n</code></pre> <p>You will then be asked to confirm that the LLM understood correctly:</p> <p></p> <p>This human-in-the-loop step can be very helpful for critical workflows where a misinterpretation from the LLM could have dramatic consequences.</p>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#8-using-search-tools","title":"8. Using search tools","text":"<p>One of the most convenient tools to retrieve information in real-time are search engines . One way to do that is to use <code>GoogleSerperAPIWrapper</code> (you will need to register to get an API key in order to use it), which provides a nice interface to query Google Search and get results quickly.</p> <p>Luckily, LangChain already provides a tool for you, so we won\u2019t have to write the function ourselves.</p> <p>Let\u2019s therefore try to ask a question on yesterday\u2019s event (Nov 20th) and see if our agent can answer. Our question is about Rafael Nadal\u2019s last official game (which he lost to van de Zandschulp).</p> <pre><code>agent_with_chat_history.invoke(\n    {\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n--- OUTPUT ---\n\"\"\"\n{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"I do not have access to real-time information, including sports results. To get the latest information on Rafael Nadal's game, I recommend checking a reliable sports website or news source. \\n\"}\n\"\"\"\n</code></pre> <p>Without being able to access Google Search, our model is unable to answer because this information was not available at the time it was trained.</p> <p>Let\u2019s now add our Serper tool to our toolbox and see if our model can use Google Search to find the information:</p> <pre><code>from langchain_community.utilities import GoogleSerperAPIWrapper\n\n# Create our new search tool here\nsearch = GoogleSerperAPIWrapper(serper_api_key=\"...\")\n\n@tool\ndef google_search(query: str):\n    \"\"\"\n    Perform a search on Google\n    Args:\n        query: the information to be retrieved with google search\n    \"\"\"\n    return search.run(query)\n\n# Add it to our existing tools\nlangchain_tool = [    list_datasets,\n    list_tables,\n    get_exchange_rate_from_api,\n    google_search\n]\n\n# Create agent\nagent = create_tool_calling_agent(gemini_llm, langchain_tool, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=langchain_tool)\n\n# Add memory\nmemory = InMemoryChatMessageHistory()\nagent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    lambda session_id: memory,\n    input_messages_key=\"input\",\n    history_messages_key=\"chat_history\",\n)\n</code></pre> <p>And rerun our query :</p> <pre><code>agent_with_chat_history.invoke({\"input\": \"What was the result of Rafael Nadal's latest game ?\"}, config)\n--- OUTPUT ---\n\"\"\"\n{'input': \"What was the result of Rafael Nadal's latest game ?\",\n 'chat_history': [],\n 'output': \"Rafael Nadal's last match was a loss to Botic van de Zandschulp in the Davis Cup. Spain was eliminated by the Netherlands. \\n\"}\n\"\"\"\n</code></pre>"},{"location":"2024/11/24/Ground%20your%20chatbot%20with%20function%20calling/#conclusion","title":"Conclusion","text":"<p>LLMs alone often hit a blocker when it comes to using personal, corporate, private or real-data. Indeed, such information is generally not available at training time. Agents and tools are a powerful way to augment these models by allowing them to interact with systems and APIs, and orchestrate workflows to boost productivity.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/","title":"Gpu on vertex","text":"<p>In this article, we will take a concrete use case where we will fine-tune a BERT model on social media comments to perform sentiment analysis. As we will see, training this kind of model on a CPU is very cumbersome and not optimal. We will therefore see how we can leverage Google Cloud Platform to speed up the process by using a GPU for only 60 cents.</p> <p>Published in Towards Data Science</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#summary","title":"Summary","text":"<ul> <li>What is BERT</li> <li>What is Sentiment Analysis</li> <li>Get and prepare the data</li> <li>Use a small BERT pretrained model</li> <li>Create the dataloaders</li> <li>Write the main script to train the model</li> <li>Dockerize the script</li> <li>Build and push an image to Google Cloud</li> <li>Create a job on Vertex AI</li> </ul>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#what-is-bert","title":"What is BERT ?","text":"<p>BERT stands for Bidirectional Encoder Representations from Transformers and was open-sourced by Google in 2018. It is mainly used for NLP tasks as it was trained to capture semantics in sentences and provide rich word embeddings (representations). The difference with other models such as Word2Vec and Glove lies in the fact that it uses Transformers to process text. Transformers (refer to my previous article if you want to know more) are a family of neural networks which, a little bit like RNNs, have the ability to process sequences in both directions, therefore able to capture context around a word for example.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#what-is-sentiment-analysis","title":"What is Sentiment Analysis ?","text":"<p>Sentiment Analysis is a specific task within the NLP domain which objective is to classify text into categories related to the tonality of it. Tonality is often expressed as positive, negative, or neutral. It is very commonly used to analyze verbatims, posts on social media, product reviews, etc.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#fine-tuning-a-bert-model-on-social-media-data","title":"Fine-tuning a BERT model on social media data","text":""},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#getting-and-preparing-the-data","title":"Getting and preparing the data","text":"<p>The dataset we will use comes from Kaggle, you can download it here : https://www.kaggle.com/datasets/farisdurrani/sentimentsearch (CC BY 4.0 License). In my experiments, I only chose the datasets from Facebook and Twitter.</p> <p>The following snippet will take the csv files and save 3 splits (training, validation, and test) to where you want. I recommend saving them in Google Cloud Storage.</p> <p>You can run the script with:</p> <pre><code>python make_splits --output-dir gs://your-bucket/\n</code></pre> <p>And here is the script in itself:</p> <pre><code>import pandas as pd\nimport argparse\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n\ndef make_splits(output_dir):\n    df=pd.concat([        \n        pd.read_csv(\"data/farisdurrani/twitter_filtered.csv\"),\n        pd.read_csv(\"data/farisdurrani/facebook_filtered.csv\")\n    ])\n    df = df.dropna(subset=['sentiment'], axis=0)\n    df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n    df_train, df_ = train_test_split(df, stratify=df['Target'], test_size=0.2)\n    df_eval, df_test = train_test_split(df_, stratify=df_['Target'], test_size=0.5)\n\n    print(f\"Files will be saved in {output_dir}\")\n\n    df_train.to_csv(output_dir + \"/train.csv\", index=False)\n    df_eval.to_csv(output_dir + \"/eval.csv\", index=False)\n    df_test.to_csv(output_dir + \"/test.csv\", index=False)\n\n    print(f\"Train : ({df_train.shape}) samples\")\n    print(f\"Val : ({df_eval.shape}) samples\")\n    print(f\"Test : ({df_test.shape}) samples\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-dir')\n    args, _ = parser.parse_known_args()\n    make_splits(args.output_dir)\n</code></pre> <p>The data should look roughly like this:</p> <p></p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#using-a-small-bert-pretrained-model","title":"Using a small BERT pretrained model","text":"<p>For our model, we will use a lightweight BERT model, BERT-Tiny. This model has already been pretrained on vasts amount of data, but not necessarily with social media data and not necessarily with the objective of doing Sentiment Analysis. This is why we will fine-tune it.</p> <p>It contains only 2 layers with a 128-units dimension, the full list of models can be seen here if you want to take a larger one.</p> <p>Let\u2019s first create a <code>main.py</code> file, with all necessary modules:</p> <pre><code>import pandas as pd\nimport argparse\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport logging\nimport os\nos.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n\n\ndef train_and_evaluate(**params):\n    pass\n    # will be updated as we go\n</code></pre> <p>Let\u2019s also write down our requirements in a dedicated <code>requirements.txt</code></p> <pre><code>transformers==4.40.1\ntorch==2.2.2\npandas==2.0.3\nscikit-learn==1.3.2\ngcsfs\n</code></pre> <p>We will now load 2 parts to train our model:</p> <ul> <li>The tokenizer, which will take care of splitting the text inputs into tokens that BERT has been trained with.</li> <li>The model itself.</li> </ul> <p>You can obtain both from Huggingface here. You can also download them to Cloud Storage. That is what I did, and will therefore load them with:</p> <pre><code># Load pretrained tokenizers and bert model\ntokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\nmodel = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n</code></pre> <p>Let\u2019s now add the following piece to our file:</p> <pre><code>class SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n        # Uncomment the below if you only want to retrain certain layers.\n        # self.bert_module.requires_grad_(False)\n        # for param in self.bert_module.encoder.parameters():\n        #     param.requires_grad = True\n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        # print(ids.size(), mask.size(), token_type_ids.size())\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n</code></pre> <p>A little break here. We have several options when it comes to reusing an existing model.</p> <ul> <li>Transfer learning : we freeze the weights of the model and use it as a \u201cfeature extractor\u201d. We can therefore append additional layers downstream. This is frequently used in Computer Vision where models like VGG, Xception, etc. can be reused to train a custom model on small datasets</li> <li>Fine-tuning : we unfreeze all or part of the weights of the model and retrain the model on a custom dataset. This is the preferred approach when training custom LLMs.</li> </ul> <p>More details on Transfer learning and Fine-tuning here:</p> <p>In the model, we have chosen to unfreeze all the model, but feel free to freeze one or more layers of the pretrained BERT module and see how it influences the performance.</p> <p>The key part here is to add a fully connected layer after the BERT module to \u201clink\u201d it to our classification task, hence the final layer with 3 units. This will allow us to reuse the pretrained BERT weights and adapt our model to our task.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#creating-the-dataloaders","title":"Creating the dataloaders","text":"<p>To create the dataloaders we will need the Tokenizer loaded above. The Tokenizer takes a string as input, and returns several outputs amongst which we can find the tokens (\u2018input_ids\u2019 in our case):</p> <p></p> <p>The BERT tokenizer is a bit special and will return several outputs, but the most important one is the <code>input_ids</code>: they are the tokens used to encode our sentence. They might be words, or parts or words. For example, the word \u201clooking\u201d might be made of 2 tokens, \u201clook\u201d and \u201c##ing\u201d.</p> <p>Let\u2019s now create a dataloader module which will handle our datasets :</p> <pre><code>class BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#writing-the-main-script-to-train-the-model","title":"Writing the main script to train the model","text":"<p>Let us define first and foremost two functions to handle the training and evaluation steps:</p> <pre><code>def train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n</code></pre> <p>We are getting closer to getting our main script up and running. Let\u2019s stitch pieces together. We have:</p> <ul> <li>A <code>BertDataset</code> class to handle the loading of the data</li> <li>A <code>SentimentBERT</code> model which takes our Tiny-BERT model and adds an additional layer for our custom use case</li> <li><code>train()</code> and <code>eval()</code> functions to handle those steps</li> <li>A <code>train_and_eval()</code> functions that bundles everything</li> </ul> <p>We will use <code>argparse</code> to be able to launch our script with arguments. Such arguments are typically the train/eval/test files to run our model with any datasets, the path where our model will be stored, and parameters related to the training.</p> <pre><code>import pandas as pd\nimport time\nimport torch.nn as nn\nimport torch\nimport logging\nimport numpy as np\nimport argparse\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\n\nlogging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG)\nlogging.getLogger().setLevel(logging.INFO)\n\n# --- CONSTANTS ---\nBERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n\nif torch.cuda.is_available():\n    logging.info(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n    DEVICE = torch.device('cuda')\nelse:\n    logging.info(\"No GPU available. Training will run on CPU.\")\n    DEVICE = torch.device('cpu')\n\n# --- Data preparation and tokenization ---\nclass BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n\n# --- Model definition ---\nclass SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n\n# --- Training loop ---\ndef train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Validation loop ---\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Main function ---\ndef train_and_evaluate(**params):\n    logging.info(\"running with the following params :\")\n    logging.info(params)\n    # Load pretrained tokenizers and bert model\n    # update the paths to whichever you are using\n    tokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\n    model = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n\n    # Training parameters\n    epochs = int(params.get('epochs'))\n    batch_size = int(params.get('batch_size'))\n    learning_rate = float(params.get('learning_rate'))\n\n    #  Load the data\n    df_train = pd.read_csv(params.get('training_file'))\n    df_eval = pd.read_csv(params.get('validation_file'))\n    df_test = pd.read_csv(params.get('testing_file'))\n    # Create dataloaders\n    train_ds = BertDataset(df_train, tokenizer, max_length=100)\n    train_loader = DataLoader(dataset=train_ds,batch_size=batch_size, shuffle=True)\n    eval_ds = BertDataset(df_eval, tokenizer, max_length=100)\n    eval_loader = DataLoader(dataset=eval_ds,batch_size=batch_size)\n    test_ds = BertDataset(df_test, tokenizer, max_length=100)\n    test_loader = DataLoader(dataset=test_ds,batch_size=batch_size)\n\n    # Create the model\n    classifier = SentimentBERT(bert_model=model).to(DEVICE)\n    total_parameters = sum([np.prod(p.size()) for p in classifier.parameters()])\n    model_parameters = filter(lambda p: p.requires_grad, classifier.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    logging.info(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\")\n\n    # Optimizer and loss functions\n    optimizer = torch.optim.Adam([p for p in classifier.parameters() if p.requires_grad], learning_rate)\n    loss_fn = nn.CrossEntropyLoss()\n    # If dry run we only\n    logging.info(f'Training model with {BERT_MODEL_NAME}')\n\n    if args.dry_run:\n        logging.info(\"Dry run mode\")\n        epochs = 1\n        steps_per_epoch = 1\n    else:\n        steps_per_epoch = None\n\n    # Action !\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train_metrics = train(epoch, classifier, train_loader, loss_fn=loss_fn, optimizer=optimizer, max_steps=steps_per_epoch)\n        eval_metrics = evaluate(classifier, eval_loader, loss_fn=loss_fn)\n\n        print(\"-\" * 59)\n        print(\n            \"End of epoch {:3d} - time: {:5.2f}s - loss: {:.4f} - accuracy: {:.4f} - valid_loss: {:.4f} - valid accuracy {:.4f} \".format(\n                epoch, time.time() - epoch_start_time, train_metrics['loss'], train_metrics['acc'], eval_metrics['loss'], eval_metrics['acc']\n            )\n        )\n        print(\"-\" * 59)\n\n    if args.dry_run:\n        # If dry run, we do not run the evaluation\n        return None\n\n    test_metrics = evaluate(classifier, test_loader, loss_fn=loss_fn)\n\n    metrics = {\n        'train': train_metrics,\n        'val': eval_metrics,\n        'test': test_metrics,\n    }\n    logging.info(metrics)\n\n    # save model and architecture to single file\n    if params.get('job_dir') is None:\n        logging.warning(\"No job dir provided, model will not be saved\")\n    else:\n        logging.info(\"Saving model to {} \".format(params.get('job_dir')))\n        torch.save(classifier.state_dict(), params.get('job_dir'))\n    logging.info(\"Bye bye\")\n\n\nif __name__ == '__main__':\n    # Create arguments here\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--training-file', required=True, type=str)\n    parser.add_argument('--validation-file', required=True, type=str)\n    parser.add_argument('--testing-file', type=str)\n    parser.add_argument('--job-dir', type=str)\n    parser.add_argument('--epochs', type=float, default=2)\n    parser.add_argument('--batch-size', type=float, default=1024)\n    parser.add_argument('--learning-rate', type=float, default=0.01)\n    parser.add_argument('--dry-run', action=\"store_true\")\n    # Parse them\n    args, _ = parser.parse_known_args()\n    # Execute training\n    train_and_evaluate(**vars(args))\n</code></pre> <p>This is great, but unfortunately, this model will take a long time to train. Indeed, with around 4.7M parameters to train, one step will take around 3s on a 16Gb Macbook Pro with Intel chip.</p> <p></p> <p>3s per step can be quite long when you have 1238 steps to go and 10 epochs to complete\u2026</p> <p>No GPU, no party.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#how-to-use-vertex-ai-and-start-the-party","title":"How to use Vertex AI and start the party?","text":"<p>Short answer : Docker and gcloud.</p> <p>If you do not have a powerful GPU on your laptop (as most of us do), and/or want to avoid burning your laptop\u2019s cooling fan, you may want to move your script on a Cloud platform such as Google Cloud (disclaimer: I use Google Cloud at my job).</p> <p>The nice thing about Google is it offers 300$ in credits when you open your own project with your Gmail account.</p> <p>And as always, when it comes to transferring your code to somewhere else, Docker is usually the go-to solution.</p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#dockerizing-the-script","title":"Dockerizing the script","text":"<p>Let\u2019s write a Docker image with GPU enabled. There are a lot of Docker images you can find on the official Docker repository, I chose the pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime as I use a Pytorch 2.2.2 version. Be sure to select a version with CUDA, otherwise you will have to install it yourself in your Dockerfile, and trust me, you don\u2019t want to do that, except if you really have to.</p> <p>This Dockerfile will preinstall necessary CUDA dependencies and drivers and ensure we can use them in a custom training job, and run your python <code>main.py</code> file with the arguments that you will pass once you call the image.</p> <pre><code>FROM pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime\nWORKDIR /src\nCOPY . .\nRUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\nENTRYPOINT [\"python\", \"main.py\"]\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#building-and-pushing-an-image-to-google-cloud","title":"Building and pushing an image to Google Cloud","text":"<p>Once our image is ready to be built, we need to build it and push it to a registry. It can be on any registry you like, but Google Cloud offers a service for that called Artefact Registry. You will therefore be able to store your images on Google Cloud very easily.</p> <p>Write this little file at the root of your directory, and be sure that the Dockerfile is at the same level:</p> <pre><code># build.sh\nexport PROJECT_ID=&lt;your-project-id&gt;\nexport IMAGE_REPO_NAME=pt_bert_sentiment\nexport IMAGE_TAG=dev\nexport IMAGE_URI=eu.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\ngcloud builds submit --tag $IMAGE_URI .\n</code></pre> <p>Run the <code>build.sh</code> file, and after waiting a couple of minutes for the image to build, you should see something like: eu.gcr.io//pt_bert_sentiment:dev SUCCESS"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#creating-a-job-on-vertex-ai","title":"Creating a job on Vertex AI","text":"<p>Once your image has been built and pushed to Artefact Registry, we will now be able to tell Vertex AI to run this image on any machine we want, including ones with powerful GPUs ! Google offers a $300 credit when you create your own GCP project, it will be largely sufficient to run our model.</p> <p>Costs are available here. In our case, we will take the n1-standard-4 machine at $0.24/hr, and attach a NVIDIA T4 GPU at $0.40/hr.</p> <p></p> <p>Create a <code>job.sh</code> file as follows, by specifying which region you are in and what kind of machine you use. Refer to the link above if you are in a different region as costs may vary.</p> <p>You\u2019ll also need to pass arguments to your training script. The syntax for the <code>gcloud ai custom-jobs create</code> consists of 2 parts:</p> <ul> <li> <p>the arguments related to the job itself : <code>--region</code> , <code>--display-name</code> , <code>--worker-pool-spec</code> , <code>--service-account</code> , and <code>--args</code></p> </li> <li> <p>the arguments related to the training : <code>--training-file</code> , <code>--epochs</code> , etc.</p> </li> </ul> <p>The latter needs to be preceded by the <code>--args</code> to indicate that all following arguments are related to the training Python script.</p> <p>Ex: supposing our script takes 2 arguments x and y, we would have: <code>--args=x=1,y=2</code></p> <pre><code># job.sh\nexport PROJECT_ID=&lt;your-project-id&gt;\nexport BUCKET=&lt;your-bucket-id&gt;\nexport REGION=\"europe-west4\"\nexport SERVICE_ACCOUNT=&lt;your-service-account&gt;\nexport JOB_NAME=\"pytorch_bert_training\"\nexport MACHINE_TYPE=\"n1-standard-4\"  # We can specify GPUs here\nexport ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\nexport IMAGE_URI=\"eu.gcr.io/$PROJECT_ID/pt_bert_sentiment:dev\"\ngcloud ai custom-jobs create \\\n--region=$REGION \\\n--display-name=$JOB_NAME \\\n--worker-pool-spec=machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=1,replica-count=1,container-image-uri=$IMAGE_URI \\\n--service-account=$SERVICE_ACCOUNT \\\n--args=\\\n--training-file=gs://$BUCKET/data/train.csv,\\\n--validation-file=gs://$BUCKET/data/eval.csv,\\\n--testing-file=gs://$BUCKET/data/test.csv,\\\n--job-dir=gs://$BUCKET/model/model.pt,\\\n--epochs=10,\\\n--batch-size=128,\\\n--learning-rate=0.0001\n</code></pre>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#running-the-job-on-vertex-ai","title":"Running the job on Vertex AI","text":"<p>Launch the script, and navigate to your GCP project, in the Training section under the Vertex menu .</p> <p></p> <p>Launch the script, and navigate to the console. You should see the job status as \u201cPending\u201d, and then \u201cTraining\u201d.</p> <p>To ensure the GPU is being used, you can check the job and its ressources :</p> <p></p> <p>This indicates that we are training with a GPU, we should therefore expect a significant speed-up now ! Let\u2019s have a look at the logs:</p> <p></p> <p>Less than 10 minutes to run 1 epoch, vs 1hr/epoch on CPU ! We have offloaded the training to Vertex and accelerated the training process. We could decide to launch other jobs with different configurations, without overloading our laptop\u2019s capabilities.</p> <p>What about the final accuracy of the model ? Well after 10 epochs, it is around 94\u201395%. We could let it run even longer and see if the score improves (we can also add an early stopping callback to avoid overfitting)</p> <p></p>"},{"location":"2024/06/03/Using%20GPUs%20on%20Vertex/#how-does-our-model-perform","title":"How does our model perform ?","text":"<p>Time to party !</p>"},{"location":"2018/11/29/Attention%20Seq2Seq%20with%20PyTorch%3A%20learning%20to%20invert%20a%20sequence/","title":"Seq2seq with pytorch","text":"<p>Work in progress</p>"},{"location":"2019/02/09/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8AExponential%20Smoothing%20and%20ARIMA%20processes/","title":"Time series part1","text":"<p>Work in progress</p>"},{"location":"2019/02/15/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%202%3A%20Dealing%20with%20seasonal%20data/","title":"Time series part2","text":"<p>Work in progress</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/","title":"Time series part3","text":""},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#introduction","title":"Introduction","text":"<p>LSTM (Long Short-Term Memory) is a type a type of recurrent neural network (RNN) architecture, and was proposed in 1997 by Sepp Hochreiter and J\u00fcrgen Schmidhuber. RNNs are Deep neural networks specially designed to handle sequential data via recurrence mechanisms. </p> <p>They behave in an autoregressive manner, as they keep track of the past via internal states (hence the \u201cmemory\u201d part). They have been used extensively for speech recognition, machine translation, speech synthesis, etc.  But what are LSTMs worth when used on time-series ? Well, they can prove to be very useful to model non-linear relationships, assuming the size of the data available is large enough..</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#the-uber-use-case-bayesian-forecasting","title":"The Uber use case: Bayesian forecasting","text":"<p>When looking for papers implementing time series forecasting with LSTMs, I found a paper written by Uber in 2017, \u201cDeep and Confident Prediction for Time Series at Uber\u201d. The basic question behind this paper is : how confident can we be (ie how can we quantify uncertainty) making predictions with LSTMs ?</p> <p>The approach developped by Uber is a mixture of an encoder-decoder (used as an autoencoder) and a fully connected feed-forward network, used to predict the number of trips in a city based on previous data, or to detect anomalies in real time.</p> <p></p> <p>The Uber LSTM forecasting architecture (Zhu &amp; Laptev, 2017)</p> <p>The Uber paper is one of the first to use a Bayesian approach for time series forecasting. If you want to know more about Bayesian neural networks and Bayesian inference, you can look at the following links:</p> <ul> <li>Making your Neural Network Say I Don\u2019t Know</li> <li>Dropout as a Bayesian Approximation</li> <li>Deep Bayesian Neural Networks</li> <li>Bayesian Methods for Hackers, Cameron Davidson-Pilon</li> </ul>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#bayesian-neural-networks","title":"Bayesian Neural Networks","text":"<p>To put Bayesian neural networks in a nutshell, BNNs estimate a probability distribution over each weight, whereas classical Neural networks try to find the optimal value for each weight. When you hear \u201cBayesian\u201d, think \u201cprobability distribution\u201d.</p> <p>\u2014 But what\u2019s the link between forecasting uncertainty and Bayesian networks ?</p> <p>Imagine two weather experts, Bob and Alice. You both know they are quite good at predicting weather temperatures, but sometimes they get it wrong too. You ask them what will the temperature be tomorrow at 8am so you can know if you need to put your coat on or not. Bob says : \u201cIt will be 15.6\u00b0C\u201d. Alice says : \u201cI\u2019m 95 percent sure that the temperature will be between 16 and 18\u00b0C\u201d. Although they do not seem to agree, who would you trust ?</p> <p>Personally, I would trust Alice for two reasons:</p> <ul> <li>She gave me a confidence interval. I feel more reassured when someone is able to tell me something and how much he/she is confident with this information.</li> <li>I do not really care about the exact temperature, because a 1\u00b0C difference in temperature will not influence my decision to put on my coat.</li> </ul> <p>However, had Alice told me \u201cI\u2019m 95 percent sure that the temperature will be between 0\u00b0C and 18\u00b0C\u201d, I would have said that although she gave me a confidence level, the interval is too large to be informative\u2026</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#uncertainty-under-the-bnn-framework","title":"Uncertainty under the BNN framework","text":"<p>We usually separate uncertainty in 3 categories : model uncertainty, inherent noise, and model misspecification. The first two are the most famous and are usually referred to as epistemic and aleatoric uncertainty. Model (or e__pi__stemic) uncertainty is the uncertainty about our model parameters. The more data you have, the more you can explain (ie \u201creduce\u201d) this uncertainty. Noise (or aleatoric) uncertainty refers to the noise in the observations. If the noise uncertainty is constant for all samples, we call that homoscedastic aleatoric uncertainty. Otherwise, if some samples are more incertain than others, we will use the term heteroscedastic aleatoric. Noise uncertainty cannot be reduced with more data.</p> <p>In the Uber paper, a third uncertainty is used : model misspecification. This uncertainty aims to \u201c capture the uncertainty when predicting unseen samples with very different patterns from the training data set\u201d.</p> <p>\u2014 Now why distinguish all of these uncertainties ?</p> <p>Well, precisely because some can be combated with more data (model/epistemic), and some cannot. Some researchers therefore argue that it is more relevant to focus on aleatoric uncertainty given it cannot be reduced even with more data (Kendall &amp; Gal, 2017)</p> <p>In this article, we will only focus on the model uncertainty, to keep it simple. Uber uses different algorithms to evaluate the two other uncertainty types, but investigating them is beyond the scope of this article.</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#getting-and-preparing-the-data","title":"Getting and Preparing the data","text":"<p>The authors in the paper use 4 years of data over 8 cities in the US to train their model. 3 years are used for training and 1 year for testing. Unfortunately, Uber hasn\u2019t released this data yet, but in order to reproduce results from their paper, we will use data available on the New York Open data portal. We selected three years of data, spanning from early 2015 to mid 2018. Data was resampled with a daily basis. Here is what the full data looks like</p> <p></p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#stationarity","title":"Stationarity","text":"<p>We can notice a couple of things which you should be familiar with if you\u2019re used to analyzing time-series:</p> <ol> <li>We observe a clear upwards trend</li> <li>Mean and variance increase through time</li> <li>We also observe spikes which may be caused by external events (holidays and weather ?)</li> </ol> <p>The first two bullet points are a sign that our series is clearly not stationary. The latter shows that we might need to incorporate external data to our series. Stationarity could be checked with an Augmented Dickey-Fuller test or a KPSS test.  So should we carry out these tests just like we did for ARIMA models ?\u2014 The answer is : not necessarily.</p> <p>What we want, when we stationarize a series, is to have no change in mean and variance throughout time. This guarantees that if you take an N-points sample to make a forecast , and repeat this with another N-point different samples, then the relationship between your N-point samples and the N+1 point you are trying to predict are the same (ie they are drawn from the same distribution). If the mean and variance are not equal, then you are taking samples from different distributions to make forecasts, and your model will surely fail to generalize.</p> <p>Unlike ARIMA, RNNs are able to model nonlinear relationships in the data. RNNs, and particularly LSTM and GRU, are able to capture long-term dependencies (provided you have sufficient amounts of data !).  Issues like de-trending and de-seasonalizing are therefore less important, but you should always ask yourself :</p> <p>Does the data I use for testing follow the same behavior (ie is from the same distribution) as the data I use for training ?</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#holidays-weather","title":"Holidays &amp; Weather","text":"<p>Adding holidays indications is quite straightforward. We use the holidays library in Python to get the holidays dates from 2015 to 2018. After adding a holiday boolean to our series, we still observe unexplained spikes\u2026 A quick look on the internet shows that a couple of them were actually days when New York was hit by extreme weather events such as blizzards and snow storms. We plot below the data with holidays marked in red and extreme weather events in green:</p> <p></p> <p>In our dataframe, we therefore have a \u201ccounts\u201d column, a \u201cis_holiday\u201d column, and a \u201cis_bad_weather\u201d column. However, given we want to make predictions, we need to \u201canticipate\u201d these dates as we would do for future predictions, we will therefore create two additional columns indicating that the next day is a holiday or that extreme weather is expected the next day :</p> <p>weather = [datetime.datetime.strptime(date, \"%Y-%m-%d\") for date in ['2018-01-04', '2018-03-21','2017-03-14','2017-02-09','2016-01-23']]holidays = [date for y in range(2015, 2019) for date, _ in sorted(holidays.US(years=y).items())]df['is_holiday'] = np.where(df.index.isin(holidays), 1, 0) df['bad_weather'] = np.where(df.index.isin(weather), 1, 0) df['next_is_holiday'] = df.is_holiday.shift(-1) df['next_bad_weather'] = df.bad_weather.shift(-1)</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#choice-of-window-size-backtesting","title":"Choice of window size &amp; Backtesting","text":"<p>When doing time series forecasting you might hear about backtesting. Backtesting is a procedure used during training which consists in splitting your data into chunks, in an incremental manner. At each iteration, a chunk is used as your training set. You then try to predict 1 or more values ahead of your chunk. Two approaches can be used, expanding and sliding windows:</p> <p></p> <p>source: https://www.datapred.com/blog/the-basics-of-backtesting</p> <p>In our case study, the authors use samples consisting of 28-days sliding windows with step size equal to 1, used to predict the next value (1-step ahead forecast).</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#logging-and-scaling","title":"Logging and scaling","text":"<p>In the paper, the authors start by taking the log of the data to \u201c alleviate exponential effects\u201d. They then within each window substract the first value of the window to remove the trend and train the network on fluctuations with regard to the first value of the window(eq(1)). Other approaches can also be thought of, such as substracting the first value and dividing by the first value (eq(2)):</p> <p></p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#building-the-dataset","title":"Building the Dataset","text":"<p>Our dataset will be a generator yielding batches of sliding windows (each batch is the previous batch shifted of 1 value in the future). To follow the paper\u2019s instructions, we will also substract, within each batch, the first value to all other values. Each batch is then split between 28-days samples and their 1-day targets.</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#defining-the-model","title":"Defining the model","text":"<p>We will use PyTorch to define our model. A simple reason for that is that we will use dropout during inference and that it is simple to implement in PyTorch. We will start by using a simple LSTM network as defined in the paper: 1 LSTM layer with 128 units, 1 LSTM layer with 32 units, and a fully connected layer with 1 output. Dropout is added after each layer.</p> <p>\u2014 An apart\u00e9 on Dropout Dropout can be seen as a way of doing Bayesian inference (though there is still a debate around this). Technically, dropout is the process used in neural networks consisting in randomly dropping units (along with their connections). The fact of randomly turning neurons on and off is roughly equivalent to performing a sampling of a Bernoulli distribution, and therefore \u201csimulates\u201d the mechanics of a Bayesian Neural Network (where weights are distributions, and not single values). Applying dropout is a bit like if we were \u201csampling\u201d from the network. And if we repeat this process several times duting inference, we will get different predictions with which we can estimate a distribution and eventually, uncertainty ! To sum up:</p> <p></p> <p>Let\u2019s now define the model by adding dropout layers between each LSTM layer (notice how train is set to True so that Dropout is used during training and testing)</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#training","title":"Training","text":"<p>We train for 5 epochs, with an Adam optimizer and learning rate set to 0.001, and batch_size of 1.</p> <p>Results :</p> <p></p> <p>Fitted values on the train set</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#testing-1-day-forecast-horizon","title":"Testing \u2014 1 day forecast horizon","text":"<p>One of the key interests of this paper is the estimation of uncertainty.</p> <p>Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from the posterior predictive distribution. As a result, the model uncertainty can be estimated by the sample variance of the model predictions in a few repetitions.</p> <p>The idea behind this paper is therefore to run several times the model with random dropout, which will yield different output values. We can then compute the empirical mean and variance of our outputs to get confidence intervals for each time step !</p> <p>For each step, we predict 100 values. All values are different given we keep the dropout set. This allows us to simulate sampling from our network (in fact, Dropout is closely linked to Bayesian Neural Networks, given that by randomly disconnecting weights, it simulates a probability distribution). We take the average of these 100 values as our predicted mean, and the standard deviation of our 100 values which will be used for our confidence intervals. Assuming that our predictions are drawn from a normal distribution N(\u03bc, \u03c3\u00b2) \u2014 with a mean \u03bc equal to our empirical mean and a standard deviation \u03c3 equal to our empirical standard error \u2014, we can then estimate confidence intervals. In our case, it is given by :</p> <p></p> <p></p> <p>Predicted values and uncertainty intervals</p> <p>The prediction and the test curves seem to be quite close ! However, we need to find a metric to see how our model performs.</p> <p>If we look at the empirical coverage of 95% predictive intervals (ie the number of true test values included in the predicted 95% confidence intervals), we obtain a value of 28.51%, far from the values obtained on the test set in the paper\u2026When we take the 99% CI, this value goes up to 44% coverage. This is not a great result, but keep in mind our confidence interval is quite small given we only predict model uncertainty\u2026</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#testing-7-days-forecast-horizon","title":"Testing \u2014 7 days forecast horizon","text":"<p>Results on a single day forecast are not so good for a prediction model, and still we are not asking a lot to our model as we are asking for a very short prediction horizon. What if we trained our model to predict larger horizon forecasts ?</p>"},{"location":"2019/02/25/Time%20Series%20in%20Python%E2%80%8A%E2%80%94%E2%80%8APart%203%3A%20Forecasting%20taxi%20trips%20with%20LSTMs/#conclusion","title":"Conclusion","text":"<p>LSTMs show interesting perspectives for modelling time series due to their ability to capture long time dependencies, but should be used only for large datasets. Never expect your RNN to give good results on a 100-sample dataset ! The balance between the number of parameters in a neural network and the size of the data available is important to avoid overfitting. Nikolay Laptev, one of the authors of the Uber paper, concludes by saying that :</p> <p>Classical models are best for: \u25cb Short or unrelated time-series \u25cb Known state of world</p> <p>Neural Network is best for: \u25cb A lot of time-series \u25cb Long time-series \u25cb Hidden interactions \u25cb Explanation is not important</p> <p>Learn more about time series in Part 1 and Part 2 !</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/","title":"Transformers guide","text":"<p>An end-to-end implementation of a Pytorch Transformer, in which we will cover key concepts such as self-attention, encoders, decoders, and much more.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#writing-our-own","title":"Writing our own","text":"<p>When I decided to dig deeper into Transformer architectures, I often felt frustrated when reading or watching tutorials online as I felt they always missed something :</p> <ul> <li>Official tutorials from Tensorflow or Pytorch used their own APIs, thus staying high-level and forcing me to have to go in their codebase to see what was under the hood. Very time-consuming and not always easy to read 1000s of lines of code.</li> <li>Other tutorials with custom code I found (links at the end of the article) often oversimplified use cases and didn\u2019t tackle concepts such as masking of variable-length sequence batch handling.</li> </ul> <p>I therefore decided to write my own Transformer to make sure I understood the concepts and be able to use it with any dataset.</p> <p>During this article, we will therefore follow a methodical approach in which we will implement a transformer layer by layer and block by block.</p> <p>There are obviously a lot of different implementations as well as high-level APIs from Pytorch or Tensorflow already available off the shelf, with \u2014 I am sure \u2014 better performance than the model we will build.</p> <p>\u201cOk, but why not use the TF/Pytorch implementations then\u201d ?</p> <p>The purpose of this article is educational, and I have no pretention in beating Pytorch or Tensorflow implementations. I do believe that the theory and the code behind transformers is not straightforward, that is why I hope that going through this step-by-step tutorial will allow you to have a better grasp over these concepts and feel more comfortable when building your own code later.</p> <p>Another reasons to build your own transformer from scratch is that it will allow you to fully understand how to use the above APIs. If we look at the Pytorch implementation of the <code>forward()</code> method of the Transformer class, you will see a lot of obscure keywords like :</p> <p> </p> <p>If you are already familiar with these keywords, then you can happily skip this article.</p> <p>Otherwise, this article will walk you through each of these keywords with the underlying concepts.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#a-very-short-introduction-to-transformers","title":"A very short introduction to Transformers","text":"<p>If you already heard about ChatGPT or Gemini, then you already met a transformer before. Actually, the \u201cT\u201d of ChatGPT stands for Transformer.</p> <p>The architecture was first coined in 2017 by Google researchers in the \u201cAttention is All you need\u201d paper. It is quite revolutionary as previous models used to do sequence-to-sequence learning (machine translation, speech-to-text, etc\u2026) relied on RNNs which were computationnally expensive in the sense they had to process sequences step by step, whereas Transformers only need to look once at the whole sequence, moving the time complexity from O(n) to O(1).</p> <p> </p> <p>Applications of transformers are quite large in the domain of NLP, and include language translation, question answering, document summarization, text generation, etc.</p> <p>The overall architecture of a transformer is as below:</p> <p> </p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#multi-head-attention","title":"Multi-head attention","text":"<p>The first block we will implement is actually the most important part of a Transformer, and is called the Multi-head Attention. Let\u2019s see where it sits in the overall architecture</p> <p> </p> <p>Attention is a mechanism which is actually not specific to transformers, and which was already used in RNN sequence-to-sequence models.</p> <p> </p> <p> </p> <pre><code>import torch\nimport torch.nn as nn\nimport math\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim=256, num_heads=4):\n        \"\"\"\n        input_dim: Dimensionality of the input.\n        num_heads: The number of attention heads to split the input into.\n        \"\"\"\n        super(MultiHeadAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        assert hidden_dim % num_heads == 0, \"Hidden dim must be divisible by num heads\"\n        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output layer\n\n\n    def check_sdpa_inputs(self, x):\n        assert x.size(1) == self.num_heads, f\"Expected size of x to be ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}), got {x.size()}\"\n        assert x.size(3) == self.hidden_dim // self.num_heads\n\n\n    def scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n\n        \"\"\"\n        self.check_sdpa_inputs(query)\n        self.check_sdpa_inputs(key)\n        self.check_sdpa_inputs(value)\n\n\n        d_k = query.size(-1)\n        tgt_len, src_len = query.size(-2), key.size(-2)\n\n\n        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n\n        # Attention mask here\n        if attention_mask is not None:\n            if attention_mask.dim() == 2:\n                assert attention_mask.size() == (tgt_len, src_len)\n                attention_mask = attention_mask.unsqueeze(0)\n                logits = logits + attention_mask\n            else:\n                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n\n\n        # Key mask here\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n            logits = logits + key_padding_mask\n\n\n        attention = torch.softmax(logits, dim=-1)\n        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n\n        return output, attention\n\n\n    def split_into_heads(self, x, num_heads):\n        batch_size, seq_length, hidden_dim = x.size()\n        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n\n        return x.transpose(1, 2) # Final dim will be (batch_size, num_heads, seq_length, , hidden_dim // num_heads)\n\n    def combine_heads(self, x):\n        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n\n\n    def forward(\n            self, \n            q, \n            k, \n            v, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        q : tensor of shape (batch_size, query_sequence_length, hidden_dim)\n        k : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        v : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, self.num_heads)\n        k = self.split_into_heads(k, self.num_heads)\n        v = self.split_into_heads(v, self.num_heads)\n\n        # attn_values, attn_weights = self.multihead_attn(q, k, v, attn_mask=attention_mask)\n        attn_values, attn_weights  = self.scaled_dot_product_attention(\n            query=q, \n            key=k, \n            value=v, \n            attention_mask=attention_mask,\n            key_padding_mask=key_padding_mask,\n        )\n        grouped = self.combine_heads(attn_values)\n        output = self.Wo(grouped)\n\n        self.attention_weigths = attn_weights\n\n        return output\n</code></pre> <p>We need to explain a few concepts here.</p>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#1-queries-keys-and-values","title":"1) Queries, Keys and Values.","text":"<p>The query is the information you are trying to match, The key and values are the stored information.</p> <p>Think of that as using a dictionary : whenever using a Python dictionary, if your query doesn\u2019t match the dictionary keys, you won\u2019t be returned anything. But what if we want our dictionary to return a blend of information which are quite close ? Like if we had :</p> <pre><code>d = {\"panther\": 1, \"bear\": 10, \"dog\":3}\nd[\"wolf\"] = 0.2*d[\"panther\"] + 0.7*d[\"dog\"] + 0.1*d[\"bear\"]\n</code></pre> <p>This is basically what attention is about : looking at different parts of your data, and blend them to obtain a synthesis as an answer to your query.</p> <p>The relevant part of the code is this one, where we compute the attention weights between the query and the keys</p> <pre><code>logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # we compute the weights of attention\n</code></pre> <p>And this one, where we apply the normalized weights to the values :</p> <pre><code>attention = torch.softmax(logits, dim=-1)\noutput = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#2-attention-masking-and-padding","title":"2) Attention masking and padding","text":"<p>When attending to parts of a sequential input, we do not want to include useless or forbidden information.</p> <p>Useless information is for example padding: padding symbols, used to align all sequences in a batch to the same sequence size, should be ignored by our model. We will come back to that in the last section</p> <p>Forbidden information is a bit more complex. When being trained, a model learns to encode the input sequence, and align targets to the inputs. However, as the inference process involves looking at previously emitted tokens to predict the next one (think of text generation in ChatGPT), we need to apply the same rules during training.</p> <p>This is why we apply a causal mask to ensure that the targets, at each time step, can only see information from the past. Here is the corresponding section where the mask is applied (computing the mask is covered at the end)</p> <pre><code>if attention_mask is not None:\n    if attention_mask.dim() == 2:\n        assert attention_mask.size() == (tgt_len, src_len)\n        attention_mask = attention_mask.unsqueeze(0)\n        logits = logits + attention_mask\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#positional-encoding","title":"Positional Encoding","text":"<p>It corresponds to the following part of the Transformer:</p> <p> </p> <p>When receiving and treating an input, a transformer has no sense of order as it looks at the sequence as a whole, in opposition to what RNNs do. We therefore need to add a hint of temporal order so that the transformer can learn dependencies.</p> <p>The specific details of how positional encoding works is out of scope for this article, but feel free to read the original paper to understand.</p> <pre><code># Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#encoders","title":"Encoders","text":"<p>We are getting close to having a full encoder working ! The encoder is the left part of the Transformer.</p> <p> </p> <p>We will add a small part to our code, which is the Feed Forward part :</p> <pre><code>class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n</code></pre> <p>Putting the pieces together, we get an Encoder module !</p> <pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(EncoderBlock, self).__init__()\n        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm2 = nn.LayerNorm(n_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, src_padding_mask=None):\n        assert x.ndim==3, \"Expected input to be 3-dim, got {}\".format(x.ndim)\n        att_output = self.mha(x, x, x, key_padding_mask=src_padding_mask)\n        x = x + self.dropout(self.norm1(att_output))\n\n        ff_output = self.ff(x)\n        output = x + self.norm2(ff_output)\n\n        return output\n</code></pre> <p>As shown in the diagram, the Encoder actually contains N Encoder blocks or layers, as well as an Embedding layer for our inputs. Let\u2019s therefore create an Encoder by adding the Embedding, the Positional Encoding and the Encoder blocks:</p> <pre><code>class Encoder(nn.Module):\n    def __init__(\n            self, \n            vocab_size: int, \n            n_dim: int, \n            dropout: float, \n            n_encoder_blocks: int,\n            n_heads: int):\n\n        super(Encoder, self).__init__()\n        self.n_dim = n_dim\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )    \n        self.encoder_blocks = nn.ModuleList([\n            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)\n        ])\n\n\n    def forward(self, x, padding_mask=None):\n        x = self.embedding(x) * math.sqrt(self.n_dim)\n        x = self.positional_encoding(x)\n        for block in self.encoder_blocks:\n            x = block(x=x, src_padding_mask=padding_mask)\n        return x\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#decoders","title":"Decoders","text":"<p>The decoder part is the part on the left and requires a bit more crafting.</p> <p> </p> <p>There is something called Masked Multi-Head Attention. Remember what we said before about causal mask ? Well this happens here. We will use the attention_mask parameter of our Multi-head attention module to represent this (more details about how we compute the mask at the end) :</p> <pre><code># Stuff before\n\nself.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\nmasked_att_output = self.self_attention(\n    q=tgt, \n    k=tgt, \n    v=tgt, \n    attention_mask=tgt_mask, &lt;-- HERE IS THE CAUSAL MASK\n    key_padding_mask=tgt_padding_mask)\n\n# Stuff after\n</code></pre> <p>The second attention is called cross-attention. It will uses the decoder\u2019s query to match with the encoder\u2019s key &amp; values ! Beware : they can have different lengths during training, so it is usually a good practice to define clearly the expected shapes of inputs as follows :</p> <pre><code>def scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n</code></pre> <p>And here is the part where we use the encoder\u2019s output, called memory, with our decoder input :</p> <pre><code># Stuff before\nself.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\ncross_att_output = self.cross_attention(\n        q=x1, \n        k=memory, \n        v=memory, \n        attention_mask=None,  &lt;-- NO CAUSAL MASK HERE\n        key_padding_mask=memory_padding_mask) &lt;-- WE NEED TO USE THE PADDING OF THE SOURCE\n# Stuff after\n</code></pre> <p>Putting the pieces together, we end up with this for the Decoder :</p> <pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(DecoderBlock, self).__init__()\n\n        # The first Multi-Head Attention has a mask to avoid looking at the future\n        self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n\n        # The second Multi-Head Attention will take inputs from the encoder as key/value inputs\n        self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm2 = nn.LayerNorm(n_dim)\n\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm3 = nn.LayerNorm(n_dim)\n        # self.dropout = nn.Dropout(dropout)\n\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n\n        masked_att_output = self.self_attention(\n            q=tgt, k=tgt, v=tgt, attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)\n        x1 = tgt + self.norm1(masked_att_output)\n\n        cross_att_output = self.cross_attention(\n            q=x1, k=memory, v=memory, attention_mask=None, key_padding_mask=memory_padding_mask)\n        x2 = x1 + self.norm2(cross_att_output)\n\n        ff_output = self.ff(x2)\n        output = x2 + self.norm3(ff_output)\n\n\n        return output\n\nclass Decoder(nn.Module):\n    def __init__(\n        self, \n        vocab_size: int, \n        n_dim: int, \n        dropout: float, \n        n_decoder_blocks: int,\n        n_heads: int):\n\n        super(Decoder, self).__init__()\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim,\n            padding_idx=0\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )\n\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)\n        ])\n\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n        x = self.embedding(tgt)\n        x = self.positional_encoding(x)\n\n        for block in self.decoder_blocks:\n            x = block(\n                x, \n                memory, \n                tgt_mask=tgt_mask, \n                tgt_padding_mask=tgt_padding_mask, \n                memory_padding_mask=memory_padding_mask)\n        return x\n</code></pre> <p>Padding &amp; Masking Remember the Multi-head attention section where we mentionned excluding certain parts of the inputs when doing attention.</p> <p>During training, we consider batches of inputs and targets, wherein each instance may have a variable length. Consider the following example where we batch 4 words : banana, watermelon, pear, blueberry. In order to process them as a single batch, we need to align all words to the length of the longest word (watermelon). We will therefore add an extra token, PAD, to each word so they all end up with the same length as watermelon.</p> <p>In the below picture, the upper table represents the raw data, the lower table the encoded version:</p> <p> </p> <p>In our case, we want to exclude padding indices from the attention weights being calculated. We can therefore compute a mask as follows, both for source and target data :</p> <pre><code>padding_mask = (x == PAD_IDX)\n</code></pre> <p>What about causal masks now ? Well if we want, at each time step, that the model can attend only steps in the past, this means that for each time step T, the model can only attend to each step t for t in 1\u2026T. It is a double for loop, we can therefore use a matrix to compute that :</p> <p> </p> <pre><code>def generate_square_subsequent_mask(size: int):\n      \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n      mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n      return mask\n</code></pre>"},{"location":"2024/02/24/A%20Complete%20Guide%20to%20Write%20your%20own%20Transformers/#case-study-a-word-reverse-transformer","title":"Case study : a Word-Reverse Transformer","text":"<p>Let\u2019s now build our Transformer by bringing parts together !</p> <p>In our use case, we will use a very simple dataset to showcase how Transformers actually learn.</p> <p>\u201cBut why use a Transformer to reverse words ? I already know how to do that in Python with word[::-1] !\u201d</p> <p>The objective here is to see whether the Transformer attention mechanism works. What we expect is to see attention weights to move from right to left when given an input sequence. If so, this means our Transformer has learned a very simple grammar, which is just reading from right to left, and could generalize to more complex grammars when doing real-life language translation.</p> <p>Let\u2019s first begin with our custom Transformer class :</p> <pre><code>import torch\nimport torch.nn as nn\nimport math\n\nfrom .encoder import Encoder\nfrom .decoder import Decoder\n\n\nclass Transformer(nn.Module):\n    def __init__(self, **kwargs):\n        super(Transformer, self).__init__()\n\n        for k, v in kwargs.items():\n            print(f\" * {k}={v}\")\n\n        self.vocab_size = kwargs.get('vocab_size')\n        self.model_dim = kwargs.get('model_dim')\n        self.dropout = kwargs.get('dropout')\n        self.n_encoder_layers = kwargs.get('n_encoder_layers')\n        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n        self.n_heads = kwargs.get('n_heads')\n        self.batch_size = kwargs.get('batch_size')\n        self.PAD_IDX = kwargs.get('pad_idx', 0)\n\n        self.encoder = Encoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_encoder_layers, self.n_heads)\n        self.decoder = Decoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_decoder_layers, self.n_heads)\n        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n\n\n    @staticmethod    \n    def generate_square_subsequent_mask(size: int):\n            \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n            mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n            return mask\n\n\n    def encode(\n            self, \n            x: torch.Tensor, \n        ) -&gt; torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, S) with elements in (0, C) where C is num_classes\n        Output\n            (B, S, E) embedding\n        \"\"\"\n\n        mask = (x == self.PAD_IDX).float()\n        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        # (B, S, E)\n        encoder_output = self.encoder(\n            x, \n            padding_mask=encoder_padding_mask\n        )  \n\n        return encoder_output, encoder_padding_mask\n\n\n    def decode(\n            self, \n            tgt: torch.Tensor, \n            memory: torch.Tensor, \n            memory_padding_mask=None\n        ) -&gt; torch.Tensor:\n        \"\"\"\n        B = Batch size\n        S = Source sequence length\n        L = Target sequence length\n        E = Model dimension\n\n        Input\n            encoded_x: (B, S, E)\n            y: (B, L) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        mask = (tgt == self.PAD_IDX).float()\n        tgt_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        decoder_output = self.decoder(\n            tgt=tgt, \n            memory=memory, \n            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)), \n            tgt_padding_mask=tgt_padding_mask, \n            memory_padding_mask=memory_padding_mask,\n        )  \n        output = self.fc(decoder_output)  # shape (B, L, C)\n        return output\n\n\n\n    def forward(\n            self, \n            x: torch.Tensor, \n            y: torch.Tensor, \n        ) -&gt; torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, Sx) with elements in (0, C) where C is num_classes\n            y: (B, Sy) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        # Encoder output shape (B, S, E)\n        encoder_output, encoder_padding_mask = self.encode(x)  \n\n        # Decoder output shape (B, L, C)\n        decoder_output = self.decode(\n            tgt=y, \n            memory=encoder_output, \n            memory_padding_mask=encoder_padding_mask\n        )  \n\n        return decoder_output\n</code></pre>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"category/agents/","title":"agents","text":""},{"location":"category/langchain/","title":"langchain","text":""},{"location":"category/gemini/","title":"gemini","text":""},{"location":"category/vertex/","title":"vertex","text":""},{"location":"category/gcp/","title":"gcp","text":""},{"location":"category/llm/","title":"llm","text":""},{"location":"category/pytorch/","title":"pytorch","text":""},{"location":"category/classification/","title":"classification","text":""},{"location":"category/catboost/","title":"catboost","text":""},{"location":"category/transformers/","title":"transformers","text":""},{"location":"category/bayesian/","title":"bayesian","text":""},{"location":"category/timeseries/","title":"timeseries","text":""}]}